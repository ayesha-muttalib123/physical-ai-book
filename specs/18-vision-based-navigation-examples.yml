---
chapter_id: "18-vision-based-navigation-examples"
title: "Vision-Based Navigation Examples"
module_id: "M5-vla-humanoids"
module_title: "Vision-Language-Action & Humanoids"
overview: "This chapter provides comprehensive examples of vision-based navigation systems that enable robots to perceive their environment and navigate autonomously. Students will learn to implement visual SLAM, landmark-based navigation, and deep learning approaches for navigation. The chapter covers both classical computer vision techniques and modern deep learning methods for visual navigation, with emphasis on real-world deployment and robustness in dynamic environments."
why_it_matters: "Vision-based navigation is crucial for robots that need to operate in human environments where traditional sensors like GPS may not be available. Visual navigation enables robots to understand their surroundings, identify landmarks, and navigate to goals using only camera inputs. This capability is essential for creating autonomous robots that can operate in offices, homes, and other indoor environments without requiring specialized infrastructure."
key_concepts:
  - "Visual SLAM: Simultaneous localization and mapping using visual inputs"
  - "Feature Detection & Matching: Identifying and tracking visual landmarks"
  - "Deep Learning Navigation: End-to-end learning for visual navigation"
  - "Visual Odometry: Estimating motion from visual sequences"
  - "Landmark Recognition: Identifying and using visual landmarks for navigation"
  - "Scene Understanding: Interpreting visual scenes for navigation decisions"
  - "Visual Path Planning: Planning paths based on visual scene analysis"
  - "Navigation in Dynamic Environments: Handling moving obstacles and changing scenes"

code_examples:
  -
    title: "Visual SLAM Implementation"
    description: "Implementation of visual SLAM for simultaneous localization and mapping"
    language: "python"
    framework: "OpenCV with Robotics Libraries"
    code: |
      #!/usr/bin/env python3
      """
      Visual SLAM Implementation
      Demonstrates visual SLAM for simultaneous localization and mapping
      """
      import rclpy
      from rclpy.node import Node
      from sensor_msgs.msg import Image
      from geometry_msgs.msg import PoseStamped, Point, Pose
      from nav_msgs.msg import Odometry, Path
      from std_msgs.msg import Header
      from cv_bridge import CvBridge
      import cv2
      import numpy as np
      from collections import deque
      import math
      from typing import List, Tuple, Dict
      import threading

      class FeatureTracker:
          """Track visual features across frames"""
          def __init__(self):
              self.detector = cv2.SIFT_create()
              # For OpenCV without SIFT, use ORB instead:
              # self.detector = cv2.ORB_create(nfeatures=1000)

              self.matcher = cv2.BFMatcher()
              self.prev_keypoints = None
              self.prev_descriptors = None
              self.feature_history = {}  # Track features over time
              self.feature_id_counter = 0

          def detect_and_match_features(self, current_frame):
              """Detect features in current frame and match with previous"""
              # Detect keypoints and descriptors
              current_keypoints, current_descriptors = self.detector.detectAndCompute(
                  current_frame, None)

              if current_descriptors is None:
                  return [], []

              matches = []
              matched_keypoints = []

              if self.prev_descriptors is not None and len(self.prev_descriptors) > 0:
                  # Match features with previous frame
                  raw_matches = self.matcher.knnMatch(
                      self.prev_descriptors, current_descriptors, k=2)

                  # Apply Lowe's ratio test for good matches
                  good_matches = []
                  for match_pair in raw_matches:
                      if len(match_pair) == 2:
                          m, n = match_pair
                          if m.distance < 0.75 * n.distance:
                              good_matches.append(m)

                  # Extract matched keypoints
                  for match in good_matches:
                      prev_idx = match.queryIdx
                      curr_idx = match.trainIdx

                      if (prev_idx < len(self.prev_keypoints) and
                          curr_idx < len(current_keypoints)):
                          prev_pt = self.prev_keypoints[prev_idx].pt
                          curr_pt = current_keypoints[curr_idx].pt
                          matches.append((prev_pt, curr_pt))
                          matched_keypoints.append(current_keypoints[curr_idx])

              # Update previous frame data
              self.prev_keypoints = current_keypoints
              self.prev_descriptors = current_descriptors

              return matches, matched_keypoints

          def get_tracked_features(self):
              """Get currently tracked features"""
              if self.prev_keypoints is not None:
                  return [(kp.pt[0], kp.pt[1]) for kp in self.prev_keypoints]
              return []

      class PoseEstimator:
          """Estimate camera pose from feature matches"""
          def __init__(self):
              self.camera_matrix = np.array([
                  [500, 0, 320],  # fx, 0, cx
                  [0, 500, 240],  # 0, fy, cy
                  [0, 0, 1]       # 0, 0, 1
              ])
              self.dist_coeffs = np.zeros((4, 1))
              self.rotation_vector = np.zeros(3)
              self.translation_vector = np.zeros(3)
              self.absolute_pose = np.eye(4)  # 4x4 transformation matrix

          def estimate_pose(self, prev_points, curr_points):
              """Estimate relative pose from feature correspondences"""
              if len(prev_points) < 10 or len(curr_points) < 10:
                  return False, np.eye(4)

              prev_points = np.array(prev_points, dtype=np.float32)
              curr_points = np.array(curr_points, dtype=np.float32)

              # Use RANSAC to find fundamental matrix and filter outliers
              try:
                  fundamental_matrix, mask = cv2.findFundamentalMat(
                      prev_points, curr_points, cv2.RANSAC, 4, 0.999)

                  # Find inliers
                  inliers = []
                  for i, m in enumerate(mask):
                      if m[0]:
                          inliers.append((prev_points[i], curr_points[i]))

                  if len(inliers) < 10:
                      return False, np.eye(4)

                  # Extract inlier points
                  inlier_prev = np.array([pair[0] for pair in inliers])
                  inlier_curr = np.array([pair[1] for pair in inliers])

                  # Estimate essential matrix
                  essential_matrix, mask = cv2.findEssentialMat(
                      inlier_prev, inlier_curr, self.camera_matrix,
                      method=cv2.RANSAC, threshold=1.0)

                  if essential_matrix is None:
                      return False, np.eye(4)

                  # Decompose essential matrix to get rotation and translation
                  _, R, t, _ = cv2.recoverPose(
                      essential_matrix, inlier_prev, inlier_curr, self.camera_matrix)

                  # Create transformation matrix
                  transformation = np.eye(4)
                  transformation[:3, :3] = R
                  transformation[:3, 3] = t.flatten()

                  # Update absolute pose
                  self.absolute_pose = self.absolute_pose @ transformation

                  return True, transformation

              except Exception as e:
                  print(f"Pose estimation error: {e}")
                  return False, np.eye(4)

          def get_current_pose(self):
              """Get current estimated pose"""
              return self.absolute_pose.copy()

      class MapBuilder:
          """Build and maintain map of the environment"""
          def __init__(self):
              self.keyframes = []  # List of keyframe poses and features
              self.map_points = {}  # 3D map points
              self.next_point_id = 0
              self.keyframe_threshold = 0.5  # Minimum translation for new keyframe

          def add_keyframe(self, pose, features_2d, depth_estimates=None):
              """Add a new keyframe to the map"""
              keyframe = {
                  'pose': pose.copy(),
                  'features_2d': features_2d,
                  'timestamp': rclpy.time.Time().nanoseconds
              }

              # Add depth information if available
              if depth_estimates:
                  keyframe['depth_estimates'] = depth_estimates

              self.keyframes.append(keyframe)

              # Triangulate new 3D points if possible
              if len(self.keyframes) > 1:
                  self.triangulate_points()

          def triangulate_points(self):
              """Triangulate 3D points from multiple keyframes"""
              # Simplified triangulation - in real implementation this would be more sophisticated
              if len(self.keyframes) < 2:
                  return

              # Get the last two keyframes
              prev_frame = self.keyframes[-2]
              curr_frame = self.keyframes[-1]

              # This is a simplified approach - real triangulation would use multiple views
              # and more sophisticated methods
              for i in range(min(len(prev_frame['features_2d']),
                                len(curr_frame['features_2d']))):
                  if i < len(prev_frame['features_2d']) and i < len(curr_frame['features_2d']):
                      # Create a simple 3D point (this is highly simplified)
                      point_3d = np.array([
                          curr_frame['features_2d'][i][0] * 0.01,  # x
                          curr_frame['features_2d'][i][1] * 0.01,  # y
                          1.0  # z (fixed for simplicity)
                      ])

                      self.map_points[self.next_point_id] = {
                          'position': point_3d,
                          'observations': [len(self.keyframes)-2, len(self.keyframes)-1]
                      }
                      self.next_point_id += 1

          def get_map_points(self):
              """Get current map points"""
              return list(self.map_points.values())

      class VisualSLAMNode(Node):
          """ROS2 node implementing visual SLAM"""
          def __init__(self):
              super().__init__('visual_slam')

              # Publishers and subscribers
              self.image_sub = self.create_subscription(
                  Image, '/camera/rgb/image_raw', self.image_callback, 10)
              self.odom_pub = self.create_publisher(Odometry, '/visual_odom', 10)
              self.map_pub = self.create_publisher(Path, '/visual_map', 10)
              self.pose_pub = self.create_publisher(PoseStamped, '/visual_pose', 10)

              # Initialize components
              self.feature_tracker = FeatureTracker()
              self.pose_estimator = PoseEstimator()
              self.map_builder = MapBuilder()
              self.bridge = CvBridge()

              # State variables
              self.prev_features = []
              self.initialized = False
              self.frame_count = 0

              # Lock for thread safety
              self.slam_lock = threading.Lock()

              self.get_logger().info('Visual SLAM Node initialized')

          def image_callback(self, msg):
              """Process incoming camera image for SLAM"""
              with self.slam_lock:
                  try:
                      # Convert ROS image to OpenCV
                      cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='mono8')

                      # Detect and match features
                      matches, matched_keypoints = self.feature_tracker.detect_and_match_features(cv_image)

                      if matches:
                          # Extract matched points
                          prev_points = [m[0] for m in matches]
                          curr_points = [m[1] for m in matches]

                          # Estimate pose
                          success, relative_pose = self.pose_estimator.estimate_pose(prev_points, curr_points)

                          if success:
                              # Get current absolute pose
                              absolute_pose = self.pose_estimator.get_current_pose()

                              # Publish odometry
                              self.publish_odometry(absolute_pose, msg.header)

                              # Publish pose
                              self.publish_pose(absolute_pose, msg.header)

                              # Add to map if significant movement occurred
                              translation_norm = np.linalg.norm(relative_pose[:3, 3])
                              if (not self.initialized or
                                  translation_norm > self.map_builder.keyframe_threshold):
                                  self.map_builder.add_keyframe(
                                      absolute_pose,
                                      [(kp.pt[0], kp.pt[1]) for kp in matched_keypoints]
                                  )
                                  self.initialized = True

                                  # Publish updated map
                                  self.publish_map(msg.header)

                              # Update previous features
                              self.prev_features = curr_points

                          self.frame_count += 1

                  except Exception as e:
                      self.get_logger().error(f'Error in SLAM processing: {e}')

          def publish_odometry(self, pose_matrix, header):
              """Publish odometry information"""
              odom = Odometry()
              odom.header = header
              odom.child_frame_id = 'camera_frame'

              # Extract position and orientation from transformation matrix
              position = pose_matrix[:3, 3]
              odom.pose.pose.position.x = float(position[0])
              odom.pose.pose.position.y = float(position[1])
              odom.pose.pose.position.z = float(position[2])

              # Convert rotation matrix to quaternion
              R = pose_matrix[:3, :3]
              qw = math.sqrt(max(0, 1 + R[0,0] + R[1,1] + R[2,2])) / 2
              qx = math.sqrt(max(0, 1 + R[0,0] - R[1,1] - R[2,2])) / 2
              qy = math.sqrt(max(0, 1 - R[0,0] + R[1,1] - R[2,2])) / 2
              qz = math.sqrt(max(0, 1 - R[0,0] - R[1,1] + R[2,2])) / 2

              # Ensure quaternion has correct sign
              if R[2,1] - R[1,2] < 0: qx = -qx
              if R[0,2] - R[2,0] < 0: qy = -qy
              if R[1,0] - R[0,1] < 0: qz = -qz

              odom.pose.pose.orientation.w = qw
              odom.pose.pose.orientation.x = qx
              odom.pose.pose.orientation.y = qy
              odom.pose.pose.orientation.z = qz

              self.odom_pub.publish(odom)

          def publish_pose(self, pose_matrix, header):
              """Publish pose information"""
              pose_stamped = PoseStamped()
              pose_stamped.header = header

              position = pose_matrix[:3, 3]
              pose_stamped.pose.position.x = float(position[0])
              pose_stamped.pose.position.y = float(position[1])
              pose_stamped.pose.position.z = float(position[2])

              # Convert rotation matrix to quaternion (same as above)
              R = pose_matrix[:3, :3]
              qw = math.sqrt(max(0, 1 + R[0,0] + R[1,1] + R[2,2])) / 2
              qx = math.sqrt(max(0, 1 + R[0,0] - R[1,1] - R[2,2])) / 2
              qy = math.sqrt(max(0, 1 - R[0,0] + R[1,1] - R[2,2])) / 2
              qz = math.sqrt(max(0, 1 - R[0,0] - R[1,1] + R[2,2])) / 2

              if R[2,1] - R[1,2] < 0: qx = -qx
              if R[0,2] - R[2,0] < 0: qy = -qy
              if R[1,0] - R[0,1] < 0: qz = -qz

              pose_stamped.pose.orientation.w = qw
              pose_stamped.pose.orientation.x = qx
              pose_stamped.pose.orientation.y = qy
              pose_stamped.pose.orientation.z = qz

              self.pose_pub.publish(pose_stamped)

          def publish_map(self, header):
              """Publish map information"""
              path_msg = Path()
              path_msg.header = header

              # Add keyframe poses to the path
              for keyframe in self.map_builder.keyframes[-20:]:  # Last 20 keyframes
                  pose_stamped = PoseStamped()
                  pose_stamped.header = header

                  pose = keyframe['pose']
                  position = pose[:3, 3]
                  pose_stamped.pose.position.x = float(position[0])
                  pose_stamped.pose.position.y = float(position[1])
                  pose_stamped.pose.position.z = float(position[2])

                  # Add to path
                  path_msg.poses.append(pose_stamped)

              self.map_pub.publish(path_msg)

      def main(args=None):
          rclpy.init(args=args)
          node = VisualSLAMNode()

          try:
              rclpy.spin(node)
          except KeyboardInterrupt:
              node.get_logger().info('Shutting down visual SLAM node...')
          finally:
              node.destroy_node()
              rclpy.shutdown()

      if __name__ == '__main__':
          main()

  -
    title: "Deep Learning Visual Navigation"
    description: "Implementation of deep learning approach for visual navigation"
    language: "python"
    framework: "PyTorch with Robotics Libraries"
    code: |
      #!/usr/bin/env python3
      """
      Deep Learning Visual Navigation
      Demonstrates end-to-end learning for visual navigation using deep neural networks
      """
      import rclpy
      from rclpy.node import Node
      from sensor_msgs.msg import Image
      from geometry_msgs.msg import Twist, Pose
      from std_msgs.msg import Float64MultiArray
      from cv_bridge import CvBridge
      import torch
      import torch.nn as nn
      import torch.nn.functional as F
      import cv2
      import numpy as np
      from collections import deque
      import math

      class VisualNavigationCNN(nn.Module):
          """CNN for processing visual input for navigation"""
          def __init__(self, num_actions=4):
              super().__init__()

              # Convolutional layers for feature extraction
              self.conv_layers = nn.Sequential(
                  nn.Conv2d(3, 32, kernel_size=8, stride=4),
                  nn.ReLU(),
                  nn.Conv2d(32, 64, kernel_size=4, stride=2),
                  nn.ReLU(),
                  nn.Conv2d(64, 64, kernel_size=3, stride=1),
                  nn.ReLU(),
                  nn.Flatten()
              )

              # Calculate the size after convolutions
              # Assuming input is 224x224 -> after convs: 7x7x64 = 3136
              self.conv_output_size = 64 * 7 * 7  # Adjust based on your input size

              # Fully connected layers
              self.fc_layers = nn.Sequential(
                  nn.Linear(self.conv_output_size, 512),
                  nn.ReLU(),
                  nn.Dropout(0.5),
                  nn.Linear(512, 256),
                  nn.ReLU(),
                  nn.Dropout(0.5),
                  nn.Linear(256, num_actions)
              )

              # Goal conditioning (optional)
              self.goal_conditioning = nn.Linear(3, 64)  # x, y, theta
              self.fusion_layer = nn.Linear(256 + 64, 256)

          def forward(self, image, goal=None):
              # Process image through conv layers
              conv_features = self.conv_layers(image)

              # Process through FC layers
              fc_features = self.fc_layers[:3](conv_features)  # Up to first dropout

              if goal is not None:
                  # Process goal
                  goal_features = self.goal_conditioning(goal)
                  # Concatenate image and goal features
                  combined_features = torch.cat([fc_features, goal_features], dim=1)
                  # Process through fusion layer
                  fused_features = F.relu(self.fusion_layer(combined_features))
                  # Final output layer
                  output = self.fc_layers[-1](fused_features)
              else:
                  output = self.fc_layers[-2:](fc_features)  # Skip last dropout, apply last linear

              return output

      class NavigationMemory:
          """Memory buffer for storing navigation experiences"""
          def __init__(self, max_size=10000):
              self.buffer = deque(maxlen=max_size)
              self.states = deque(maxlen=max_size)
              self.actions = deque(maxlen=max_size)
              self.rewards = deque(maxlen=max_size)
              self.next_states = deque(maxlen=max_size)
              self.dones = deque(maxlen=max_size)

          def add_experience(self, state, action, reward, next_state, done):
              """Add experience to memory"""
              self.states.append(state)
              self.actions.append(action)
              self.rewards.append(reward)
              self.next_states.append(next_state)
              self.dones.append(done)

          def sample_batch(self, batch_size):
              """Sample a batch of experiences"""
              if len(self.states) < batch_size:
                  batch_size = len(self.states)

              indices = np.random.choice(len(self.states), batch_size, replace=False)

              states = torch.stack([self.states[i] for i in indices])
              actions = torch.tensor([self.actions[i] for i in indices])
              rewards = torch.tensor([self.rewards[i] for i in indices], dtype=torch.float32)
              next_states = torch.stack([self.next_states[i] for i in indices])
              dones = torch.tensor([self.dones[i] for i in indices], dtype=torch.float32)

              return states, actions, rewards, next_states, dones

      class DeepNavigationAgent:
          """Deep learning navigation agent"""
          def __init__(self, device='cpu'):
              self.device = device
              self.network = VisualNavigationCNN(num_actions=4).to(device)  # linear_x, linear_y, angular_z, stop
              self.target_network = VisualNavigationCNN(num_actions=4).to(device)
              self.optimizer = torch.optim.Adam(self.network.parameters(), lr=1e-4)
              self.memory = NavigationMemory()

              # Training parameters
              self.gamma = 0.99  # Discount factor
              self.epsilon = 1.0  # Exploration rate
              self.epsilon_min = 0.01
              self.epsilon_decay = 0.995
              self.update_target_freq = 1000
              self.step_count = 0

              # Copy network weights to target network
              self.target_network.load_state_dict(self.network.state_dict())

          def preprocess_image(self, image):
              """Preprocess image for network input"""
              # Resize image to network input size (224x224)
              resized = cv2.resize(image, (224, 224))

              # Convert BGR to RGB and normalize
              rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
              normalized = rgb.astype(np.float32) / 255.0

              # Convert to tensor and change to CHW format
              tensor = torch.from_numpy(normalized).permute(2, 0, 1).unsqueeze(0)
              tensor = tensor.to(self.device)

              return tensor

          def select_action(self, state, goal=None):
              """Select action using epsilon-greedy policy"""
              if np.random.random() < self.epsilon:
                  # Random action (explore)
                  return np.random.randint(0, 4)
              else:
                  # Greedy action (exploit)
                  with torch.no_grad():
                      if goal is not None:
                          goal_tensor = torch.tensor(goal, dtype=torch.float32).unsqueeze(0).to(self.device)
                          q_values = self.network(state, goal_tensor)
                      else:
                          q_values = self.network(state)
                      return q_values.argmax().item()

          def train(self, batch_size=32):
              """Train the network on a batch of experiences"""
              if len(self.memory.states) < batch_size:
                  return

              # Sample batch from memory
              states, actions, rewards, next_states, dones = self.memory.sample_batch(batch_size)

              # Compute current Q values
              current_q_values = self.network(states).gather(1, actions.unsqueeze(1))

              # Compute next Q values using target network
              with torch.no_grad():
                  next_q_values = self.target_network(next_states).max(1)[0]
                  target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))

              # Compute loss
              loss = F.mse_loss(current_q_values.squeeze(), target_q_values)

              # Optimize
              self.optimizer.zero_grad()
              loss.backward()
              self.optimizer.step()

              # Decay exploration rate
              if self.epsilon > self.epsilon_min:
                  self.epsilon *= self.epsilon_decay

              # Update target network periodically
              self.step_count += 1
              if self.step_count % self.update_target_freq == 0:
                  self.target_network.load_state_dict(self.network.state_dict())

          def update_memory(self, state, action, reward, next_state, done):
              """Update memory with new experience"""
              self.memory.add_experience(state, action, reward, next_state, done)

      class DeepNavigationNode(Node):
          """ROS2 node for deep learning visual navigation"""
          def __init__(self):
              super().__init__('deep_navigation')

              # Publishers and subscribers
              self.image_sub = self.create_subscription(
                  Image, '/camera/rgb/image_raw', self.image_callback, 10)
              self.goal_sub = self.create_subscription(
                  Pose, '/navigation_goal', self.goal_callback, 10)
              self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
              self.reward_pub = self.create_publisher(Float64MultiArray, '/navigation_reward', 10)

              # Initialize components
              self.agent = DeepNavigationAgent()
              self.bridge = CvBridge()

              # Navigation state
              self.current_image = None
              self.current_goal = None
              self.current_pose = None
              self.navigation_active = False

              # Training parameters
              self.training_mode = True  # Set to False for deployment
              self.last_action_time = self.get_clock().now()

              self.get_logger().info('Deep Navigation Node initialized')

          def image_callback(self, msg):
              """Process incoming camera image"""
              try:
                  # Convert ROS image to OpenCV
                  cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

                  # Preprocess image for network
                  preprocessed_image = self.agent.preprocess_image(cv_image)
                  self.current_image = preprocessed_image

                  # If navigation is active, select and execute action
                  if self.navigation_active and self.current_image is not None:
                      # Prepare goal tensor if available
                      goal_tensor = None
                      if self.current_goal is not None:
                          goal_pos = np.array([
                              self.current_goal.position.x,
                              self.current_goal.position.y,
                              self.current_goal.position.z
                          ])
                          goal_tensor = goal_pos

                      # Select action
                      action_idx = self.agent.select_action(self.current_image, goal_tensor)

                      # Convert action to velocity command
                      cmd_vel = self.convert_action_to_velocity(action_idx)

                      # Publish command
                      self.cmd_vel_pub.publish(cmd_vel)

                      # Calculate reward (simplified)
                      reward = self.calculate_reward(cmd_vel)

                      reward_msg = Float64MultiArray()
                      reward_msg.data = [float(reward), float(action_idx)]
                      self.reward_pub.publish(reward_msg)

                      # Training: Update memory if in training mode
                      if self.training_mode and hasattr(self, 'prev_state'):
                          self.agent.update_memory(
                              self.prev_state, action_idx, reward,
                              self.current_image, False  # Not done
                          )
                          # Train the network
                          self.agent.train()

                      # Store current state for next iteration
                      self.prev_state = self.current_image.clone()

              except Exception as e:
                  self.get_logger().error(f'Error processing image: {e}')

          def goal_callback(self, msg):
              """Handle navigation goal"""
              self.current_goal = msg
              self.navigation_active = True
              self.get_logger().info(f'Navigation goal set: ({msg.position.x}, {msg.position.y})')

          def convert_action_to_velocity(self, action_idx):
              """Convert discrete action to velocity command"""
              cmd_vel = Twist()

              # Define actions: 0=forward, 1=left, 2=right, 3=backward
              if action_idx == 0:  # Forward
                  cmd_vel.linear.x = 0.3
              elif action_idx == 1:  # Turn left
                  cmd_vel.angular.z = 0.5
              elif action_idx == 2:  # Turn right
                  cmd_vel.angular.z = -0.5
              elif action_idx == 3:  # Backward
                  cmd_vel.linear.x = -0.2

              return cmd_vel

          def calculate_reward(self, cmd_vel):
              """Calculate reward based on action and environment"""
              # Simplified reward function
              # In real implementation, this would consider goal distance, obstacles, etc.
              reward = -0.01  # Small negative reward for time

              # Positive reward for moving toward goal (if goal is set)
              if self.current_goal is not None and self.current_pose is not None:
                  # Calculate distance to goal
                  dist_to_goal = np.sqrt(
                      (self.current_goal.position.x - self.current_pose.position.x)**2 +
                      (self.current_goal.position.y - self.current_pose.position.y)**2
                  )

                  # Reward for getting closer to goal
                  reward += max(0, 1.0 - dist_to_goal)

              # Penalty for excessive turning
              if abs(cmd_vel.angular.z) > 0.8:
                  reward -= 0.1

              return reward

      def main(args=None):
          rclpy.init(args=args)
          node = DeepNavigationNode()

          try:
              rclpy.spin(node)
          except KeyboardInterrupt:
              node.get_logger().info('Shutting down deep navigation node...')
          finally:
              node.destroy_node()
              rclpy.shutdown()

      if __name__ == '__main__':
          main()

  -
    title: "Landmark-Based Navigation System"
    description: "Implementation of landmark-based navigation using visual recognition"
    language: "python"
    framework: "OpenCV with Robotics Libraries"
    code: |
      #!/usr/bin/env python3
      """
      Landmark-Based Navigation System
      Demonstrates navigation using recognized visual landmarks
      """
      import rclpy
      from rclpy.node import Node
      from sensor_msgs.msg import Image
      from geometry_msgs.msg import PoseStamped, Twist
      from std_msgs.msg import String
      from cv_bridge import CvBridge
      import cv2
      import numpy as np
      from collections import defaultdict
      import math
      from typing import Dict, List, Tuple

      class LandmarkDetector:
          """Detect and recognize visual landmarks"""
          def __init__(self):
              # Initialize ORB detector for landmark detection
              self.detector = cv2.ORB_create(nfeatures=500)
              self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)

              # Store known landmarks
              self.known_landmarks = {}  # landmark_id: {'descriptors': descriptors, 'position': (x,y,z)}
              self.landmark_matches = {}  # frame_id: [(landmark_id, position)]

              # Initialize landmark ID counter
              self.next_landmark_id = 0

          def add_landmark(self, image, position, landmark_name=None):
              """Add a new landmark to the database"""
              keypoints, descriptors = self.detector.detectAndCompute(image, None)

              if descriptors is not None:
                  landmark_id = landmark_name or f"landmark_{self.next_landmark_id}"
                  self.known_landmarks[landmark_id] = {
                      'keypoints': keypoints,
                      'descriptors': descriptors,
                      'position': position
                  }
                  self.next_landmark_id += 1
                  return landmark_id
              return None

          def detect_landmarks(self, image):
              """Detect landmarks in the current image"""
              current_keypoints, current_descriptors = self.detector.detectAndCompute(image, None)

              if current_descriptors is None:
                  return []

              detected_landmarks = []

              for landmark_id, landmark_data in self.known_landmarks.items():
                  if landmark_data['descriptors'] is not None:
                      # Match current descriptors with landmark descriptors
                      matches = self.matcher.knnMatch(
                          current_descriptors, landmark_data['descriptors'], k=2)

                      # Apply Lowe's ratio test
                      good_matches = []
                      for match_pair in matches:
                          if len(match_pair) == 2:
                              m, n = match_pair
                              if m.distance < 0.75 * n.distance:
                                  good_matches.append(m)

                      # If enough matches, consider landmark detected
                      if len(good_matches) > 10:  # Threshold for landmark detection
                          detected_landmarks.append({
                              'id': landmark_id,
                              'position': landmark_data['position'],
                              'confidence': len(good_matches) / len(landmark_data['descriptors']) if len(landmark_data['descriptors']) > 0 else 0,
                              'matches': len(good_matches)
                          })

              return detected_landmarks

          def get_landmark_position(self, landmark_id):
              """Get the 3D position of a landmark"""
              if landmark_id in self.known_landmarks:
                  return self.known_landmarks[landmark_id]['position']
              return None

      class LandmarkBasedNavigator:
          """Navigate using detected landmarks"""
          def __init__(self):
              self.current_pose = np.array([0.0, 0.0, 0.0])  # x, y, theta
              self.detected_landmarks = {}
              self.landmark_positions = {}
              self.navigation_goals = []
              self.current_goal = None

          def update_landmark_info(self, detected_landmarks):
              """Update information about detected landmarks"""
              for landmark in detected_landmarks:
                  self.detected_landmarks[landmark['id']] = {
                      'position': landmark['position'],
                      'confidence': landmark['confidence'],
                      'timestamp': rclpy.time.Time().nanoseconds
                  }

          def estimate_pose_from_landmarks(self):
              """Estimate robot pose based on detected landmarks"""
              if not self.detected_landmarks:
                  return False

              # Simple pose estimation using triangulation from known landmarks
              # In real implementation, this would use more sophisticated methods
              estimated_positions = []

              for landmark_id, landmark_info in self.detected_landmarks.items():
                  if landmark_id in self.landmark_positions:
                      # Use landmark position and bearing to estimate robot position
                      landmark_pos = np.array(self.landmark_positions[landmark_id])
                      # This is a simplified estimation - real implementation would use
                      # bearing estimation from image and known landmark positions
                      estimated_pos = landmark_pos  # Placeholder
                      estimated_positions.append(estimated_pos)

              if estimated_positions:
                  # Average estimated positions
                  avg_position = np.mean(estimated_positions, axis=0)
                  self.current_pose[:2] = avg_position[:2]  # Update x, y
                  return True

              return False

          def set_navigation_goal(self, goal_position):
              """Set a navigation goal using landmark coordinates"""
              self.current_goal = goal_position
              self.navigation_goals.append(goal_position)

          def compute_navigation_command(self):
              """Compute navigation command based on current pose and goal"""
              if self.current_goal is None:
                  return None

              # Calculate direction to goal
              dx = self.current_goal[0] - self.current_pose[0]
              dy = self.current_goal[1] - self.current_pose[1]
              distance_to_goal = math.sqrt(dx*dx + dy*dy)

              # Simple proportional controller
              cmd_vel = Twist()

              if distance_to_goal > 0.5:  # If not close to goal
                  # Calculate desired angle
                  desired_angle = math.atan2(dy, dx)

                  # Get current orientation (simplified)
                  current_angle = self.current_pose[2]

                  # Calculate angle difference
                  angle_diff = desired_angle - current_angle
                  while angle_diff > math.pi:
                      angle_diff -= 2 * math.pi
                  while angle_diff < -math.pi:
                      angle_diff += 2 * math.pi

                  # Set velocities
                  cmd_vel.linear.x = min(distance_to_goal * 0.5, 0.3)  # Max 0.3 m/s
                  cmd_vel.angular.z = max(min(angle_diff * 2.0, 1.0), -1.0)  # Max 1.0 rad/s
              else:
                  # Reached goal
                  cmd_vel.linear.x = 0.0
                  cmd_vel.angular.z = 0.0
                  self.current_goal = None

              return cmd_vel

      class LandmarkNavigationNode(Node):
          """ROS2 node for landmark-based navigation"""
          def __init__(self):
              super().__init__('landmark_navigation')

              # Publishers and subscribers
              self.image_sub = self.create_subscription(
                  Image, '/camera/rgb/image_raw', self.image_callback, 10)
              self.goal_sub = self.create_subscription(
                  PoseStamped, '/landmark_goal', self.goal_callback, 10)
              self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
              self.pose_pub = self.create_publisher(PoseStamped, '/landmark_pose', 10)
              self.landmark_pub = self.create_publisher(String, '/detected_landmarks', 10)

              # Initialize components
              self.landmark_detector = LandmarkDetector()
              self.navigator = LandmarkBasedNavigator()
              self.bridge = CvBridge()

              # Add some example landmarks (in a real system, these would be added during mapping)
              self.add_example_landmarks()

              # Navigation state
              self.navigation_active = False

              self.get_logger().info('Landmark Navigation Node initialized')

          def add_example_landmarks(self):
              """Add example landmarks for demonstration"""
              # In a real system, landmarks would be added during an initial mapping phase
              self.landmark_detector.add_landmark(
                  np.zeros((100, 100, 3), dtype=np.uint8),  # Placeholder image
                  (5.0, 0.0, 0.0),  # Position (x, y, z)
                  "landmark_001"
              )
              self.landmark_detector.add_landmark(
                  np.zeros((100, 100, 3), dtype=np.uint8),  # Placeholder image
                  (0.0, 5.0, 0.0),  # Position (x, y, z)
                  "landmark_002"
              )
              self.landmark_detector.add_landmark(
                  np.zeros((100, 100, 3), dtype=np.uint8),  # Placeholder image
                  (-5.0, 0.0, 0.0),  # Position (x, y, z)
                  "landmark_003"
              )

          def image_callback(self, msg):
              """Process incoming camera image for landmark detection"""
              try:
                  # Convert ROS image to OpenCV
                  cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

                  # Detect landmarks in the image
                  detected_landmarks = self.landmark_detector.detect_landmarks(cv_image)

                  # Update navigator with detected landmarks
                  self.navigator.update_landmark_info(detected_landmarks)

                  # Estimate pose from landmarks
                  pose_estimated = self.navigator.estimate_pose_from_landmarks()

                  # If navigation is active, compute and publish navigation command
                  if self.navigation_active:
                      cmd_vel = self.navigator.compute_navigation_command()
                      if cmd_vel:
                          self.cmd_vel_pub.publish(cmd_vel)

                  # Publish detected landmarks info
                  if detected_landmarks:
                      landmark_info = f"Detected: {len(detected_landmarks)} landmarks: "
                      for lm in detected_landmarks:
                          landmark_info += f"{lm['id']}({lm['confidence']:.2f}) "

                      landmark_msg = String()
                      landmark_msg.data = landmark_info
                      self.landmark_pub.publish(landmark_msg)

                  # Publish estimated pose
                  if pose_estimated:
                      pose_msg = PoseStamped()
                      pose_msg.header.stamp = self.get_clock().now().to_msg()
                      pose_msg.header.frame_id = 'map'

                      pose_msg.pose.position.x = float(self.navigator.current_pose[0])
                      pose_msg.pose.position.y = float(self.navigator.current_pose[1])
                      pose_msg.pose.position.z = 0.0  # Assume flat ground

                      # Convert theta to quaternion (simplified)
                      theta = self.navigator.current_pose[2]
                      pose_msg.pose.orientation.z = math.sin(theta / 2)
                      pose_msg.pose.orientation.w = math.cos(theta / 2)

                      self.pose_pub.publish(pose_msg)

              except Exception as e:
                  self.get_logger().error(f'Error in landmark navigation: {e}')

          def goal_callback(self, msg):
              """Handle navigation goal"""
              goal_position = np.array([
                  msg.pose.position.x,
                  msg.pose.position.y,
                  msg.pose.position.z
              ])

              self.navigator.set_navigation_goal(goal_position)
              self.navigation_active = True

              self.get_logger().info(f'Landmark navigation goal set: ({goal_position[0]}, {goal_position[1]})')

      def main(args=None):
          rclpy.init(args=args)
          node = LandmarkNavigationNode()

          try:
              rclpy.spin(node)
          except KeyboardInterrupt:
              node.get_logger().info('Shutting down landmark navigation node...')
          finally:
              node.destroy_node()
              rclpy.shutdown()

      if __name__ == '__main__':
          main()

practical_examples:
  -
    title: "Visual Navigation in Indoor Environment"
    description: "Students implement a complete visual navigation system for indoor environments using SLAM and landmark recognition."
    objectives:
      - "Implement visual SLAM for mapping"
      - "Create landmark recognition system"
      - "Develop path planning using visual information"
      - "Test navigation in simulated indoor environment"
    required_components:
      - "Camera sensor"
      - "SLAM algorithms"
      - "Feature detection libraries"
      - "Path planning algorithms"
      - "Simulation environment"
    evaluation_criteria:
      - "Mapping accuracy"
      - "Navigation success rate"
      - "Localization precision"
      - "System robustness"
  -
    title: "Deep Learning Navigation System"
    description: "Students develop an end-to-end deep learning navigation system that learns to navigate from visual input."
    objectives:
      - "Implement CNN for visual processing"
      - "Create reinforcement learning agent"
      - "Train navigation policy"
      - "Validate performance in simulation"
    required_components:
      - "Deep learning framework"
      - "Reinforcement learning algorithms"
      - "Simulation environment"
      - "Training infrastructure"
      - "Performance evaluation tools"
    evaluation_criteria:
      - "Learning effectiveness"
      - "Navigation success rate"
      - "Generalization to new environments"
      - "Training efficiency"
  -
    title: "Vision-Based Obstacle Avoidance"
    description: "Students create a vision-based obstacle avoidance system that can navigate around dynamic obstacles."
    objectives:
      - "Implement real-time obstacle detection"
      - "Create dynamic path replanning"
      - "Develop reactive avoidance behaviors"
      - "Test with moving obstacles"
    required_components:
      - "Stereo or depth camera"
      - "Obstacle detection algorithms"
      - "Path planning system"
      - "Motion control system"
      - "Obstacle simulation tools"
    evaluation_criteria:
      - "Obstacle detection accuracy"
      - "Avoidance success rate"
      - "Navigation efficiency"
      - "Safety in dynamic environments"

summary: "Chapter 18 provided comprehensive examples of vision-based navigation, covering visual SLAM, deep learning navigation, and landmark-based navigation systems. Students learned to implement visual navigation techniques that enable robots to perceive their environment and navigate autonomously using only camera inputs. The chapter emphasized both classical computer vision approaches and modern deep learning methods for creating robust navigation systems."

quiz:
  -
    question: "What does SLAM stand for in robotics?"
    options:
      - A: Simultaneous Localization and Mapping
      - B: Simulated Learning and Mapping
      - C: Sensor Localization and Mapping
      - D: Sequential Learning and Mapping
    correct_answer: "A"
    explanation: "SLAM stands for Simultaneous Localization and Mapping, a technique for building a map of an unknown environment while tracking the robot's location."
  -
    question: "What is visual odometry?"
    options:
      - A: Measuring distance using visual sensors
      - B: Estimating motion from visual sequences
      - C: Camera calibration technique
      - D: Visual object detection
    correct_answer: "B"
    explanation: "Visual odometry estimates the motion of a camera by analyzing the movement of features in consecutive images."
  -
    question: "Why are landmarks important in visual navigation?"
    options:
      - A: They make the robot move faster
      - B: They provide reference points for localization and navigation
      - C: They reduce power consumption
      - D: They eliminate the need for programming
    correct_answer: "B"
    explanation: "Landmarks provide reference points that help robots localize themselves and navigate to goals in the environment."
  -
    question: "What is a key advantage of deep learning approaches to visual navigation?"
    options:
      - A: They require less computational power
      - B: They can learn complex navigation behaviors from experience
      - C: They are always more accurate than classical methods
      - D: They eliminate the need for sensors
    correct_answer: "B"
    explanation: "Deep learning approaches can learn complex navigation behaviors and adapt to various environments through experience."
  -
    question: "What is the main challenge in vision-based navigation?"
    options:
      - A: Too many sensors required
      - B: Lighting changes and dynamic environments can affect performance
      - C: Robots become too fast
      - D: Colors become too bright
    correct_answer: "B"
    explanation: "Lighting changes, dynamic environments, and visual ambiguities can make vision-based navigation challenging."

module_learning_outcomes:
  - "Implement multimodal learning systems"
  - "Integrate vision, language, and action components"
  - "Develop interactive learning algorithms"
  - "Create human-robot interaction systems"

prerequisites:
  - "Basic understanding of Python programming"
  - "Fundamentals of linear algebra and calculus"
  - "Basic knowledge of robotics concepts"
  - "Introduction to machine learning concepts"
  - "Completion of Module 0 (Introduction and Foundations)"
  - "Completion of Chapter 01 (Physical AI Basics)"
  - "Completion of Chapter 03 (ROS2 Nodes, Topics & Services)"
  - "Completion of Chapter 16 (Vision-Language-Action Concepts)"

estimated_duration: "6 hours"
...