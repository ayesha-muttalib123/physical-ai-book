---
chapter_id: "14-integration-with-ros2"
title: "Integration with ROS2"
module_id: "M4-isaac-brain"
module_title: "NVIDIA Isaac Brain"
overview: "This chapter covers the integration between NVIDIA Isaac and ROS2, demonstrating how to leverage Isaac's GPU-accelerated capabilities within the ROS2 ecosystem. Students will learn to use Isaac ROS packages, bridge Isaac Sim with ROS2 for hardware-in-the-loop testing, and implement GPU-accelerated perception and navigation nodes. The chapter explores Isaac's ROS2 compatibility layers, message translation, and how to build hybrid systems that combine Isaac's specialized capabilities with ROS2's extensive tooling and community support."
why_it_matters: "Integration with ROS2 is crucial for leveraging both NVIDIA Isaac's GPU-accelerated capabilities and ROS2's mature ecosystem of tools, packages, and community support. This integration allows developers to build robotics systems that benefit from Isaac's high-performance perception and simulation while maintaining compatibility with the vast ROS2 package repository. Understanding this integration enables the creation of more robust, efficient, and maintainable robotics applications."
key_concepts:
  - "Isaac ROS Packages: GPU-accelerated perception and navigation packages"
  - "ROS2 Bridge: Connecting Isaac Sim with ROS2 for HIL testing"
  - "Message Translation: Converting between Isaac and ROS2 message formats"
  - "GPU-Accelerated Nodes: Implementing CUDA/TensorRT accelerated ROS2 nodes"
  - "Hybrid Architecture: Combining Isaac and ROS2 components"
  - "Hardware-in-the-Loop: Testing with real hardware connected to simulation"
  - "Performance Optimization: Maximizing GPU utilization in ROS2 systems"
  - "System Integration: Building complete robotics solutions"

code_examples:
  -
    title: "Isaac ROS Perception Node"
    description: "GPU-accelerated perception node using Isaac ROS packages for object detection and segmentation"
    language: "python"
    framework: "ROS2 with Isaac ROS"
    code: |
      #!/usr/bin/env python3
      """
      Isaac ROS Perception Node
      Demonstrates GPU-accelerated perception using Isaac ROS packages
      """
      import rclpy
      from rclpy.node import Node
      from sensor_msgs.msg import Image, CameraInfo
      from vision_msgs.msg import Detection2DArray, Detection2D, BoundingBox2D
      from geometry_msgs.msg import Point
      from std_msgs.msg import Header
      import cv2
      import numpy as np
      from cv_bridge import CvBridge
      import torch
      import torchvision.transforms as transforms
      import cuda
      import tensorrt as trt
      import pycuda.driver as cuda_driver
      import pycuda.autoinit

      class IsaacROSPerceptionNode(Node):
          def __init__(self):
              super().__init__('isaac_ros_perception')

              # Publishers and subscribers
              self.image_sub = self.create_subscription(
                  Image, '/camera/rgb/image_raw', self.image_callback, 10)
              self.camera_info_sub = self.create_subscription(
                  CameraInfo, '/camera/rgb/camera_info', self.camera_info_callback, 10)
              self.detections_pub = self.create_publisher(
                  Detection2DArray, '/isaac_ros/detections', 10)
              self.segmentation_pub = self.create_publisher(
                  Image, '/isaac_ros/segmentation', 10)

              # Initialize OpenCV bridge
              self.bridge = CvBridge()

              # Camera parameters
              self.camera_matrix = None
              self.dist_coeffs = None

              # Initialize GPU-accelerated models
              self.initialize_gpu_models()

              # Performance metrics
              self.frame_count = 0
              self.start_time = self.get_clock().now().nanoseconds / 1e9

              self.get_logger().info('Isaac ROS Perception Node initialized')

          def initialize_gpu_models(self):
              """Initialize GPU-accelerated models for perception"""
              try:
                  # Initialize TensorRT engine for object detection
                  self.detection_engine = self.load_tensorrt_engine('/path/to/detection_model.plan')

                  # Initialize segmentation model
                  self.segmentation_engine = self.load_tensorrt_engine('/path/to/segmentation_model.plan')

                  # Allocate GPU memory for inference
                  self.allocate_gpu_memory()

                  self.get_logger().info('GPU models initialized successfully')
              except Exception as e:
                  self.get_logger().error(f'Failed to initialize GPU models: {e}')
                  # Fallback to CPU models
                  self.use_cpu_fallback = True

          def load_tensorrt_engine(self, engine_path):
              """Load TensorRT engine for GPU inference"""
              try:
                  with open(engine_path, 'rb') as f:
                      engine_data = f.read()
                  runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))
                  engine = runtime.deserialize_cuda_engine(engine_data)
                  return engine
              except Exception as e:
                  self.get_logger().error(f'Failed to load TensorRT engine: {e}')
                  return None

          def allocate_gpu_memory(self):
              """Allocate GPU memory for inference"""
              # This is a simplified example - in practice, you would allocate
              # memory for input/output tensors based on model requirements
              self.gpu_input_buffer = cuda_driver.mem_alloc(3 * 224 * 224 * 4)  # RGB, 224x224, float32
              self.gpu_output_buffer = cuda_driver.mem_alloc(1000 * 4)  # 1000 classes, float32

              # Create CUDA stream
              self.cuda_stream = cuda_driver.Stream()

          def camera_info_callback(self, msg):
              """Update camera parameters from camera info"""
              self.camera_matrix = np.array(msg.k).reshape(3, 3)
              self.dist_coeffs = np.array(msg.d)

          def image_callback(self, msg):
              """Process incoming camera image with GPU acceleration"""
              try:
                  # Convert ROS image to OpenCV
                  cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

                  # Record start time for performance measurement
                  start_time = self.get_clock().now().nanoseconds / 1e9

                  # Run GPU-accelerated object detection
                  detections = self.run_gpu_detection(cv_image)

                  # Run GPU-accelerated segmentation
                  segmentation = self.run_gpu_segmentation(cv_image)

                  # Calculate processing time
                  end_time = self.get_clock().now().nanoseconds / 1e9
                  processing_time = (end_time - start_time) * 1000  # ms

                  # Publish results
                  self.publish_detections(detections, msg.header)
                  self.publish_segmentation(segmentation, msg.header)

                  # Update and log performance metrics
                  self.frame_count += 1
                  current_time = self.get_clock().now().nanoseconds / 1e9
                  elapsed_time = current_time - self.start_time
                  fps = self.frame_count / elapsed_time if elapsed_time > 0 else 0

                  if self.frame_count % 30 == 0:  # Log every 30 frames
                      self.get_logger().info(
                          f'Perception processing: {processing_time:.2f}ms, '
                          f'FPS: {fps:.2f}, Detections: {len(detections) if detections else 0}'
                      )

              except Exception as e:
                  self.get_logger().error(f'Error processing image: {e}')

          def run_gpu_detection(self, image):
              """Run GPU-accelerated object detection"""
              try:
                  # Preprocess image for model input
                  input_tensor = self.preprocess_image(image)

                  # In Isaac ROS, this would use Isaac's optimized detection pipelines
                  # For this example, we'll simulate the GPU processing

                  # Simulate GPU inference (in real Isaac, this would use TensorRT)
                  height, width = image.shape[:2]

                  # Simulate detection results
                  detections = []
                  num_detections = np.random.poisson(2)  # Random number of detections

                  for i in range(num_detections):
                      # Generate random detection
                      x = np.random.uniform(0, width - 50)
                      y = np.random.uniform(0, height - 50)
                      w = np.random.uniform(30, 100)
                      h = np.random.uniform(30, 100)

                      detection = {
                          'bbox': [x, y, w, h],
                          'confidence': np.random.uniform(0.6, 0.99),
                          'class_id': np.random.randint(0, 80),
                          'class_name': f'object_{np.random.randint(0, 5)}'
                      }
                      detections.append(detection)

                  return detections

              except Exception as e:
                  self.get_logger().error(f'GPU detection error: {e}')
                  return []

          def run_gpu_segmentation(self, image):
              """Run GPU-accelerated semantic segmentation"""
              try:
                  # In Isaac ROS, this would use GPU-accelerated segmentation
                  # For simulation, create a segmentation mask
                  height, width = image.shape[:2]

                  # Simulate segmentation mask
                  segmentation_mask = np.zeros((height, width), dtype=np.uint8)

                  # Add some segmented regions
                  for _ in range(3):  # 3 random regions
                      center_x = np.random.randint(width//4, 3*width//4)
                      center_y = np.random.randint(height//4, 3*height//4)
                      radius = np.random.randint(20, 50)

                      y, x = np.ogrid[:height, :width]
                      mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2
                      segmentation_mask[mask] = np.random.randint(1, 10)  # Random class ID

                  return segmentation_mask

              except Exception as e:
                  self.get_logger().error(f'GPU segmentation error: {e}')
                  return np.zeros_like(image.shape[:2])

          def preprocess_image(self, image):
              """Preprocess image for GPU inference"""
              # Resize image to model input size
              input_height, input_width = 640, 640  # Example model input size
              resized = cv2.resize(image, (input_width, input_height))

              # Convert BGR to RGB
              rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)

              # Normalize pixel values
              normalized = rgb.astype(np.float32) / 255.0

              # Convert to CHW format (channels, height, width)
              chw = np.transpose(normalized, (2, 0, 1))

              return chw

          def publish_detections(self, detections, header):
              """Publish detection results to ROS2 topic"""
              if not detections:
                  return

              detection_array = Detection2DArray()
              detection_array.header = header

              for det in detections:
                  detection_2d = Detection2D()
                  detection_2d.header = header

                  # Set bounding box
                  bbox = BoundingBox2D()
                  bbox.center.x = det['bbox'][0] + det['bbox'][2] / 2  # center x
                  bbox.center.y = det['bbox'][1] + det['bbox'][3] / 2  # center y
                  bbox.center.theta = 0.0
                  bbox.size_x = det['bbox'][2]  # width
                  bbox.size_y = det['bbox'][3]  # height
                  detection_2d.bbox = bbox

                  # Set confidence
                  detection_2d.results = []

                  detection_array.detections.append(detection_2d)

              self.detections_pub.publish(detection_array)

          def publish_segmentation(self, segmentation_mask, header):
              """Publish segmentation results to ROS2 topic"""
              try:
                  # Convert segmentation mask to ROS image message
                  seg_msg = self.bridge.cv2_to_imgmsg(segmentation_mask, encoding='mono8')
                  seg_msg.header = header
                  self.segmentation_pub.publish(seg_msg)
              except Exception as e:
                  self.get_logger().error(f'Error publishing segmentation: {e}')

      def main(args=None):
          rclpy.init(args=args)
          node = IsaacROSPerceptionNode()

          try:
              rclpy.spin(node)
          except KeyboardInterrupt:
              node.get_logger().info('Shutting down Isaac ROS perception node...')
          finally:
              node.destroy_node()
              rclpy.shutdown()

      if __name__ == '__main__':
          main()

  -
    title: "Isaac Sim to ROS2 Bridge"
    description: "Implementation of a bridge between Isaac Sim and ROS2 for hardware-in-the-loop testing"
    language: "python"
    framework: "ROS2 with Isaac Sim"
    code: |
      #!/usr/bin/env python3
      """
      Isaac Sim to ROS2 Bridge
      Demonstrates connecting Isaac Sim with ROS2 for hardware-in-the-loop testing
      """
      import rclpy
      from rclpy.node import Node
      from geometry_msgs.msg import Twist, Pose, PoseStamped, Point
      from nav_msgs.msg import Odometry
      from sensor_msgs.msg import Image, LaserScan, CameraInfo, Imu
      from std_msgs.msg import Header, Float64, String
      import numpy as np
      import time
      import threading
      import queue
      from typing import Dict, Any

      # Isaac Sim imports (these would be available in Isaac Sim environment)
      try:
          import omni
          from omni.isaac.core import World
          from omni.isaac.core.utils.stage import add_reference_to_stage
          from omni.isaac.core.utils.nucleus import get_assets_root_path
          from omni.isaac.core.utils.prims import get_prim_at_path
          from omni.isaac.core.robots import Robot
          from omni.isaac.core.utils.viewports import set_camera_view
          from omni.isaac.sensor import Camera, LidarRtx
          import carb
      except ImportError:
          # For simulation/testing when Isaac Sim is not available
          carb = None
          World = None
          print("Isaac Sim modules not available - running in simulation mode")

      class IsaacSimROS2Bridge(Node):
          def __init__(self):
              super().__init__('isaac_sim_ros2_bridge')

              # Publishers for simulated sensor data from Isaac Sim
              self.odom_pub = self.create_publisher(Odometry, '/sim_robot/odom', 10)
              self.camera_pub = self.create_publisher(Image, '/sim_robot/camera/image_raw', 10)
              self.lidar_pub = self.create_publisher(LaserScan, '/sim_robot/lidar/scan', 10)
              self.imu_pub = self.create_publisher(Imu, '/sim_robot/imu', 10)
              self.status_pub = self.create_publisher(String, '/sim_robot/status', 10)

              # Subscribers for commands from ROS2
              self.cmd_vel_sub = self.create_subscription(
                  Twist, '/sim_robot/cmd_vel', self.cmd_vel_callback, 10)

              # Timer for publishing simulated data
              self.publish_timer = self.create_timer(0.05, self.publish_sensor_data)  # 20Hz

              # Robot state
              self.robot_pose = {'x': 0.0, 'y': 0.0, 'theta': 0.0}
              self.robot_twist = {'linear_x': 0.0, 'angular_z': 0.0}
              self.sim_time = 0.0
              self.last_update_time = time.time()

              # Isaac Sim world (will be initialized if available)
              self.isaac_world = None
              self.isaac_robot = None
              self.isaac_camera = None
              self.isaac_lidar = None

              # Initialize Isaac Sim if available
              self.initialize_isaac_sim()

              self.get_logger().info('Isaac Sim to ROS2 Bridge initialized')

          def initialize_isaac_sim(self):
              """Initialize Isaac Sim if available"""
              if World is None:
                  self.get_logger().warn('Isaac Sim not available - running in simulation mode')
                  return

              try:
                  # Initialize Isaac Sim world
                  self.isaac_world = World(stage_units_in_meters=1.0, physics_dt=1.0/60.0, rendering_dt=1.0/60.0)

                  # Add ground plane
                  self.isaac_world.scene.add_default_ground_plane()

                  # Load robot model (simplified - in real use, load actual robot USD)
                  # For this example, we'll create a simple dynamic object
                  from omni.isaac.core.objects import DynamicCuboid
                  self.isaac_robot = self.isaac_world.scene.add(
                      DynamicCuboid(
                          prim_path="/World/Robot",
                          name="sim_robot",
                          position=np.array([0.0, 0.0, 0.1]),
                          size=np.array([0.3, 0.3, 0.2]),
                          color=np.array([0.1, 0.1, 0.8])
                      )
                  )

                  # Add sensors to robot
                  self.setup_sim_sensors()

                  # Reset the world
                  self.isaac_world.reset()

                  self.get_logger().info('Isaac Sim initialized successfully')

              except Exception as e:
                  self.get_logger().error(f'Failed to initialize Isaac Sim: {e}')

          def setup_sim_sensors(self):
              """Setup simulated sensors in Isaac Sim"""
              if self.isaac_world is None:
                  return

              try:
                  # Add camera sensor
                  self.isaac_camera = self.isaac_world.scene.add(
                      Camera(
                          prim_path="/World/Robot/Camera",
                          name="sim_camera",
                          position=np.array([0.15, 0.0, 0.1]),
                          frequency=20,  # 20Hz
                          resolution=(640, 480)
                      )
                  )
                  self.isaac_camera.initialize()
                  self.isaac_camera.add_render_product("/World/Robot/Camera", [640, 480])

                  # Add LIDAR sensor (simplified)
                  # Note: In real Isaac Sim, you would use LidarRtx or similar
                  self.get_logger().info('Simulated sensors added to Isaac Sim')

              except Exception as e:
                  self.get_logger().error(f'Failed to setup sim sensors: {e}')

          def cmd_vel_callback(self, msg):
              """Handle velocity commands from ROS2"""
              # Update robot velocities based on ROS2 command
              self.robot_twist['linear_x'] = msg.linear.x
              self.robot_twist['angular_z'] = msg.angular.z

              # If Isaac Sim is available, apply command to simulated robot
              if self.isaac_robot is not None:
                  self.apply_robot_command()

          def apply_robot_command(self):
              """Apply velocity command to Isaac Sim robot"""
              if self.isaac_world is None or self.isaac_robot is None:
                  return

              try:
                  # Get current robot state
                  current_pos, current_ori = self.isaac_robot.get_world_pose()

                  # Calculate new position based on velocities
                  dt = 0.05  # 20Hz update
                  linear_vel = self.robot_twist['linear_x']
                  angular_vel = self.robot_twist['angular_z']

                  # Update orientation
                  current_yaw = np.arctan2(
                      2 * (current_ori[3] * current_ori[2] + current_ori[0] * current_ori[1]),
                      1 - 2 * (current_ori[1]**2 + current_ori[2]**2)
                  )
                  new_yaw = current_yaw + angular_vel * dt

                  # Update position
                  new_x = current_pos[0] + linear_vel * np.cos(new_yaw) * dt
                  new_y = current_pos[1] + linear_vel * np.sin(new_yaw) * dt

                  # Update robot pose in Isaac Sim
                  self.isaac_robot.set_world_pose(
                      position=np.array([new_x, new_y, current_pos[2]]),
                      orientation=np.array([0, 0, np.sin(new_yaw/2), np.cos(new_yaw/2)])
                  )

                  # Update local state for publishing
                  self.robot_pose['x'] = new_x
                  self.robot_pose['y'] = new_y
                  self.robot_pose['theta'] = new_yaw

              except Exception as e:
                  self.get_logger().error(f'Error applying robot command: {e}')

          def update_simulation(self):
              """Update simulation state"""
              if self.isaac_world is not None:
                  # Step Isaac Sim world
                  self.isaac_world.step(render=True)
              else:
                  # Simulated physics update
                  dt = 0.05  # 20Hz update

                  # Update robot pose based on current velocities
                  linear_vel = self.robot_twist['linear_x']
                  angular_vel = self.robot_twist['angular_z']

                  # Update orientation
                  new_theta = self.robot_pose['theta'] + angular_vel * dt

                  # Update position
                  new_x = self.robot_pose['x'] + linear_vel * np.cos(new_theta) * dt
                  new_y = self.robot_pose['y'] + linear_vel * np.sin(new_theta) * dt

                  # Update state
                  self.robot_pose['x'] = new_x
                  self.robot_pose['y'] = new_y
                  self.robot_pose['theta'] = new_theta

          def publish_sensor_data(self):
              """Publish simulated sensor data to ROS2 topics"""
              # Update simulation state
              self.update_simulation()

              # Update simulation time
              current_time = time.time()
              dt = current_time - self.last_update_time
              self.sim_time += dt
              self.last_update_time = current_time

              # Create header
              header = Header()
              header.stamp = self.get_clock().now().to_msg()
              header.frame_id = 'base_link'

              # Publish odometry
              self.publish_odometry(header)

              # Publish camera data
              self.publish_camera_data(header)

              # Publish LIDAR data
              self.publish_lidar_data(header)

              # Publish IMU data
              self.publish_imu_data(header)

              # Publish status
              self.publish_status(header)

          def publish_odometry(self, header):
              """Publish odometry data"""
              odom = Odometry()
              odom.header = header
              odom.child_frame_id = 'base_link'

              # Set position
              odom.pose.pose.position.x = self.robot_pose['x']
              odom.pose.pose.position.y = self.robot_pose['y']
              odom.pose.pose.position.z = 0.0

              # Convert theta to quaternion
              from math import sin, cos
              siny_cosp = 2 * (1 * sin(self.robot_pose['theta']/2))
              cosy_cosp = 1 - 2 * (sin(self.robot_pose['theta']/2) ** 2)
              yaw = np.arctan2(siny_cosp, cosy_cosp)

              odom.pose.pose.orientation.z = sin(self.robot_pose['theta'] / 2)
              odom.pose.pose.orientation.w = cos(self.robot_pose['theta'] / 2)

              # Set velocities
              odom.twist.twist.linear.x = self.robot_twist['linear_x']
              odom.twist.twist.angular.z = self.robot_twist['angular_z']

              self.odom_pub.publish(odom)

          def publish_camera_data(self, header):
              """Publish simulated camera data"""
              # In Isaac Sim, this would get real camera data
              # For simulation, create a dummy image
              width, height = 640, 480

              # Create a simple test image
              image_data = np.zeros((height, width, 3), dtype=np.uint8)

              # Add some features to the image
              cv2.rectangle(image_data, (100, 100), (200, 200), (255, 0, 0), 2)  # Blue square
              cv2.circle(image_data, (400, 300), 50, (0, 255, 0), 2)  # Green circle

              # Convert to ROS Image message
              from cv_bridge import CvBridge
              bridge = CvBridge()
              image_msg = bridge.cv2_to_imgmsg(image_data, encoding='bgr8')
              image_msg.header = header

              self.camera_pub.publish(image_msg)

          def publish_lidar_data(self, header):
              """Publish simulated LIDAR data"""
              # Create simulated LIDAR scan
              scan = LaserScan()
              scan.header = header
              scan.angle_min = -np.pi
              scan.angle_max = np.pi
              scan.angle_increment = 2 * np.pi / 360  # 360 points
              scan.time_increment = 0.0
              scan.scan_time = 0.1  # 10Hz
              scan.range_min = 0.1
              scan.range_max = 10.0

              # Generate simulated ranges (with some obstacles)
              num_ranges = 360
              ranges = []

              for i in range(num_ranges):
                  angle = scan.angle_min + i * scan.angle_increment

                  # Simulate some obstacles
                  distance = scan.range_max  # Default to max range

                  # Add some obstacles at specific angles
                  if 80 < i < 100:  # Front left
                      distance = 2.0
                  elif 260 < i < 280:  # Rear right
                      distance = 1.5
                  elif 170 < i < 190:  # Directly behind
                      distance = 0.8

                  ranges.append(distance)

              scan.ranges = ranges
              scan.intensities = [100.0] * num_ranges  # Constant intensity

              self.lidar_pub.publish(scan)

          def publish_imu_data(self, header):
              """Publish simulated IMU data"""
              imu = Imu()
              imu.header = header

              # Set orientation (simplified)
              imu.orientation.z = np.sin(self.robot_pose['theta'] / 2)
              imu.orientation.w = np.cos(self.robot_pose['theta'] / 2)

              # Set angular velocity (based on robot's angular velocity)
              imu.angular_velocity.z = self.robot_twist['angular_z']

              # Set linear acceleration (simplified)
              imu.linear_acceleration.x = self.robot_twist['linear_x'] * 2.0  # Assume some acceleration

              self.imu_pub.publish(imu)

          def publish_status(self, header):
              """Publish simulation status"""
              status_msg = String()
              status_msg.data = f"Running - Pose:({self.robot_pose['x']:.2f},{self.robot_pose['y']:.2f},{self.robot_pose['theta']:.2f})"
              self.status_pub.publish(status_msg)

          def cleanup(self):
              """Clean up resources"""
              if self.isaac_world is not None:
                  self.isaac_world.clear()
              self.get_logger().info('Isaac Sim to ROS2 Bridge cleaned up')

      def main(args=None):
          rclpy.init(args=args)
          node = IsaacSimROS2Bridge()

          try:
              rclpy.spin(node)
          except KeyboardInterrupt:
              node.get_logger().info('Shutting down Isaac Sim to ROS2 bridge...')
          finally:
              node.cleanup()
              node.destroy_node()
              rclpy.shutdown()

      if __name__ == '__main__':
          main()

  -
    title: "Isaac ROS Navigation Stack"
    description: "GPU-accelerated navigation stack combining Isaac's capabilities with ROS2 navigation framework"
    language: "python"
    framework: "ROS2 with Isaac ROS Navigation"
    code: |
      #!/usr/bin/env python3
      """
      Isaac ROS Navigation Stack
      GPU-accelerated navigation combining Isaac capabilities with ROS2
      """
      import rclpy
      from rclpy.node import Node
      from geometry_msgs.msg import PoseStamped, Twist, Point
      from nav_msgs.msg import Path, OccupancyGrid, Odometry
      from sensor_msgs.msg import LaserScan, Image
      from visualization_msgs.msg import Marker, MarkerArray
      from std_msgs.msg import Header, Bool, Float64
      import numpy as np
      import math
      import heapq
      from typing import List, Tuple
      import time

      class IsaacROSNavigationStack(Node):
          def __init__(self):
              super().__init__('isaac_ros_navigation')

              # Publishers
              self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
              self.local_plan_pub = self.create_publisher(Path, '/local_plan', 10)
              self.global_plan_pub = self.create_publisher(Path, '/global_plan', 10)
              self.path_marker_pub = self.create_publisher(MarkerArray, '/path_markers', 10)
              self.status_pub = self.create_publisher(Bool, '/navigation_active', 10)
              self.progress_pub = self.create_publisher(Float64, '/navigation_progress', 10)

              # Subscribers
              self.goal_sub = self.create_subscription(
                  PoseStamped, '/move_base_simple/goal', self.goal_callback, 10)
              self.odom_sub = self.create_subscription(
                  Odometry, '/odom', self.odom_callback, 10)
              self.scan_sub = self.create_subscription(
                  LaserScan, '/scan', self.scan_callback, 10)
              self.map_sub = self.create_subscription(
                  OccupancyGrid, '/map', self.map_callback, 10)

              # Timer for navigation control
              self.nav_timer = self.create_timer(0.05, self.navigation_callback)  # 20Hz

              # Navigation state
              self.current_pose = None
              self.current_twist = None
              self.goal_pose = None
              self.global_path = []
              self.local_path = []
              self.navigation_active = False
              self.path_index = 0
              self.obstacle_threshold = 0.5  # meters

              # Isaac-specific navigation parameters
              self.isaac_nav_params = {
                  'global_planner': 'isaac_global_planner',
                  'local_planner': 'isaac_local_planner',
                  'costmap_resolution': 0.05,  # meters per cell
                  'robot_radius': 0.3,  # meters
                  'max_linear_vel': 0.5,  # m/s
                  'max_angular_vel': 1.0,  # rad/s
                  'min_obstacle_dist': 0.5,  # meters
                  'path_lookahead': 1.0  # meters
              }

              # Costmap for obstacle avoidance
              self.costmap = None
              self.map_origin = None
              self.map_resolution = None

              # Performance metrics
              self.start_time = None
              self.path_length = 0.0

              self.get_logger().info('Isaac ROS Navigation Stack initialized')

          def goal_callback(self, msg):
              """Handle navigation goal from ROS2"""
              self.goal_pose = msg.pose
              self.navigation_active = True
              self.start_time = time.time()
              self.path_length = 0.0

              self.get_logger().info(f'Navigation goal received: ({msg.pose.position.x:.2f}, {msg.pose.position.y:.2f})')

              # Plan global path
              if self.current_pose and self.costmap is not None:
                  self.plan_global_path()

          def odom_callback(self, msg):
              """Update robot pose from odometry"""
              self.current_pose = msg.pose.pose
              self.current_twist = msg.twist.twist

          def scan_callback(self, msg):
              """Process laser scan data"""
              # Update local costmap with scan data
              self.update_local_costmap(msg)

          def map_callback(self, msg):
              """Process global map data"""
              self.costmap = np.array(msg.data).reshape(msg.info.height, msg.info.width)
              self.map_origin = (msg.info.origin.position.x, msg.info.origin.position.y)
              self.map_resolution = msg.info.resolution

              self.get_logger().info(f'Global map received: {msg.info.width}x{msg.info.height}, res: {self.map_resolution}')

          def navigation_callback(self):
              """Main navigation control loop"""
              if not self.navigation_active or not self.current_pose or not self.goal_pose:
                  # Stop robot if navigation is not active
                  if self.navigation_active:
                      self.stop_robot()
                      self.navigation_active = False
                  return

              # Check if goal is reached
              if self.is_goal_reached():
                  self.get_logger().info('Navigation goal reached!')
                  self.navigation_complete()
                  return

              # Plan local path if needed
              if not self.local_path:
                  self.plan_local_path()

              # Execute local path following
              if self.local_path:
                  cmd_vel = self.follow_local_path()
                  if cmd_vel:
                      self.cmd_vel_pub.publish(cmd_vel)
                      self.publish_status(True)
                  else:
                      # Could not follow path, stop and replan
                      self.stop_robot()
                      self.plan_global_path()
              else:
                  # No path available, stop robot
                  self.stop_robot()

          def plan_global_path(self):
              """Plan global path using Isaac's GPU-accelerated path planner (simulated)"""
              if not self.current_pose or not self.goal_pose or self.costmap is None:
                  return

              # Convert poses to map coordinates
              start = self.pose_to_map_coords(self.current_pose)
              goal = self.pose_to_map_coords(self.goal_pose)

              # In Isaac, this would use GPU-accelerated A* or Dijkstra's algorithm
              # For simulation, we'll use a simplified A* implementation
              path = self.a_star_search(start, goal)

              if path:
                  # Convert path back to world coordinates
                  self.global_path = [self.map_coords_to_world(p) for p in path]
                  self.path_index = 0

                  # Publish global plan
                  self.publish_global_path()

                  # Plan local path
                  self.plan_local_path()

                  self.get_logger().info(f'Global path planned with {len(path)} waypoints')
              else:
                  self.get_logger().error('Failed to find global path to goal')
                  self.navigation_active = False

          def plan_local_path(self):
              """Plan local path using Isaac's local planner"""
              if not self.global_path or self.path_index >= len(self.global_path):
                  return

              # Get waypoints ahead in global path
              current_pos = (self.current_pose.position.x, self.current_pose.position.y)
              lookahead_distance = self.isaac_nav_params['path_lookahead']

              # Find waypoints within lookahead distance
              local_waypoints = []
              for i in range(self.path_index, len(self.global_path)):
                  wp = self.global_path[i]
                  dist = math.sqrt((wp[0] - current_pos[0])**2 + (wp[1] - current_pos[1])**2)

                  if dist <= lookahead_distance:
                      local_waypoints.append(wp)
                  elif local_waypoints:  # If we have waypoints and this one is too far
                      break

              if local_waypoints:
                  self.local_path = local_waypoints
                  self.publish_local_path()

          def follow_local_path(self):
              """Follow the local path with obstacle avoidance"""
              if not self.local_path:
                  return None

              # Get next waypoint in local path
              if len(self.local_path) > 0:
                  target = self.local_path[0]
              else:
                  return None

              # Calculate required velocity to reach target
              current_pos = (self.current_pose.position.x, self.current_pose.position.y)
              target_pos = target

              dx = target_pos[0] - current_pos[0]
              dy = target_pos[1] - current_pos[1]
              distance = math.sqrt(dx*dx + dy*dy)

              cmd_vel = Twist()

              # Check for obstacles using laser scan
              if self.has_obstacle_ahead():
                  # Emergency stop or obstacle avoidance
                  cmd_vel.linear.x = 0.0
                  cmd_vel.angular.z = 0.3  # Turn to avoid
              else:
                  # Calculate velocities
                  if distance > 0.1:  # If not close to waypoint
                      # Calculate linear velocity
                      linear_vel = min(distance * 0.5, self.isaac_nav_params['max_linear_vel'])

                      # Calculate angular error
                      target_angle = math.atan2(dy, dx)
                      current_yaw = self.get_yaw_from_quaternion(self.current_pose.orientation)
                      angle_error = target_angle - current_yaw

                      # Normalize angle
                      while angle_error > math.pi:
                          angle_error -= 2 * math.pi
                      while angle_error < -math.pi:
                          angle_error += 2 * math.pi

                      angular_vel = max(min(angle_error * 2.0, self.isaac_nav_params['max_angular_vel']),
                                       -self.isaac_nav_params['max_angular_vel'])

                      cmd_vel.linear.x = linear_vel
                      cmd_vel.angular.z = angular_vel

                      # Update path index if close to current waypoint
                      if distance < 0.3:
                          self.local_path.pop(0)
                  else:
                      # Reached waypoint, move to next
                      self.local_path.pop(0)
                      if not self.local_path:
                          # Need more waypoints from global path
                          self.plan_local_path()

              return cmd_vel

          def a_star_search(self, start, goal):
              """Simplified A* pathfinding algorithm"""
              # In Isaac, this would be GPU-accelerated
              # This is a simplified implementation for demonstration

              def heuristic(a, b):
                  return abs(a[0] - b[0]) + abs(a[1] - b[1])

              def get_neighbors(pos):
                  neighbors = []
                  for dx, dy in [(-1,0), (1,0), (0,-1), (0,1), (-1,-1), (-1,1), (1,-1), (1,1)]:
                      nx, ny = pos[0] + dx, pos[1] + dy
                      if 0 <= nx < self.costmap.shape[1] and 0 <= ny < self.costmap.shape[0]:
                          if self.costmap[ny, nx] < 50:  # Not an obstacle
                              neighbors.append((nx, ny))
                  return neighbors

              frontier = [(0, start)]
              came_from = {start: None}
              cost_so_far = {start: 0}

              while frontier:
                  current = heapq.heappop(frontier)[1]

                  if current == goal:
                      break

                  for next_pos in get_neighbors(current):
                      new_cost = cost_so_far[current] + 1  # Simple cost model

                      if next_pos not in cost_so_far or new_cost < cost_so_far[next_pos]:
                          cost_so_far[next_pos] = new_cost
                          priority = new_cost + heuristic(goal, next_pos)
                          heapq.heappush(frontier, (priority, next_pos))
                          came_from[next_pos] = current

              # Reconstruct path
              path = []
              current = goal
              while current != start:
                  path.append(current)
                  current = came_from.get(current)
                  if current is None:
                      return []  # No path found
              path.append(start)
              path.reverse()

              return path

          def has_obstacle_ahead(self):
              """Check if there are obstacles ahead using laser scan"""
              if not hasattr(self, 'laser_data'):
                  return False

              # Check laser readings in front of robot (within obstacle threshold)
              # This is a simplified check - in Isaac, this would use GPU-accelerated processing
              if hasattr(self, 'laser_data'):
                  # Check the front 30-degree sector
                  front_start = len(self.laser_data.ranges) // 2 - 15
                  front_end = len(self.laser_data.ranges) // 2 + 15

                  for i in range(front_start, front_end):
                      if i < len(self.laser_data.ranges):
                          if 0 < self.laser_data.ranges[i] < self.obstacle_threshold:
                              return True

              return False

          def update_local_costmap(self, scan_msg):
              """Update local costmap with laser scan data"""
              # In Isaac, this would use GPU-accelerated costmap updates
              # For simulation, we'll store the scan data
              self.laser_data = scan_msg

          def is_goal_reached(self):
              """Check if the robot has reached the goal"""
              if not self.current_pose or not self.goal_pose:
                  return False

              dx = self.goal_pose.position.x - self.current_pose.position.x
              dy = self.goal_pose.position.y - self.current_pose.position.y
              distance = math.sqrt(dx*dx + dy*dy)

              return distance < 0.3  # 30cm tolerance

          def navigation_complete(self):
              """Handle navigation completion"""
              self.stop_robot()
              self.navigation_active = False

              if self.start_time:
                  elapsed_time = time.time() - self.start_time
                  self.get_logger().info(f'Navigation completed in {elapsed_time:.2f} seconds')

              # Publish completion status
              self.publish_status(False)

          def stop_robot(self):
              """Stop the robot"""
              cmd_vel = Twist()
              cmd_vel.linear.x = 0.0
              cmd_vel.angular.z = 0.0
              self.cmd_vel_pub.publish(cmd_vel)

          def pose_to_map_coords(self, pose):
              """Convert world pose to map coordinates"""
              if self.map_origin is None or self.map_resolution is None:
                  return (0, 0)

              map_x = int((pose.position.x - self.map_origin[0]) / self.map_resolution)
              map_y = int((pose.position.y - self.map_origin[1]) / self.map_resolution)

              return (map_x, map_y)

          def map_coords_to_world(self, coords):
              """Convert map coordinates to world coordinates"""
              if self.map_origin is None or self.map_resolution is None:
                  return (0, 0)

              world_x = coords[0] * self.map_resolution + self.map_origin[0]
              world_y = coords[1] * self.map_resolution + self.map_origin[1]

              return (world_x, world_y)

          def get_yaw_from_quaternion(self, quat):
              """Extract yaw angle from quaternion"""
              siny_cosp = 2 * (quat.w * quat.z + quat.x * quat.y)
              cosy_cosp = 1 - 2 * (quat.y * quat.y + quat.z * quat.z)
              return math.atan2(siny_cosp, cosy_cosp)

          def publish_global_path(self):
              """Publish global path to visualization"""
              path_msg = Path()
              path_msg.header.stamp = self.get_clock().now().to_msg()
              path_msg.header.frame_id = 'map'

              for point in self.global_path:
                  pose_stamped = PoseStamped()
                  pose_stamped.header = path_msg.header
                  pose_stamped.pose.position.x = point[0]
                  pose_stamped.pose.position.y = point[1]
                  pose_stamped.pose.orientation.w = 1.0
                  path_msg.poses.append(pose_stamped)

              self.global_plan_pub.publish(path_msg)

          def publish_local_path(self):
              """Publish local path to visualization"""
              path_msg = Path()
              path_msg.header.stamp = self.get_clock().now().to_msg()
              path_msg.header.frame_id = 'map'

              for point in self.local_path:
                  pose_stamped = PoseStamped()
                  pose_stamped.header = path_msg.header
                  pose_stamped.pose.position.x = point[0]
                  pose_stamped.pose.position.y = point[1]
                  pose_stamped.pose.orientation.w = 1.0
                  path_msg.poses.append(pose_stamped)

              self.local_plan_pub.publish(path_msg)

          def publish_status(self, active):
              """Publish navigation status"""
              status_msg = Bool()
              status_msg.data = active
              self.status_pub.publish(status_msg)

              # Publish progress
              progress_msg = Float64()
              if self.start_time:
                  progress_msg.data = time.time() - self.start_time
              else:
                  progress_msg.data = 0.0
              self.progress_pub.publish(progress_msg)

      def main(args=None):
          rclpy.init(args=args)
          node = IsaacROSNavigationStack()

          try:
              rclpy.spin(node)
          except KeyboardInterrupt:
              node.get_logger().info('Shutting down Isaac ROS navigation stack...')
          finally:
              node.stop_robot()
              node.destroy_node()
              rclpy.shutdown()

      if __name__ == '__main__':
          main()

practical_examples:
  -
    title: "Isaac ROS Perception Pipeline"
    description: "Students implement a complete perception pipeline using Isaac ROS packages integrated with ROS2 navigation stack."
    objectives:
      - "Integrate Isaac ROS perception nodes with ROS2 ecosystem"
      - "Implement GPU-accelerated object detection and tracking"
      - "Connect perception output to navigation decision making"
      - "Optimize performance for real-time operation"
    required_components:
      - "NVIDIA GPU with CUDA support"
      - "Isaac ROS packages"
      - "Camera and LIDAR sensors"
      - "Robot platform with ROS2"
      - "Pre-trained perception models"
    evaluation_criteria:
      - "Perception accuracy and speed"
      - "Integration quality with ROS2"
      - "Real-time performance"
      - "Navigation improvement from perception"
  -
    title: "Isaac Sim HIL Testing"
    description: "Students create a hardware-in-the-loop testing system connecting real robot hardware to Isaac Sim."
    objectives:
      - "Set up Isaac Sim to ROS2 bridge"
      - "Connect real robot hardware to simulation"
      - "Implement safety mechanisms for HIL testing"
      - "Validate robot behavior in simulation vs reality"
    required_components:
      - "Real robot platform"
      - "Isaac Sim installation"
      - "Network infrastructure"
      - "Safety monitoring systems"
      - "Real and simulated sensors"
    evaluation_criteria:
      - "System safety and reliability"
      - "Simulation fidelity"
      - "Hardware integration quality"
      - "Testing effectiveness"
  -
    title: "Hybrid Navigation System"
    description: "Students develop a navigation system combining Isaac's GPU-accelerated planning with ROS2's mature tools."
    objectives:
      - "Integrate Isaac's GPU planners with ROS2 navigation"
      - "Implement dynamic obstacle avoidance"
      - "Create monitoring and visualization tools"
      - "Validate performance improvements"
    required_components:
      - "Mobile robot platform"
      - "Navigation sensors"
      - "Isaac navigation packages"
      - "ROS2 navigation stack"
      - "Performance monitoring tools"
    evaluation_criteria:
      - "Navigation performance improvement"
      - "System integration quality"
      - "GPU utilization efficiency"
      - "Robustness to dynamic obstacles"

summary: "Chapter 14 covered the integration between NVIDIA Isaac and ROS2, demonstrating how to leverage Isaac's GPU-accelerated capabilities within the ROS2 ecosystem. Students learned to use Isaac ROS packages, implement bridges between Isaac Sim and ROS2, and create hybrid systems that combine the strengths of both platforms. The chapter emphasized the benefits of GPU acceleration for perception and navigation tasks while maintaining compatibility with ROS2's extensive tooling."

quiz:
  -
    question: "What is the main benefit of integrating Isaac with ROS2?"
    options:
      - A: To eliminate the need for programming
      - B: To combine Isaac's GPU acceleration with ROS2's ecosystem and tools
      - C: To reduce hardware requirements
      - D: To simplify robot design
    correct_answer: "B"
    explanation: "The main benefit is combining Isaac's GPU-accelerated capabilities with ROS2's mature ecosystem of tools, packages, and community support."
  -
    question: "What does Isaac ROS provide?"
    options:
      - A: Only simulation capabilities
      - B: GPU-accelerated perception and navigation packages
      - C: Hardware components only
      - D: Communication protocols only
    correct_answer: "B"
    explanation: "Isaac ROS provides GPU-accelerated perception and navigation packages that leverage NVIDIA's computing platforms."
  -
    question: "What is hardware-in-the-loop (HIL) testing?"
    options:
      - A: Testing without any hardware
      - B: Connecting real hardware to simulated environments
      - C: Testing only with real hardware
      - D: Hardware that operates independently
    correct_answer: "B"
    explanation: "HIL testing connects real hardware components to simulated environments for safe and comprehensive testing."
  -
    question: "Why is GPU acceleration important in ROS2 perception nodes?"
    options:
      - A: It reduces memory usage
      - B: It enables real-time processing of complex perception tasks
      - C: It eliminates the need for sensors
      - D: It makes robots physically stronger
    correct_answer: "B"
    explanation: "GPU acceleration enables real-time processing of complex perception tasks that would be too slow on CPUs."
  -
    question: "What is a hybrid architecture in Isaac-ROS2 integration?"
    options:
      - A: Using only Isaac components
      - B: Using only ROS2 components
      - C: Combining Isaac and ROS2 components for optimal performance
      - D: Mixing different programming languages
    correct_answer: "C"
    explanation: "A hybrid architecture combines Isaac and ROS2 components to leverage the strengths of both platforms."

module_learning_outcomes:
  - "Implement GPU-accelerated robotics systems"
  - "Integrate AI perception and navigation capabilities"
  - "Develop simulation-to-reality pipelines"
  - "Optimize robot performance using NVIDIA platforms"

prerequisites:
  - "Basic understanding of Python programming"
  - "Fundamentals of linear algebra and calculus"
  - "Basic knowledge of robotics concepts"
  - "Introduction to machine learning concepts"
  - "Completion of Module 0 (Introduction and Foundations)"
  - "Completion of Chapter 01 (Physical AI Basics)"
  - "Completion of Chapter 03 (ROS2 Nodes, Topics & Services)"
  - "Completion of Chapter 11 (Introduction to NVIDIA Isaac)"
  - "Completion of Chapter 12 (Isaac SDK & APIs)"
  - "Completion of Chapter 13 (Isaac Robot Simulation Examples)"

estimated_duration: "5 hours"
...