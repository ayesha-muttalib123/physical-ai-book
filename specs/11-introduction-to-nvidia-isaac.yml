---
chapter_id: "11-introduction-to-nvidia-isaac"
title: "Introduction to NVIDIA Isaac"
module_id: "M4-isaac-brain"
module_title: "NVIDIA Isaac Brain"
overview: "This chapter introduces the NVIDIA Isaac platform, a comprehensive robotics development platform that combines hardware, software, and simulation tools for building intelligent robots. Students will learn about the Isaac ecosystem, including Isaac Sim for simulation, Isaac ROS for perception and navigation, and Isaac Apps for reference applications. The chapter covers the architecture of the Isaac platform, its integration with NVIDIA's GPU computing ecosystem, and how it accelerates robotics development with AI capabilities."
why_it_matters: "NVIDIA Isaac represents a significant advancement in robotics development by providing an integrated platform that combines high-performance computing, AI capabilities, and comprehensive tools. Understanding Isaac is crucial for modern robotics development, especially for applications requiring perception, navigation, and manipulation powered by deep learning. The platform's integration with NVIDIA's GPU ecosystem enables faster development and deployment of AI-powered robots."
key_concepts:
  - "Isaac Ecosystem: Overview of Isaac Sim, Isaac ROS, Isaac Apps, and Isaac Lab"
  - "GPU-Accelerated Robotics: Leveraging NVIDIA GPUs for robotics computation"
  - "Isaac Sim: High-fidelity physics simulation and synthetic data generation"
  - "Isaac ROS: GPU-accelerated perception and navigation packages"
  - "AI-First Robotics: Building robots with AI at the core of design"
  - "Synthetic Data Generation: Creating training data in simulation"
  - "Isaac Navigation: GPU-accelerated path planning and navigation"
  - "Perception Pipelines: Building AI-powered perception systems"

code_examples:
  -
    title: "Isaac ROS Perception Pipeline"
    description: "Basic perception pipeline using Isaac ROS packages for object detection and pose estimation"
    language: "python"
    framework: "ROS2 with Isaac ROS"
    code: |
      #!/usr/bin/env python3
      import rclpy
      from rclpy.node import Node
      from sensor_msgs.msg import Image, CameraInfo
      from geometry_msgs.msg import PoseStamped
      from vision_msgs.msg import Detection2DArray
      from std_msgs.msg import Header
      import cv2
      import numpy as np
      from cv_bridge import CvBridge
      import torch
      import torchvision.transforms as transforms

      class IsaacPerceptionPipeline(Node):
          def __init__(self):
              super().__init__('isaac_perception_pipeline')

              # Publishers and subscribers
              self.image_sub = self.create_subscription(
                  Image, '/camera/rgb/image_rect_color', self.image_callback, 10)
              self.camera_info_sub = self.create_subscription(
                  CameraInfo, '/camera/rgb/camera_info', self.camera_info_callback, 10)
              self.detections_pub = self.create_publisher(
                  Detection2DArray, '/isaac_ros/detections', 10)
              self.object_pose_pub = self.create_publisher(
                  PoseStamped, '/isaac_ros/object_pose', 10)

              # Initialize OpenCV bridge
              self.bridge = CvBridge()

              # Camera parameters (will be updated from camera_info)
              self.camera_matrix = None
              self.dist_coeffs = None

              # Load Isaac-compatible detection model (example with TorchVision)
              self.detection_model = self.load_detection_model()
              self.transform = transforms.Compose([
                  transforms.ToTensor(),
                  transforms.Resize((640, 480)),
                  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
              ])

              self.get_logger().info('Isaac Perception Pipeline initialized')

          def load_detection_model(self):
              """Load a pre-trained object detection model"""
              # Using TorchVision's pre-trained model as an example
              # In practice, this would be an Isaac-optimized model
              model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
              model.eval()
              return model

          def camera_info_callback(self, msg):
              """Update camera parameters from camera info"""
              self.camera_matrix = np.array(msg.k).reshape(3, 3)
              self.dist_coeffs = np.array(msg.d)

          def image_callback(self, msg):
              """Process incoming camera image"""
              try:
                  # Convert ROS image to OpenCV
                  cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

                  # Run object detection
                  detections = self.run_detection(cv_image)

                  # Publish detections
                  self.publish_detections(detections, msg.header)

                  # Estimate object pose if possible
                  if detections.detections:
                      pose = self.estimate_object_pose(cv_image, detections)
                      if pose is not None:
                          self.publish_pose(pose, msg.header)

              except Exception as e:
                  self.get_logger().error(f'Error processing image: {e}')

          def run_detection(self, image):
              """Run object detection on the image"""
              # Convert image for model input
              input_tensor = self.transform(image).unsqueeze(0)

              # Run detection (in Isaac, this would use TensorRT for GPU acceleration)
              with torch.no_grad():
                  results = self.detection_model(input_tensor)

              # Process results
              detections = Detection2DArray()
              detections.header.stamp = self.get_clock().now().to_msg()
              detections.header.frame_id = 'camera_rgb_optical_frame'

              # Convert YOLO results to vision_msgs format
              for *xyxy, conf, cls in results.xyxy[0].tolist():
                  if conf > 0.5:  # Confidence threshold
                      detection = Detection2D()
                      detection.header.stamp = detections.header.stamp
                      detection.header.frame_id = detections.header.frame_id

                      # Bounding box
                      bbox = detection.bbox
                      bbox.center.x = (xyxy[0] + xyxy[2]) / 2
                      bbox.center.y = (xyxy[1] + xyxy[3]) / 2
                      bbox.size_x = xyxy[2] - xyxy[0]
                      bbox.size_y = xyxy[3] - xyxy[1]

                      # Add to detections
                      detections.detections.append(detection)

              return detections

          def publish_detections(self, detections, header):
              """Publish detection results"""
              detections.header = header
              self.detections_pub.publish(detections)

          def estimate_object_pose(self, image, detections):
              """Estimate 3D pose of detected objects (simplified)"""
              if not detections.detections or self.camera_matrix is None:
                  return None

              # This is a simplified pose estimation
              # In Isaac, this would use more sophisticated GPU-accelerated methods
              detection = detections.detections[0]  # Use first detection
              center_x = int(detection.bbox.center.x)
              center_y = int(detection.bbox.center.y)

              # Simple depth estimation (in real application, use depth camera)
              depth = 1.0  # meters (placeholder)

              # Convert pixel coordinates to 3D using camera matrix
              fx = self.camera_matrix[0, 0]
              fy = self.camera_matrix[1, 1]
              cx = self.camera_matrix[0, 2]
              cy = self.camera_matrix[1, 2]

              x = (center_x - cx) * depth / fx
              y = (center_y - cy) * depth / fy
              z = depth

              return (x, y, z)

          def publish_pose(self, pose, header):
              """Publish estimated object pose"""
              pose_msg = PoseStamped()
              pose_msg.header = header
              pose_msg.pose.position.x = pose[0]
              pose_msg.pose.position.y = pose[1]
              pose_msg.pose.position.z = pose[2]
              # Simple orientation (identity quaternion)
              pose_msg.pose.orientation.w = 1.0

              self.object_pose_pub.publish(pose_msg)

      def main(args=None):
          rclpy.init(args=args)
          node = IsaacPerceptionPipeline()

          try:
              rclpy.spin(node)
          except KeyboardInterrupt:
              node.get_logger().info('Shutting down Isaac perception pipeline...')
          finally:
              node.destroy_node()
              rclpy.shutdown()

      if __name__ == '__main__':
          main()

  -
    title: "Isaac Sim Basic Robot Control"
    description: "Basic example of controlling a simulated robot in Isaac Sim using ROS2"
    language: "python"
    framework: "ROS2 with Isaac Sim"
    code: |
      #!/usr/bin/env python3
      import rclpy
      from rclpy.node import Node
      from geometry_msgs.msg import Twist, Pose
      from nav_msgs.msg import Odometry
      from sensor_msgs.msg import JointState
      import math
      import time

      class IsaacSimRobotController(Node):
          def __init__(self):
              super().__init__('isaac_sim_robot_controller')

              # Publishers for robot control
              self.cmd_vel_pub = self.create_publisher(Twist, '/isaac_sim/robot/cmd_vel', 10)
              self.joint_cmd_pub = self.create_publisher(JointState, '/isaac_sim/robot/joint_commands', 10)

              # Subscribers for robot feedback
              self.odom_sub = self.create_subscription(
                  Odometry, '/isaac_sim/robot/odom', self.odom_callback, 10)
              self.joint_state_sub = self.create_subscription(
                  JointState, '/isaac_sim/robot/joint_states', self.joint_state_callback, 10)

              # Timer for control loop
              self.control_timer = self.create_timer(0.1, self.control_loop)  # 10Hz

              # Robot state
              self.current_pose = Pose()
              self.current_twist = Twist()
              self.current_joint_positions = {}
              self.target_pose = {'x': 1.0, 'y': 1.0, 'theta': 0.0}
              self.moving = False

              self.get_logger().info('Isaac Sim Robot Controller initialized')

          def odom_callback(self, msg):
              """Update robot pose from odometry"""
              self.current_pose = msg.pose.pose
              self.current_twist = msg.twist.twist

          def joint_state_callback(self, msg):
              """Update joint positions from joint state"""
              for i, name in enumerate(msg.name):
                  if i < len(msg.position):
                      self.current_joint_positions[name] = msg.position[i]

          def control_loop(self):
              """Main control loop for robot navigation"""
              if not self.moving:
                  # Simple navigation to target
                  self.navigate_to_target()
              else:
                  # Continue movement
                  self.execute_movement()

          def navigate_to_target(self):
              """Navigate to target position"""
              # Calculate distance to target
              dx = self.target_pose['x'] - self.current_pose.position.x
              dy = self.target_pose['y'] - self.current_pose.position.y
              distance = math.sqrt(dx*dx + dy*dy)

              if distance > 0.1:  # If not close to target
                  # Calculate desired angle
                  desired_angle = math.atan2(dy, dx)

                  # Calculate angle difference
                  current_angle = self.get_yaw_from_quaternion(self.current_pose.orientation)
                  angle_diff = desired_angle - current_angle

                  # Normalize angle difference
                  while angle_diff > math.pi:
                      angle_diff -= 2 * math.pi
                  while angle_diff < -math.pi:
                      angle_diff += 2 * math.pi

                  # Create command
                  cmd = Twist()
                  if abs(angle_diff) > 0.1:  # Turn to face target
                      cmd.angular.z = max(min(angle_diff * 1.0, 1.0), -1.0)
                  else:  # Move forward
                      cmd.linear.x = min(distance * 1.0, 0.5)  # Max 0.5 m/s

                  self.cmd_vel_pub.publish(cmd)
                  self.moving = True
              else:
                  # Reached target, stop
                  cmd = Twist()
                  self.cmd_vel_pub.publish(cmd)
                  self.moving = False
                  self.get_logger().info(f'Reached target at ({self.target_pose["x"]}, {self.target_pose["y"]})')

          def execute_movement(self):
              """Execute ongoing movement"""
              # Check if we've reached the target
              dx = self.target_pose['x'] - self.current_pose.position.x
              dy = self.target_pose['y'] - self.current_pose.position.y
              distance = math.sqrt(dx*dx + dy*dy)

              if distance <= 0.1:  # Close enough to target
                  cmd = Twist()
                  self.cmd_vel_pub.publish(cmd)
                  self.moving = False
                  self.get_logger().info(f'Final position: ({self.current_pose.position.x:.2f}, {self.current_pose.position.y:.2f})')

          def get_yaw_from_quaternion(self, quat):
              """Extract yaw angle from quaternion"""
              siny_cosp = 2 * (quat.w * quat.z + quat.x * quat.y)
              cosy_cosp = 1 - 2 * (quat.y * quat.y + quat.z * quat.z)
              return math.atan2(siny_cosp, cosy_cosp)

      def main(args=None):
          rclpy.init(args=args)
          node = IsaacSimRobotController()

          try:
              rclpy.spin(node)
          except KeyboardInterrupt:
              node.get_logger().info('Shutting down Isaac Sim robot controller...')
          finally:
              # Stop the robot before shutting down
              cmd = Twist()
              node.cmd_vel_pub.publish(cmd)
              node.destroy_node()
              rclpy.shutdown()

      if __name__ == '__main__':
          main()

  -
    title: "Isaac Navigation Stack Integration"
    description: "Integration example showing Isaac's navigation capabilities with GPU acceleration"
    language: "python"
    framework: "ROS2 with Isaac Navigation"
    code: |
      #!/usr/bin/env python3
      import rclpy
      from rclpy.node import Node
      from geometry_msgs.msg import PoseStamped, PoseWithCovarianceStamped
      from nav_msgs.msg import Path, OccupancyGrid
      from sensor_msgs.msg import LaserScan, PointCloud2
      from std_msgs.msg import String
      import numpy as np
      import math

      class IsaacNavigationStack(Node):
          def __init__(self):
              super().__init__('isaac_navigation_stack')

              # Publishers
              self.goal_pub = self.create_publisher(PoseStamped, '/isaac_ros/navigation/goal', 10)
              self.cmd_vel_pub = self.create_publisher(PoseStamped, '/isaac_ros/navigation/cmd_vel', 10)
              self.path_pub = self.create_publisher(Path, '/isaac_ros/navigation/local_plan', 10)
              self.status_pub = self.create_publisher(String, '/isaac_ros/navigation/status', 10)

              # Subscribers
              self.scan_sub = self.create_subscription(
                  LaserScan, '/isaac_ros/lidar/scan', self.scan_callback, 10)
              self.map_sub = self.create_subscription(
                  OccupancyGrid, '/isaac_ros/navigation/global_costmap/costmap', self.map_callback, 10)
              self.odom_sub = self.create_subscription(
                  PoseWithCovarianceStamped, '/isaac_ros/robot/pose', self.odom_callback, 10)

              # Timer for navigation planning
              self.nav_timer = self.create_timer(0.05, self.navigation_callback)  # 20Hz

              # Navigation state
              self.current_pose = PoseWithCovarianceStamped()
              self.laser_data = LaserScan()
              self.global_map = OccupancyGrid()
              self.navigation_goals = []
              self.current_goal = None
              self.navigation_active = False
              self.path = Path()

              # Isaac-specific navigation parameters
              self.isaac_params = {
                  'global_planner': 'isaac_global_planner',
                  'local_planner': 'isaac_local_planner',
                  'costmap_resolution': 0.05,  # meters per cell
                  'robot_radius': 0.3,  # meters
                  'max_linear_vel': 0.5,  # m/s
                  'max_angular_vel': 1.0,  # rad/s
                  'min_obstacle_dist': 0.5  # meters
              }

              self.get_logger().info('Isaac Navigation Stack initialized')

          def odom_callback(self, msg):
              """Update robot pose from odometry"""
              self.current_pose = msg

          def scan_callback(self, msg):
              """Update laser scan data"""
              self.laser_data = msg

          def map_callback(self, msg):
              """Update global map"""
              self.global_map = msg

          def navigation_callback(self):
              """Main navigation callback"""
              if not self.navigation_active or not self.current_goal:
                  return

              # Plan path to goal
              planned_path = self.plan_path_to_goal()

              if planned_path:
                  # Execute path following
                  cmd_vel = self.follow_path(planned_path)
                  if cmd_vel:
                      # In Isaac, this would be sent to GPU-accelerated controllers
                      self.execute_command(cmd_vel)
              else:
                  self.get_logger().warn('No valid path found to goal')

          def plan_path_to_goal(self):
              """Plan path to current goal using Isaac's GPU-accelerated planners"""
              # This is a simplified path planner
              # In Isaac, this would use GPU-accelerated A* or Dijkstra's algorithm

              if not self.global_map.data:
                  return None

              # Convert current pose to grid coordinates
              current_x = self.current_pose.pose.pose.position.x
              current_y = self.current_pose.pose.pose.position.y

              goal_x = self.current_goal.pose.position.x
              goal_y = self.current_goal.pose.position.y

              # Simple straight-line path (in real Isaac, this would be a proper path planner)
              path = Path()
              path.header.frame_id = self.global_map.header.frame_id
              path.header.stamp = self.get_clock().now().to_msg()

              # Calculate intermediate points
              steps = 10
              for i in range(steps + 1):
                  t = i / steps
                  x = current_x + t * (goal_x - current_x)
                  y = current_y + t * (goal_y - current_y)

                  pose_stamped = PoseStamped()
                  pose_stamped.header = path.header
                  pose_stamped.pose.position.x = x
                  pose_stamped.pose.position.y = y
                  pose_stamped.pose.position.z = 0.0
                  pose_stamped.pose.orientation.w = 1.0

                  path.poses.append(pose_stamped)

              self.path = path
              self.path_pub.publish(path)
              return path

          def follow_path(self, path):
              """Follow the planned path using Isaac's local planner"""
              if not path.poses:
                  return None

              # Get next waypoint
              target_pose = path.poses[0].pose  # Simplified - would be more sophisticated in real implementation

              # Calculate required velocity to reach waypoint
              current_pos = self.current_pose.pose.pose.position
              target_pos = target_pose.position

              dx = target_pos.x - current_pos.x
              dy = target_pos.y - current_pos.y
              distance = math.sqrt(dx*dx + dy*dy)

              cmd_vel = PoseStamped()
              cmd_vel.header.stamp = self.get_clock().now().to_msg()
              cmd_vel.header.frame_id = 'base_link'

              if distance > 0.1:  # If not close to waypoint
                  # Calculate linear and angular velocities
                  linear_vel = min(distance * 0.5, self.isaac_params['max_linear_vel'])

                  # Calculate angular error
                  target_angle = math.atan2(dy, dx)
                  current_yaw = self.get_yaw_from_quaternion(self.current_pose.pose.pose.orientation)
                  angle_error = target_angle - current_yaw

                  # Normalize angle
                  while angle_error > math.pi:
                      angle_error -= 2 * math.pi
                  while angle_error < -math.pi:
                      angle_error += 2 * math.pi

                  angular_vel = max(min(angle_error * 2.0, self.isaac_params['max_angular_vel']),
                                   -self.isaac_params['max_angular_vel'])

                  cmd_vel.pose.position.x = linear_vel
                  cmd_vel.pose.position.z = angular_vel
              else:
                  # Reached waypoint, stop
                  cmd_vel.pose.position.x = 0.0
                  cmd_vel.pose.position.z = 0.0

              return cmd_vel

          def execute_command(self, cmd_vel):
              """Execute navigation command (would interface with Isaac's GPU controllers)"""
              # In Isaac, this would send commands through GPU-accelerated interfaces
              self.cmd_vel_pub.publish(cmd_vel)

          def get_yaw_from_quaternion(self, quat):
              """Extract yaw angle from quaternion"""
              siny_cosp = 2 * (quat.w * quat.z + quat.x * quat.y)
              cosy_cosp = 1 - 2 * (quat.y * quat.y + quat.z * quat.z)
              return math.atan2(siny_cosp, cosy_cosp)

          def set_navigation_goal(self, x, y, theta=0.0):
              """Set navigation goal"""
              goal = PoseStamped()
              goal.header.frame_id = 'map'
              goal.header.stamp = self.get_clock().now().to_msg()
              goal.pose.position.x = x
              goal.pose.position.y = y
              goal.pose.position.z = 0.0

              # Convert theta to quaternion
              goal.pose.orientation.z = math.sin(theta / 2.0)
              goal.pose.orientation.w = math.cos(theta / 2.0)

              self.current_goal = goal
              self.navigation_active = True

              # Publish goal
              self.goal_pub.publish(goal)
              self.get_logger().info(f'Set navigation goal to ({x}, {y})')

      def main(args=None):
          rclpy.init(args=args)
          node = IsaacNavigationStack()

          # Set an example goal
          node.set_navigation_goal(2.0, 2.0)

          try:
              rclpy.spin(node)
          except KeyboardInterrupt:
              node.get_logger().info('Shutting down Isaac navigation stack...')
          finally:
              node.destroy_node()
              rclpy.shutdown()

      if __name__ == '__main__':
          main()

practical_examples:
  -
    title: "Isaac Sim Warehouse Navigation"
    description: "Students implement a warehouse navigation task using Isaac Sim with GPU-accelerated perception and navigation."
    objectives:
      - "Set up Isaac Sim environment for warehouse navigation"
      - "Implement GPU-accelerated object detection pipeline"
      - "Create navigation system with obstacle avoidance"
      - "Validate performance against real-world constraints"
    required_components:
      - "NVIDIA GPU with CUDA support"
      - "Isaac Sim installation"
      - "Warehouse simulation assets"
      - "Robot models with sensors"
      - "Navigation algorithms"
    evaluation_criteria:
      - "Navigation accuracy and efficiency"
      - "Perception system performance"
      - "GPU utilization and optimization"
      - "Robustness to environmental changes"
  -
    title: "Isaac Manipulation Task"
    description: "Students develop a robotic manipulation task using Isaac's GPU-accelerated perception and control systems."
    objectives:
      - "Implement 3D object detection and pose estimation"
      - "Create manipulation planning pipeline"
      - "Execute grasp and manipulation tasks"
      - "Validate success rate and efficiency"
    required_components:
      - "Robotic manipulator model"
      - "Object models for manipulation"
      - "3D perception sensors"
      - "GPU-accelerated inference engines"
      - "Manipulation planning algorithms"
    evaluation_criteria:
      - "Object detection accuracy"
      - "Grasp success rate"
      - "Manipulation efficiency"
      - "System integration quality"
  -
    title: "Isaac Multi-Robot Coordination"
    description: "Students implement multi-robot coordination using Isaac's distributed computing capabilities."
    objectives:
      - "Set up multi-robot simulation environment"
      - "Implement coordination and communication protocols"
      - "Create task allocation and scheduling system"
      - "Optimize performance across multiple robots"
    required_components:
      - "Multiple robot models"
      - "Communication infrastructure"
      - "Task allocation algorithms"
      - "Distributed computing setup"
      - "Coordination protocols"
    evaluation_criteria:
      - "Coordination effectiveness"
      - "Task completion efficiency"
      - "Communication reliability"
      - "Scalability with robot count"

summary: "Chapter 11 introduced the NVIDIA Isaac platform, covering its ecosystem of tools including Isaac Sim, Isaac ROS, and Isaac Apps. Students learned about GPU-accelerated robotics, Isaac's simulation capabilities, and how to implement perception and navigation pipelines. The chapter emphasized Isaac's role in AI-first robotics development and the advantages of GPU computing for robotics applications."

quiz:
  -
    question: "What is the primary advantage of the NVIDIA Isaac platform for robotics?"
    options:
      - A: Lower hardware costs
      - B: GPU-accelerated computing for AI and perception
      - C: Simpler programming requirements
      - D: Reduced need for sensors
    correct_answer: "B"
    explanation: "The primary advantage of NVIDIA Isaac is GPU-accelerated computing, which enables high-performance AI and perception processing for robotics."
  -
    question: "Which component of Isaac provides high-fidelity simulation?"
    options:
      - A: Isaac ROS
      - B: Isaac Sim
      - C: Isaac Apps
      - D: Isaac Lab
    correct_answer: "B"
    explanation: "Isaac Sim provides high-fidelity physics simulation and synthetic data generation capabilities."
  -
    question: "What does Isaac ROS provide?"
    options:
      - A: Only simulation capabilities
      - B: GPU-accelerated perception and navigation packages
      - C: Hardware components only
      - D: Communication protocols only
    correct_answer: "B"
    explanation: "Isaac ROS provides GPU-accelerated perception and navigation packages for robotics applications."
  -
    question: "Why is synthetic data generation important in Isaac?"
    options:
      - A: It reduces the need for real sensors
      - B: It enables training of AI models with labeled data from simulation
      - C: It makes robots move faster
      - D: It eliminates the need for programming
    correct_answer: "B"
    explanation: "Synthetic data generation enables training of AI models with perfectly labeled data from simulation environments."
  -
    question: "What type of robotics approach does Isaac emphasize?"
    options:
      - A: Hardware-first design
      - B: AI-first robotics design
      - C: Simple mechanical systems
      - D: Manual control systems
    correct_answer: "B"
    explanation: "Isaac emphasizes AI-first robotics design, where AI capabilities are central to the robot's architecture."

module_learning_outcomes:
  - "Implement GPU-accelerated robotics systems"
  - "Integrate AI perception and navigation capabilities"
  - "Develop simulation-to-reality pipelines"
  - "Optimize robot performance using NVIDIA platforms"

prerequisites:
  - "Basic understanding of Python programming"
  - "Fundamentals of linear algebra and calculus"
  - "Basic knowledge of robotics concepts"
  - "Introduction to machine learning concepts"
  - "Completion of Module 0 (Introduction and Foundations)"
  - "Completion of Chapter 01 (Physical AI Basics)"
  - "Completion of Chapter 03 (ROS2 Nodes, Topics & Services)"
  - "Completion of Chapter 06 (Introduction to Digital Twins)"

estimated_duration: "4 hours"
...