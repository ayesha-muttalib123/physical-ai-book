"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[411],{1357:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"15-Chapter-3-Integration-With-ROS2","title":"Chapter 3: Integration With ROS2","description":"Overview","source":"@site/docusaurus/docs/15-Chapter-3-Integration-With-ROS2.md","sourceDirName":".","slug":"/15-Chapter-3-Integration-With-ROS2","permalink":"/physical-ai-book/ur/docs/15-Chapter-3-Integration-With-ROS2","draft":false,"unlisted":false,"editUrl":"https://github.com/ayesha-muttalib123/physical-ai-book/tree/main/docusaurus/docs/15-Chapter-3-Integration-With-ROS2.md","tags":[],"version":"current","sidebarPosition":15,"frontMatter":{"id":"15-Chapter-3-Integration-With-ROS2","title":"Chapter 3: Integration With ROS2","sidebar_position":15},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Isaac Robot Simulation Examples","permalink":"/physical-ai-book/ur/docs/14-Chapter-2-Isaac-Robot-Simulation-Examples"},"next":{"title":"Chapter 4: Isaac Best Practices & Optimization","permalink":"/physical-ai-book/ur/docs/16-Chapter-4-Isaac-Best-Practices-Optimization"}}');var t=r(4848),s=r(8453);const i={id:"15-Chapter-3-Integration-With-ROS2",title:"Chapter 3: Integration With ROS2",sidebar_position:15},o="Chapter 3: Integration With ROS2",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Why It Matters",id:"why-it-matters",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Isaac ROS Packages",id:"isaac-ros-packages",level:3},{value:"ROS2 Bridge",id:"ros2-bridge",level:3},{value:"Message Translation",id:"message-translation",level:3},{value:"GPU-Accelerated Nodes",id:"gpu-accelerated-nodes",level:3},{value:"Hybrid Architecture",id:"hybrid-architecture",level:3},{value:"Hardware-in-the-Loop (HIL)",id:"hardware-in-the-loop-hil",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"System Integration",id:"system-integration",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Isaac ROS Perception Node",id:"isaac-ros-perception-node",level:3},{value:"Isaac Sim to ROS2 Bridge",id:"isaac-sim-to-ros2-bridge",level:3},{value:"Isaac ROS Navigation Stack",id:"isaac-ros-navigation-stack",level:3},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Isaac ROS Perception Pipeline",id:"isaac-ros-perception-pipeline",level:3},{value:"Isaac Sim HIL Testing",id:"isaac-sim-hil-testing",level:3},{value:"Hybrid Navigation System",id:"hybrid-navigation-system",level:3},{value:"Summary",id:"summary",level:2},{value:"Quiz",id:"quiz",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Estimated Duration",id:"estimated-duration",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-3-integration-with-ros2",children:"Chapter 3: Integration With ROS2"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"This chapter covers the integration between NVIDIA Isaac and ROS2, demonstrating how to leverage Isaac's GPU-accelerated capabilities within the ROS2 ecosystem. Students will learn to use Isaac ROS packages, bridge Isaac Sim with ROS2 for hardware-in-the-loop testing, and implement GPU-accelerated perception and navigation nodes. The chapter explores Isaac's ROS2 compatibility layers, message translation, and how to build hybrid systems that combine Isaac's specialized capabilities with ROS2's extensive tooling and community support."}),"\n",(0,t.jsx)(n.h2,{id:"why-it-matters",children:"Why It Matters"}),"\n",(0,t.jsx)(n.p,{children:"Integration with ROS2 is crucial for leveraging both NVIDIA Isaac's GPU-accelerated capabilities and ROS2's mature ecosystem of tools, packages, and community support. This integration allows developers to build robotics systems that benefit from Isaac's high-performance perception and simulation while maintaining compatibility with the vast ROS2 package repository. Understanding this integration enables the creation of more robust, efficient, and maintainable robotics applications."}),"\n",(0,t.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-packages",children:"Isaac ROS Packages"}),"\n",(0,t.jsx)(n.p,{children:"GPU-accelerated perception and navigation packages that leverage NVIDIA's computing platforms. These packages provide optimized implementations of common robotics algorithms that run efficiently on GPUs, offering significant performance improvements over CPU-based alternatives."}),"\n",(0,t.jsx)(n.h3,{id:"ros2-bridge",children:"ROS2 Bridge"}),"\n",(0,t.jsx)(n.p,{children:"Connecting Isaac Sim with ROS2 for hardware-in-the-loop (HIL) testing. This enables real hardware components to interact with simulated environments, allowing for safe and comprehensive testing of robotic systems before deployment."}),"\n",(0,t.jsx)(n.h3,{id:"message-translation",children:"Message Translation"}),"\n",(0,t.jsx)(n.p,{children:"Converting between Isaac and ROS2 message formats. This involves implementing adapters that transform data structures between Isaac's native formats and ROS2 message types to enable seamless communication between systems."}),"\n",(0,t.jsx)(n.h3,{id:"gpu-accelerated-nodes",children:"GPU-Accelerated Nodes"}),"\n",(0,t.jsx)(n.p,{children:"Implementing CUDA/TensorRT accelerated ROS2 nodes. These nodes utilize GPU computing to perform computationally intensive tasks like perception, planning, and control more efficiently than traditional CPU implementations."}),"\n",(0,t.jsx)(n.h3,{id:"hybrid-architecture",children:"Hybrid Architecture"}),"\n",(0,t.jsx)(n.p,{children:"Combining Isaac and ROS2 components to create systems that leverage the strengths of both platforms. This approach allows developers to use Isaac's GPU-accelerated capabilities where needed while benefiting from ROS2's extensive tooling and package ecosystem."}),"\n",(0,t.jsx)(n.h3,{id:"hardware-in-the-loop-hil",children:"Hardware-in-the-Loop (HIL)"}),"\n",(0,t.jsx)(n.p,{children:"Testing with real hardware connected to simulation. This technique connects real hardware components to simulated environments for safe and comprehensive testing without the risks associated with real-world testing."}),"\n",(0,t.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.p,{children:"Maximizing GPU utilization in ROS2 systems. This involves designing nodes and communication patterns that take advantage of GPU parallelism while minimizing data transfer overhead between CPU and GPU."}),"\n",(0,t.jsx)(n.h3,{id:"system-integration",children:"System Integration"}),"\n",(0,t.jsx)(n.p,{children:"Building complete robotics solutions that seamlessly combine Isaac and ROS2 components. This includes proper initialization, resource management, and error handling across both systems."}),"\n",(0,t.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-perception-node",children:"Isaac ROS Perception Node"}),"\n",(0,t.jsx)(n.p,{children:"GPU-accelerated perception node using Isaac ROS packages for object detection and segmentation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nIsaac ROS Perception Node\r\nDemonstrates GPU-accelerated perception using Isaac ROS packages\r\n\"\"\"\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom vision_msgs.msg import Detection2DArray, Detection2D, BoundingBox2D\r\nfrom geometry_msgs.msg import Point\r\nfrom std_msgs.msg import Header\r\nimport cv2\r\nimport numpy as np\r\nfrom cv_bridge import CvBridge\r\nimport torch\r\nimport torchvision.transforms as transforms\r\nimport tensorrt as trt\r\nimport pycuda.driver as cuda\r\nimport pycuda.autoinit\r\n\r\nclass IsaacROSPerceptionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('isaac_ros_perception')\r\n\r\n        # Publishers and subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image, '/camera/rgb/image_raw', self.image_callback, 10)\r\n        self.camera_info_sub = self.create_subscription(\r\n            CameraInfo, '/camera/rgb/camera_info', self.camera_info_callback, 10)\r\n        self.detections_pub = self.create_publisher(\r\n            Detection2DArray, '/isaac_ros/detections', 10)\r\n        self.segmentation_pub = self.create_publisher(\r\n            Image, '/isaac_ros/segmentation', 10)\r\n\r\n        # Initialize OpenCV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Camera parameters (will be updated from camera_info)\r\n        self.camera_matrix = None\r\n        self.dist_coeffs = None\r\n\r\n        # Initialize GPU-accelerated models\r\n        self.initialize_gpu_models()\r\n\r\n        # Performance metrics\r\n        self.frame_count = 0\r\n        self.start_time = self.get_clock().now().nanoseconds / 1e9\r\n\r\n        self.get_logger().info('Isaac ROS Perception Node initialized')\r\n\r\n    def initialize_gpu_models(self):\r\n        \"\"\"Initialize GPU-accelerated models for perception\"\"\"\r\n        try:\r\n            # Initialize TensorRT engine for object detection\r\n            self.detection_engine = self.load_tensorrt_engine('/path/to/detection_model.plan')\r\n\r\n            # Initialize segmentation model\r\n            self.segmentation_engine = self.load_tensorrt_engine('/path/to/segmentation_model.plan')\r\n\r\n            # Allocate GPU memory for inference\r\n            self.allocate_gpu_memory()\r\n\r\n            self.get_logger().info('GPU models initialized successfully')\r\n        except Exception as e:\r\n            self.get_logger().error(f'Failed to initialize GPU models: {e}')\r\n            # Fallback to CPU models\r\n            self.use_cpu_fallback = True\r\n\r\n    def load_tensorrt_engine(self, engine_path):\r\n        \"\"\"Load TensorRT engine for GPU inference\"\"\"\r\n        try:\r\n            with open(engine_path, 'rb') as f:\r\n                engine_data = f.read()\r\n            runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))\r\n            engine = runtime.deserialize_cuda_engine(engine_data)\r\n            return engine\r\n        except Exception as e:\r\n            self.get_logger().error(f'Failed to load TensorRT engine: {e}')\r\n            return None\r\n\r\n    def allocate_gpu_memory(self):\r\n        \"\"\"Allocate GPU memory for inference\"\"\"\r\n        # This is a simplified example - in practice, you would allocate\r\n        # memory for input/output tensors based on model requirements\r\n        self.gpu_input_buffer = cuda.mem_alloc(3 * 224 * 224 * 4)  # RGB, 224x224, float32\r\n        self.gpu_output_buffer = cuda.mem_alloc(1000 * 4)  # 1000 classes, float32\r\n\r\n        # Create CUDA stream\r\n        self.cuda_stream = cuda.Stream()\r\n\r\n    def camera_info_callback(self, msg):\r\n        \"\"\"Update camera parameters from camera info\"\"\"\r\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\r\n        self.dist_coeffs = np.array(msg.d)\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Process incoming camera image with GPU acceleration\"\"\"\r\n        try:\r\n            # Convert ROS image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\r\n\r\n            # Record start time for performance measurement\r\n            start_time = self.get_clock().now().nanoseconds / 1e9\r\n\r\n            # Run GPU-accelerated object detection\r\n            detections = self.run_gpu_detection(cv_image)\r\n\r\n            # Run GPU-accelerated segmentation\r\n            segmentation = self.run_gpu_segmentation(cv_image)\r\n\r\n            # Calculate processing time\r\n            end_time = self.get_clock().now().nanoseconds / 1e9\r\n            processing_time = (end_time - start_time) * 1000  # ms\r\n\r\n            # Publish results\r\n            self.publish_detections(detections, msg.header)\r\n            self.publish_segmentation(segmentation, msg.header)\r\n\r\n            # Update and log performance metrics\r\n            self.frame_count += 1\r\n            current_time = self.get_clock().now().nanoseconds / 1e9\r\n            elapsed_time = current_time - self.start_time\r\n            fps = self.frame_count / elapsed_time if elapsed_time > 0 else 0\r\n\r\n            if self.frame_count % 30 == 0:  # Log every 30 frames\r\n                self.get_logger().info(\r\n                    f'Perception processing: {processing_time:.2f}ms, '\r\n                    f'FPS: {fps:.2f}, Detections: {len(detections) if detections else 0}'\r\n                )\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing image: {e}')\r\n\r\n    def run_gpu_detection(self, image):\r\n        \"\"\"Run GPU-accelerated object detection\"\"\"\r\n        try:\r\n            # Preprocess image for model input\r\n            input_tensor = self.preprocess_image(image)\r\n\r\n            # In Isaac ROS, this would use Isaac's optimized detection pipelines\r\n            # For this example, we'll simulate the GPU processing\r\n\r\n            # Simulate GPU inference (in real Isaac, this would use TensorRT)\r\n            height, width = image.shape[:2]\r\n\r\n            # Simulate detection results\r\n            detections = []\r\n            num_detections = np.random.poisson(2)  # Random number of detections\r\n\r\n            for i in range(num_detections):\r\n                # Generate random detection\r\n                x = np.random.uniform(0, width - 50)\r\n                y = np.random.uniform(0, height - 50)\r\n                w = np.random.uniform(30, 100)\r\n                h = np.random.uniform(30, 100)\r\n\r\n                detection = {\r\n                    'bbox': [x, y, w, h],\r\n                    'confidence': np.random.uniform(0.6, 0.99),\r\n                    'class_id': np.random.randint(0, 80),\r\n                    'class_name': f'object_{np.random.randint(0, 5)}'\r\n                }\r\n                detections.append(detection)\r\n\r\n            return detections\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'GPU detection error: {e}')\r\n            return []\r\n\r\n    def run_gpu_segmentation(self, image):\r\n        \"\"\"Run GPU-accelerated semantic segmentation\"\"\"\r\n        # In Isaac ROS, this would use GPU-accelerated segmentation\r\n        # For simulation, create a segmentation mask\r\n        height, width = image.shape[:2]\r\n\r\n        # Simulate segmentation mask\r\n        segmentation_mask = np.zeros((height, width), dtype=np.uint8)\r\n\r\n        # Add some segmented regions\r\n        for _ in range(3):  # 3 random regions\r\n            center_x = np.random.randint(width//4, 3*width//4)\r\n            center_y = np.random.randint(height//4, 3*height//4)\r\n            radius = np.random.randint(20, 50)\r\n\r\n            y, x = np.ogrid[:height, :width]\r\n            mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2\r\n            segmentation_mask[mask] = np.random.randint(1, 10)  # Random class ID\r\n\r\n        return segmentation_mask\r\n\r\n    def preprocess_image(self, image):\r\n        \"\"\"Preprocess image for GPU inference\"\"\"\r\n        # Resize image to model input size\r\n        input_height, input_width = 640, 640  # Example model input size\r\n        resized = cv2.resize(image, (input_width, input_height))\r\n\r\n        # Convert BGR to RGB\r\n        rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\r\n\r\n        # Normalize pixel values\r\n        normalized = rgb.astype(np.float32) / 255.0\r\n\r\n        # Convert to CHW format (channels, height, width)\r\n        chw = np.transpose(normalized, (2, 0, 1))\r\n\r\n        return chw\r\n\r\n    def publish_detections(self, detections, header):\r\n        \"\"\"Publish detection results to ROS2 topic\"\"\"\r\n        if not detections:\r\n            return\r\n\r\n        detection_array = Detection2DArray()\r\n        detection_array.header = header\r\n\r\n        for det in detections:\r\n            detection_2d = Detection2D()\r\n            detection_2d.header = header\r\n\r\n            # Set bounding box\r\n            bbox = BoundingBox2D()\r\n            bbox.center.x = det['bbox'][0] + det['bbox'][2] / 2  # center x\r\n            bbox.center.y = det['bbox'][1] + det['bbox'][3] / 2  # center y\r\n            bbox.center.theta = 0.0\r\n            bbox.size_x = det['bbox'][2]  # width\r\n            bbox.size_y = det['bbox'][3]  # height\r\n            detection_2d.bbox = bbox\r\n\r\n            # Set confidence\r\n            detection_2d.results = []\r\n\r\n            detection_array.detections.append(detection_2d)\r\n\r\n        self.detections_pub.publish(detection_array)\r\n\r\n    def publish_segmentation(self, segmentation_mask, header):\r\n        \"\"\"Publish segmentation results to ROS2 topic\"\"\"\r\n        try:\r\n            # Convert segmentation mask to ROS image message\r\n            seg_msg = self.bridge.cv2_to_imgmsg(segmentation_mask, encoding='mono8')\r\n            seg_msg.header = header\r\n            self.segmentation_pub.publish(seg_msg)\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error publishing segmentation: {e}')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = IsaacROSPerceptionNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('Shutting down Isaac ROS perception node...')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"isaac-sim-to-ros2-bridge",children:"Isaac Sim to ROS2 Bridge"}),"\n",(0,t.jsx)(n.p,{children:"Implementation of a bridge between Isaac Sim and ROS2 for hardware-in-the-loop testing:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nIsaac Sim to ROS2 Bridge\r\nDemonstrates connecting Isaac Sim with ROS2 for hardware-in-the-loop testing\r\n\"\"\"\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import Twist, Pose, PoseStamped, Point\r\nfrom nav_msgs.msg import Odometry\r\nfrom sensor_msgs.msg import Image, LaserScan, CameraInfo, Imu\r\nfrom std_msgs.msg import Header, Float64, String\r\nimport numpy as np\r\nimport time\r\nimport threading\r\nimport queue\r\nfrom typing import Dict, Any\r\n\r\n# Isaac Sim imports (these would be available in Isaac Sim environment)\r\ntry:\r\n    import omni\r\n    from omni.isaac.core import World\r\n    from omni.isaac.core.utils.stage import add_reference_to_stage\r\n    from omni.isaac.core.utils.nucleus import get_assets_root_path\r\n    from omni.isaac.core.utils.prims import get_prim_at_path\r\n    from omni.isaac.core.utils.viewports import set_camera_view\r\n    from omni.isaac.core.robots import Robot\r\n    from omni.isaac.sensor import Camera, LidarRtx\r\n    import carb\r\nexcept ImportError:\r\n    # For simulation/testing when Isaac Sim is not available\r\n    carb = None\r\n    World = None\r\n    print(\"Isaac Sim modules not available - running in simulation mode\")\r\n\r\nclass IsaacSimROS2Bridge(Node):\r\n    def __init__(self):\r\n        super().__init__('isaac_sim_ros2_bridge')\r\n\r\n        # Publishers for simulated sensor data from Isaac Sim\r\n        self.odom_pub = self.create_publisher(Odometry, '/sim_robot/odom', 10)\r\n        self.camera_pub = self.create_publisher(Image, '/sim_robot/camera/image_raw', 10)\r\n        self.lidar_pub = self.create_publisher(LaserScan, '/sim_robot/lidar/scan', 10)\r\n        self.imu_pub = self.create_publisher(Imu, '/sim_robot/imu', 10)\r\n        self.status_pub = self.create_publisher(String, '/sim_robot/status', 10)\r\n\r\n        # Subscribers for commands from ROS2\r\n        self.cmd_vel_sub = self.create_subscription(\r\n            Twist, '/sim_robot/cmd_vel', self.cmd_vel_callback, 10)\r\n\r\n        # Timer for publishing simulated data\r\n        self.publish_timer = self.create_timer(0.05, self.publish_sensor_data)  # 20Hz\r\n\r\n        # Robot state\r\n        self.robot_pose = {'x': 0.0, 'y': 0.0, 'theta': 0.0}\r\n        self.robot_twist = {'linear_x': 0.0, 'angular_z': 0.0}\r\n        self.sim_time = 0.0\r\n        self.last_update_time = time.time()\r\n\r\n        # Isaac Sim world (will be initialized if available)\r\n        self.isaac_world = None\r\n        self.isaac_robot = None\r\n        self.isaac_camera = None\r\n        self.isaac_lidar = None\r\n\r\n        # Initialize Isaac Sim if available\r\n        self.initialize_isaac_sim()\r\n\r\n        self.get_logger().info('Isaac Sim to ROS2 Bridge initialized')\r\n\r\n    def initialize_isaac_sim(self):\r\n        \"\"\"Initialize Isaac Sim if available\"\"\"\r\n        if World is None:\r\n            self.get_logger().warn('Isaac Sim not available - running in simulation mode')\r\n            return\r\n\r\n        try:\r\n            # Initialize Isaac Sim world\r\n            self.isaac_world = World(stage_units_in_meters=1.0, physics_dt=1.0/60.0, rendering_dt=1.0/60.0)\r\n\r\n            # Add default ground plane\r\n            self.isaac_world.scene.add_default_ground_plane()\r\n\r\n            # Set up the viewport camera view\r\n            set_camera_view(eye=np.array([2.5, 2.5, 2.0]), target=np.array([0, 0, 0.5]))\r\n\r\n            # Add a simple environment with obstacles\r\n            self.setup_environment()\r\n\r\n            # Add robot (using a simple differential drive robot as example)\r\n            self.setup_robot()\r\n\r\n            # Add sensors to robot\r\n            self.setup_sensors()\r\n\r\n            # Reset the world\r\n            self.isaac_world.reset()\r\n\r\n            self.get_logger().info('Isaac Sim initialized successfully')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Failed to initialize Isaac Sim: {e}')\r\n\r\n    def setup_environment(self):\r\n        \"\"\"Setup the simulation environment with objects\"\"\"\r\n        # Add static obstacles\r\n        from omni.isaac.core.objects import FixedCuboid\r\n\r\n        obstacle1 = self.isaac_world.scene.add(\r\n            FixedCuboid(\r\n                prim_path=\"/World/Obstacle1\",\r\n                name=\"obstacle1\",\r\n                position=np.array([1.0, 0.5, 0.25]),\r\n                size=np.array([0.5, 0.5, 0.5]),\r\n                color=np.array([0.5, 0.5, 0.5])\r\n            )\r\n        )\r\n\r\n        obstacle2 = self.isaac_world.scene.add(\r\n            FixedCuboid(\r\n                prim_path=\"/World/Obstacle2\",\r\n                name=\"obstacle2\",\r\n                position=np.array([-0.8, -0.5, 0.25]),\r\n                size=np.array([0.4, 0.4, 0.4]),\r\n                color=np.array([0.7, 0.3, 0.3])\r\n            )\r\n        )\r\n\r\n        # Add physics material\r\n        from omni.isaac.core.materials import PhysicsMaterial\r\n        material = PhysicsMaterial(\r\n            prim_path=\"/World/physics_material\",\r\n            static_friction=0.5,\r\n            dynamic_friction=0.5,\r\n            restitution=0.1\r\n        )\r\n\r\n        obstacle1.set_material(material)\r\n        obstacle2.set_material(material)\r\n\r\n    def setup_robot(self):\r\n        \"\"\"Setup the robot in the simulation\"\"\"\r\n        # For this example, we'll create a simple differential drive robot\r\n        # In real applications, you would load a URDF or USD robot model\r\n        from omni.isaac.core.objects import DynamicCuboid\r\n\r\n        # Create robot body\r\n        self.isaac_robot = self.isaac_world.scene.add(\r\n            DynamicCuboid(\r\n                prim_path=\"/World/RobotBody\",\r\n                name=\"sim_robot\",\r\n                position=np.array([0.0, 0.0, 0.3]),\r\n                size=np.array([0.3, 0.4, 0.2]),\r\n                color=np.array([0.1, 0.1, 0.8])\r\n            )\r\n        )\r\n\r\n        self.get_logger().info('Robot setup complete')\r\n\r\n    def setup_sensors(self):\r\n        \"\"\"Setup sensors for the robot\"\"\"\r\n        # In Isaac Sim, this would involve creating actual sensors\r\n        # For this example, we'll just set up the simulation parameters\r\n        self.get_logger().info('Sensors setup complete')\r\n\r\n    def cmd_vel_callback(self, msg):\r\n        \"\"\"Handle velocity commands from ROS2\"\"\"\r\n        # Update robot velocities based on ROS2 command\r\n        self.robot_twist['linear_x'] = msg.linear.x\r\n        self.robot_twist['angular_z'] = msg.angular.z\r\n\r\n        # If Isaac Sim is available, apply command to simulated robot\r\n        if self.isaac_robot is not None:\r\n            self.apply_robot_command()\r\n\r\n    def apply_robot_command(self):\r\n        \"\"\"Apply velocity command to Isaac Sim robot\"\"\"\r\n        if self.isaac_world is None or self.isaac_robot is None:\r\n            return\r\n\r\n        try:\r\n            # Get current robot state\r\n            current_pos, current_ori = self.isaac_robot.get_world_pose()\r\n\r\n            # Calculate new position based on velocities\r\n            dt = 0.05  # 20Hz update\r\n            linear_vel = self.robot_twist['linear_x']\r\n            angular_vel = self.robot_twist['angular_z']\r\n\r\n            # Update orientation\r\n            current_yaw = np.arctan2(\r\n                2 * (current_ori[3] * current_ori[2] + current_ori[0] * current_ori[1]),\r\n                1 - 2 * (current_ori[1]**2 + current_ori[2]**2)\r\n            )\r\n            new_yaw = current_yaw + angular_vel * dt\r\n\r\n            # Update position\r\n            new_x = current_pos[0] + linear_vel * np.cos(new_yaw) * dt\r\n            new_y = current_pos[1] + linear_vel * np.sin(new_yaw) * dt\r\n\r\n            # Update robot pose in Isaac Sim\r\n            self.isaac_robot.set_world_pose(\r\n                position=np.array([new_x, new_y, current_pos[2]]),\r\n                orientation=np.array([0, 0, np.sin(new_yaw/2), np.cos(new_yaw/2)])\r\n            )\r\n\r\n            # Update local state for publishing\r\n            self.robot_pose['x'] = new_x\r\n            self.robot_pose['y'] = new_y\r\n            self.robot_pose['theta'] = new_yaw\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error applying robot command: {e}')\r\n\r\n    def update_simulation(self):\r\n        \"\"\"Update simulation state\"\"\"\r\n        if self.isaac_world is not None:\r\n            # Step Isaac Sim world\r\n            self.isaac_world.step(render=True)\r\n        else:\r\n            # Simulated physics update\r\n            dt = 0.05  # 20Hz update\r\n\r\n            # Update robot pose based on current velocities\r\n            linear_vel = self.robot_twist['linear_x']\r\n            angular_vel = self.robot_twist['angular_z']\r\n\r\n            # Update orientation\r\n            new_theta = self.robot_pose['theta'] + angular_vel * dt\r\n\r\n            # Update position\r\n            new_x = self.robot_pose['x'] + linear_vel * np.cos(new_theta) * dt\r\n            new_y = self.robot_pose['y'] + linear_vel * np.sin(new_theta) * dt\r\n\r\n            # Update state\r\n            self.robot_pose['x'] = new_x\r\n            self.robot_pose['y'] = new_y\r\n            self.robot_pose['theta'] = new_theta\r\n\r\n    def publish_sensor_data(self):\r\n        \"\"\"Publish simulated sensor data to ROS2 topics\"\"\"\r\n        # Update simulation state\r\n        self.update_simulation()\r\n\r\n        # Update simulation time\r\n        current_time = time.time()\r\n        dt = current_time - self.last_update_time\r\n        self.sim_time += dt\r\n        self.last_update_time = current_time\r\n\r\n        # Create header\r\n        header = Header()\r\n        header.stamp = self.get_clock().now().to_msg()\r\n        header.frame_id = 'base_link'\r\n\r\n        # Publish odometry\r\n        self.publish_odometry(header)\r\n\r\n        # Publish camera data\r\n        self.publish_camera_data(header)\r\n\r\n        # Publish LIDAR data\r\n        self.publish_lidar_data(header)\r\n\r\n        # Publish IMU data\r\n        self.publish_imu_data(header)\r\n\r\n        # Publish status\r\n        self.publish_status(header)\r\n\r\n    def publish_odometry(self, header):\r\n        \"\"\"Publish odometry data\"\"\"\r\n        odom = Odometry()\r\n        odom.header = header\r\n        odom.child_frame_id = 'base_link'\r\n\r\n        # Set position\r\n        odom.pose.pose.position.x = self.robot_pose['x']\r\n        odom.pose.pose.position.y = self.robot_pose['y']\r\n        odom.pose.pose.position.z = 0.0\r\n\r\n        # Convert theta to quaternion\r\n        from math import sin, cos\r\n        siny_cosp = 2 * (1 * sin(self.robot_pose['theta']/2))\r\n        cosy_cosp = 1 - 2 * (sin(self.robot_pose['theta']/2) ** 2)\r\n        yaw = np.arctan2(siny_cosp, cosy_cosp)\r\n\r\n        odom.pose.pose.orientation.z = sin(self.robot_pose['theta'] / 2)\r\n        odom.pose.pose.orientation.w = cos(self.robot_pose['theta'] / 2)\r\n\r\n        # Set velocities\r\n        odom.twist.twist.linear.x = self.robot_twist['linear_x']\r\n        odom.twist.twist.angular.z = self.robot_twist['angular_z']\r\n\r\n        self.odom_pub.publish(odom)\r\n\r\n    def publish_camera_data(self, header):\r\n        \"\"\"Publish simulated camera data\"\"\"\r\n        # In Isaac Sim, this would get real camera data\r\n        # For simulation, create a dummy image\r\n        width, height = 640, 480\r\n\r\n        # Create a simple test image\r\n        image_data = np.zeros((height, width, 3), dtype=np.uint8)\r\n\r\n        # Add some features to the image\r\n        cv2.rectangle(image_data, (100, 100), (200, 200), (255, 0, 0), 2)  # Blue square\r\n        cv2.circle(image_data, (400, 300), 50, (0, 255, 0), 2)  # Green circle\r\n\r\n        # Convert to ROS Image message\r\n        from cv_bridge import CvBridge\r\n        bridge = CvBridge()\r\n        image_msg = bridge.cv2_to_imgmsg(image_data, encoding='bgr8')\r\n        image_msg.header = header\r\n\r\n        self.camera_pub.publish(image_msg)\r\n\r\n    def publish_lidar_data(self, header):\r\n        \"\"\"Publish simulated LIDAR data\"\"\"\r\n        # Create simulated LIDAR scan\r\n        scan = LaserScan()\r\n        scan.header = header\r\n        scan.angle_min = -np.pi\r\n        scan.angle_max = np.pi\r\n        scan.angle_increment = 2 * np.pi / 360  # 360 points\r\n        scan.time_increment = 0.0\r\n        scan.scan_time = 0.1  # 10Hz\r\n        scan.range_min = 0.1\r\n        scan.range_max = 10.0\r\n\r\n        # Generate simulated ranges (with some obstacles)\r\n        num_ranges = 360\r\n        ranges = []\r\n\r\n        for i in range(num_ranges):\r\n            angle = scan.angle_min + i * scan.angle_increment\r\n\r\n            # Simulate some obstacles\r\n            distance = scan.range_max  # Default to max range\r\n\r\n            # Add some obstacles at specific angles\r\n            if 80 < i < 100:  # Front left\r\n                distance = 2.0\r\n            elif 260 < i < 280:  # Rear right\r\n                distance = 1.5\r\n            elif 170 < i < 190:  # Directly behind\r\n                distance = 0.8\r\n\r\n            ranges.append(distance)\r\n\r\n        scan.ranges = ranges\r\n        scan.intensities = [100.0] * num_ranges  # Constant intensity\r\n\r\n        self.lidar_pub.publish(scan)\r\n\r\n    def publish_imu_data(self, header):\r\n        \"\"\"Publish simulated IMU data\"\"\"\r\n        imu = Imu()\r\n        imu.header = header\r\n\r\n        # Set orientation (simplified)\r\n        imu.orientation.z = np.sin(self.robot_pose['theta'] / 2)\r\n        imu.orientation.w = np.cos(self.robot_pose['theta'] / 2)\r\n\r\n        # Set angular velocity (based on robot's angular velocity)\r\n        imu.angular_velocity.z = self.robot_twist['angular_z']\r\n\r\n        # Set linear acceleration (simplified)\r\n        imu.linear_acceleration.x = self.robot_twist['linear_x'] * 2.0  # Assume some acceleration\r\n\r\n        self.imu_pub.publish(imu)\r\n\r\n    def publish_status(self, header):\r\n        \"\"\"Publish simulation status\"\"\"\r\n        status_msg = String()\r\n        status_msg.data = f\"Running - Pose:({self.robot_pose['x']:.2f},{self.robot_pose['y']:.2f},{self.robot_pose['theta']:.2f})\"\r\n        self.status_pub.publish(status_msg)\r\n\r\n    def cleanup(self):\r\n        \"\"\"Clean up resources\"\"\"\r\n        if self.isaac_world is not None:\r\n            self.isaac_world.clear()\r\n        self.get_logger().info('Isaac Sim to ROS2 Bridge cleaned up')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = IsaacSimROS2Bridge()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('Shutting down Isaac Sim to ROS2 bridge...')\r\n    finally:\r\n        node.cleanup()\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-navigation-stack",children:"Isaac ROS Navigation Stack"}),"\n",(0,t.jsx)(n.p,{children:"GPU-accelerated navigation stack combining Isaac's capabilities with ROS2 navigation framework:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nIsaac ROS Navigation Stack\r\nGPU-accelerated navigation combining Isaac capabilities with ROS2\r\n"""\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import PoseStamped, Twist, Point\r\nfrom nav_msgs.msg import Path, OccupancyGrid, Odometry\r\nfrom sensor_msgs.msg import LaserScan, Image\r\nfrom visualization_msgs.msg import Marker, MarkerArray\r\nfrom std_msgs.msg import Header, Bool, Float64\r\nimport numpy as np\r\nimport math\r\nimport heapq\r\nfrom typing import List, Tuple\r\nimport time\r\n\r\nclass IsaacROSNavigationStack(Node):\r\n    def __init__(self):\r\n        super().__init__(\'isaac_ros_navigation\')\r\n\r\n        # Publishers\r\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        self.local_plan_pub = self.create_publisher(Path, \'/local_plan\', 10)\r\n        self.global_plan_pub = self.create_publisher(Path, \'/global_plan\', 10)\r\n        self.path_marker_pub = self.create_publisher(MarkerArray, \'/path_markers\', 10)\r\n        self.status_pub = self.create_publisher(Bool, \'/navigation_active\', 10)\r\n        self.progress_pub = self.create_publisher(Float64, \'/navigation_progress\', 10)\r\n\r\n        # Subscribers\r\n        self.goal_sub = self.create_subscription(\r\n            PoseStamped, \'/move_base_simple/goal\', self.goal_callback, 10)\r\n        self.odom_sub = self.create_subscription(\r\n            Odometry, \'/odom\', self.odom_callback, 10)\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan, \'/scan\', self.scan_callback, 10)\r\n        self.map_sub = self.create_subscription(\r\n            OccupancyGrid, \'/map\', self.map_callback, 10)\r\n\r\n        # Timer for navigation control\r\n        self.nav_timer = self.create_timer(0.05, self.navigation_callback)  # 20Hz\r\n\r\n        # Navigation state\r\n        self.current_pose = None\r\n        self.current_twist = None\r\n        self.goal_pose = None\r\n        self.global_path = []\r\n        self.local_path = []\r\n        self.navigation_active = False\r\n        self.path_index = 0\r\n        self.obstacle_threshold = 0.5  # meters\r\n\r\n        # Isaac-specific navigation parameters\r\n        self.isaac_nav_params = {\r\n            \'global_planner\': \'isaac_global_planner\',\r\n            \'local_planner\': \'isaac_local_planner\',\r\n            \'costmap_resolution\': 0.05,  # meters per cell\r\n            \'robot_radius\': 0.3,  # meters\r\n            \'max_linear_vel\': 0.5,  # m/s\r\n            \'max_angular_vel\': 1.0,  # rad/s\r\n            \'min_obstacle_dist\': 0.5  # meters\r\n        }\r\n\r\n        # Costmap for obstacle avoidance\r\n        self.costmap = None\r\n        self.map_origin = None\r\n        self.map_resolution = None\r\n\r\n        # Performance metrics\r\n        self.start_time = None\r\n        self.path_length = 0.0\r\n\r\n        self.get_logger().info(\'Isaac ROS Navigation Stack initialized\')\r\n\r\n    def goal_callback(self, msg):\r\n        """Handle navigation goal from ROS2"""\r\n        self.goal_pose = msg.pose\r\n        self.navigation_active = True\r\n        self.start_time = time.time()\r\n        self.path_length = 0.0\r\n\r\n        self.get_logger().info(f\'Navigation goal received: ({msg.pose.position.x:.2f}, {msg.pose.position.y:.2f})\')\r\n\r\n        # Plan global path\r\n        if self.current_pose and self.costmap is not None:\r\n            self.plan_global_path()\r\n\r\n    def odom_callback(self, msg):\r\n        """Update robot pose from odometry"""\r\n        self.current_pose = msg.pose.pose\r\n        self.current_twist = msg.twist.twist\r\n\r\n    def scan_callback(self, msg):\r\n        """Process laser scan data"""\r\n        # Update local costmap with scan data\r\n        self.update_local_costmap(msg)\r\n\r\n    def map_callback(self, msg):\r\n        """Process global map data"""\r\n        self.costmap = np.array(msg.data).reshape(msg.info.height, msg.info.width)\r\n        self.map_origin = (msg.info.origin.position.x, msg.info.origin.position.y)\r\n        self.map_resolution = msg.info.resolution\r\n\r\n        self.get_logger().info(f\'Global map received: {msg.info.width}x{msg.info.height}, res: {self.map_resolution}\')\r\n\r\n    def navigation_callback(self):\r\n        """Main navigation control loop"""\r\n        if not self.navigation_active or not self.current_pose or not self.goal_pose:\r\n            # Stop robot if navigation is not active\r\n            if self.navigation_active:\r\n                self.stop_robot()\r\n                self.navigation_active = False\r\n            return\r\n\r\n        # Check if goal is reached\r\n        if self.is_goal_reached():\r\n            self.get_logger().info(\'Navigation goal reached!\')\r\n            self.navigation_complete()\r\n            return\r\n\r\n        # Plan local path if needed\r\n        if not self.local_path:\r\n            self.plan_local_path()\r\n\r\n        # Execute local path following\r\n        if self.local_path:\r\n            cmd_vel = self.follow_local_path()\r\n            if cmd_vel:\r\n                self.cmd_vel_pub.publish(cmd_vel)\r\n                self.publish_status(True)\r\n            else:\r\n                # Could not follow path, stop and replan\r\n                self.stop_robot()\r\n                self.plan_global_path()\r\n        else:\r\n            # No path available, stop robot\r\n            self.stop_robot()\r\n\r\n    def plan_global_path(self):\r\n        """Plan global path using Isaac\'s GPU-accelerated path planner (simulated)"""\r\n        if not self.current_pose or not self.goal_pose or self.costmap is None:\r\n            return\r\n\r\n        # Convert poses to map coordinates\r\n        start = self.pose_to_map_coords(self.current_pose)\r\n        goal = self.pose_to_map_coords(self.goal_pose)\r\n\r\n        # In Isaac, this would use GPU-accelerated A* or Dijkstra\'s algorithm\r\n        # For simulation, we\'ll use a simplified A* implementation\r\n        path = self.a_star_search(start, goal)\r\n\r\n        if path:\r\n            # Convert path back to world coordinates\r\n            self.global_path = [self.map_coords_to_world(p) for p in path]\r\n            self.path_index = 0\r\n\r\n            # Publish global plan\r\n            self.publish_global_path()\r\n\r\n            # Plan local path\r\n            self.plan_local_path()\r\n\r\n            self.get_logger().info(f\'Global path planned with {len(path)} waypoints\')\r\n        else:\r\n            self.get_logger().error(\'Failed to find global path to goal\')\r\n            self.navigation_active = False\r\n\r\n    def plan_local_path(self):\r\n        """Plan local path using Isaac\'s local planner"""\r\n        if not self.global_path or self.path_index >= len(self.global_path):\r\n            return\r\n\r\n        # Get waypoints ahead in global path\r\n        current_pos = (self.current_pose.position.x, self.current_pose.position.y)\r\n        lookahead_distance = self.isaac_nav_params[\'path_lookahead\']\r\n\r\n        # Find waypoints within lookahead distance\r\n        local_waypoints = []\r\n        for i in range(self.path_index, len(self.global_path)):\r\n            wp = self.global_path[i]\r\n            dist = math.sqrt((wp[0] - current_pos[0])**2 + (wp[1] - current_pos[1])**2)\r\n\r\n            if dist <= lookahead_distance:\r\n                local_waypoints.append(wp)\r\n            elif local_waypoints:  # If we have waypoints and this one is too far\r\n                break\r\n\r\n        if local_waypoints:\r\n            self.local_path = local_waypoints\r\n            self.publish_local_path()\r\n\r\n    def follow_local_path(self):\r\n        """Follow the local path with obstacle avoidance"""\r\n        if not self.local_path:\r\n            return None\r\n\r\n        # Get next waypoint in local path\r\n        if len(self.local_path) > 0:\r\n            target = self.local_path[0]\r\n        else:\r\n            return None\r\n\r\n        # Calculate required velocity to reach target\r\n        current_pos = (self.current_pose.position.x, self.current_pose.position.y)\r\n        target_pos = target\r\n\r\n        dx = target_pos[0] - current_pos[0]\r\n        dy = target_pos[1] - current_pos[1]\r\n        distance = math.sqrt(dx*dx + dy*dy)\r\n\r\n        cmd_vel = Twist()\r\n\r\n        # Check for obstacles using laser scan\r\n        if self.has_obstacle_ahead():\r\n            # Emergency stop or obstacle avoidance\r\n            cmd_vel.linear.x = 0.0\r\n            cmd_vel.angular.z = 0.3  # Turn to avoid\r\n            self.get_logger().warn(\'Obstacle detected ahead, executing avoidance maneuver\')\r\n        else:\r\n            # Calculate velocities\r\n            if distance > 0.1:  # If not close to waypoint\r\n                # Calculate linear velocity\r\n                linear_vel = min(distance * 0.5, self.isaac_nav_params[\'max_linear_vel\'])\r\n\r\n                # Calculate angular error\r\n                target_angle = math.atan2(dy, dx)\r\n                current_yaw = self.get_yaw_from_quaternion(self.current_pose.orientation)\r\n                angle_error = target_angle - current_yaw\r\n\r\n                # Normalize angle\r\n                while angle_error > math.pi:\r\n                    angle_error -= 2 * math.pi\r\n                while angle_error < -math.pi:\r\n                    angle_error += 2 * math.pi\r\n\r\n                angular_vel = max(min(angle_error * 1.0, self.isaac_nav_params[\'max_angular_vel\']),\r\n                                 -self.isaac_nav_params[\'max_angular_vel\'])\r\n\r\n                cmd_vel.linear.x = linear_vel\r\n                cmd_vel.angular.z = angular_vel\r\n\r\n                # Update path index if close to current waypoint\r\n                if distance < 0.3:\r\n                    self.local_path.pop(0)\r\n            else:\r\n                # Reached waypoint, move to next\r\n                self.local_path.pop(0)\r\n                if not self.local_path:\r\n                    # Need more waypoints from global path\r\n                    self.plan_local_path()\r\n\r\n        return cmd_vel\r\n\r\n    def a_star_search(self, start, goal):\r\n        """Simplified A* pathfinding algorithm"""\r\n        # In Isaac, this would be GPU-accelerated\r\n        # This is a simplified implementation for demonstration\r\n\r\n        def heuristic(a, b):\r\n            return abs(a[0] - b[0]) + abs(a[1] - b[1])\r\n\r\n        def get_neighbors(pos):\r\n            neighbors = []\r\n            for dx, dy in [(-1,0), (1,0), (0,-1), (0,1), (-1,-1), (-1,1), (1,-1), (1,1)]:\r\n                nx, ny = pos[0] + dx, pos[1] + dy\r\n                if 0 <= nx < self.costmap.shape[1] and 0 <= ny < self.costmap.shape[0]:\r\n                    if self.costmap[ny, nx] < 50:  # Not an obstacle\r\n                        neighbors.append((nx, ny))\r\n            return neighbors\r\n\r\n        frontier = [(0, start)]\r\n        came_from = {start: None}\r\n        cost_so_far = {start: 0}\r\n\r\n        while frontier:\r\n            current = heapq.heappop(frontier)[1]\r\n\r\n            if current == goal:\r\n                break\r\n\r\n            for next_pos in get_neighbors(current):\r\n                new_cost = cost_so_far[current] + 1  # Simple cost model\r\n\r\n                if next_pos not in cost_so_far or new_cost < cost_so_far[next_pos]:\r\n                    cost_so_far[next_pos] = new_cost\r\n                    priority = new_cost + heuristic(goal, next_pos)\r\n                    heapq.heappush(frontier, (priority, next_pos))\r\n                    came_from[next_pos] = current\r\n\r\n        # Reconstruct path\r\n        path = []\r\n        current = goal\r\n        while current != start:\r\n            path.append(current)\r\n            current = came_from.get(current)\r\n            if current is None:\r\n                return []  # No path found\r\n        path.append(start)\r\n        path.reverse()\r\n\r\n        return path\r\n\r\n    def has_obstacle_ahead(self):\r\n        """Check if there are obstacles ahead using laser scan"""\r\n        if not hasattr(self, \'laser_data\'):\r\n            return False\r\n\r\n        # Check laser readings in front of robot (within obstacle threshold)\r\n        # This is a simplified check - in Isaac, this would use GPU-accelerated processing\r\n        if hasattr(self, \'laser_data\'):\r\n            # Check the front 30-degree sector\r\n            front_start = len(self.laser_data.ranges) // 2 - 15\r\n            front_end = len(self.laser_data.ranges) // 2 + 15\r\n\r\n            for i in range(front_start, front_end):\r\n                if i < len(self.laser_data.ranges):\r\n                    if 0 < self.laser_data.ranges[i] < self.obstacle_threshold:\r\n                        return True\r\n\r\n        return False\r\n\r\n    def update_local_costmap(self, scan_msg):\r\n        """Update local costmap with laser scan data"""\r\n        # In Isaac, this would use GPU-accelerated costmap updates\r\n        # For simulation, we\'ll store the scan data\r\n        self.laser_data = scan_msg\r\n\r\n    def is_goal_reached(self):\r\n        """Check if the robot has reached the goal"""\r\n        if not self.current_pose or not self.goal_pose:\r\n            return False\r\n\r\n        dx = self.goal_pose.position.x - self.current_pose.position.x\r\n        dy = self.goal_pose.position.y - self.current_pose.position.y\r\n        distance = math.sqrt(dx*dx + dy*dy)\r\n\r\n        return distance < 0.3  # 30cm tolerance\r\n\r\n    def navigation_complete(self):\r\n        """Handle navigation completion"""\r\n        self.stop_robot()\r\n        self.navigation_active = False\r\n\r\n        if self.start_time:\r\n            elapsed_time = time.time() - self.start_time\r\n            self.get_logger().info(f\'Navigation completed in {elapsed_time:.2f} seconds\')\r\n\r\n        # Publish completion status\r\n        self.publish_status(False)\r\n\r\n    def stop_robot(self):\r\n        """Stop the robot"""\r\n        cmd_vel = Twist()\r\n        cmd_vel.linear.x = 0.0\r\n        cmd_vel.angular.z = 0.0\r\n        self.cmd_vel_pub.publish(cmd_vel)\r\n\r\n    def pose_to_map_coords(self, pose):\r\n        """Convert world pose to map coordinates"""\r\n        if self.map_origin is None or self.map_resolution is None:\r\n            return (0, 0)\r\n\r\n        map_x = int((pose.position.x - self.map_origin[0]) / self.map_resolution)\r\n        map_y = int((pose.position.y - self.map_origin[1]) / self.map_resolution)\r\n\r\n        return (map_x, map_y)\r\n\r\n    def map_coords_to_world(self, coords):\r\n        """Convert map coordinates to world coordinates"""\r\n        if self.map_origin is None or self.map_resolution is None:\r\n            return (0, 0)\r\n\r\n        world_x = coords[0] * self.map_resolution + self.map_origin[0]\r\n        world_y = coords[1] * self.map_resolution + self.map_origin[1]\r\n\r\n        return (world_x, world_y)\r\n\r\n    def get_yaw_from_quaternion(self, quat):\r\n        """Extract yaw angle from quaternion"""\r\n        siny_cosp = 2 * (quat.w * quat.z + quat.x * quat.y)\r\n        cosy_cosp = 1 - 2 * (quat.y * quat.y + quat.z * quat.z)\r\n        return math.atan2(siny_cosp, cosy_cosp)\r\n\r\n    def publish_global_path(self):\r\n        """Publish global path to visualization"""\r\n        path_msg = Path()\r\n        path_msg.header.stamp = self.get_clock().now().to_msg()\r\n        path_msg.header.frame_id = \'map\'\r\n\r\n        for point in self.global_path:\r\n            pose_stamped = PoseStamped()\r\n            pose_stamped.header = path_msg.header\r\n            pose_stamped.pose.position.x = point[0]\r\n            pose_stamped.pose.position.y = point[1]\r\n            pose_stamped.pose.orientation.w = 1.0\r\n            path_msg.poses.append(pose_stamped)\r\n\r\n        self.global_plan_pub.publish(path_msg)\r\n\r\n    def publish_local_path(self):\r\n        """Publish local path to visualization"""\r\n        path_msg = Path()\r\n        path_msg.header.stamp = self.get_clock().now().to_msg()\r\n        path_msg.header.frame_id = \'map\'\r\n\r\n        for point in self.local_path:\r\n            pose_stamped = PoseStamped()\r\n            pose_stamped.header = path_msg.header\r\n            pose_stamped.pose.position.x = point[0]\r\n            pose_stamped.pose.position.y = point[1]\r\n            pose_stamped.pose.orientation.w = 1.0\r\n            path_msg.poses.append(pose_stamped)\r\n\r\n        self.local_plan_pub.publish(path_msg)\r\n\r\n    def publish_status(self, active):\r\n        """Publish navigation status"""\r\n        status_msg = Bool()\r\n        status_msg.data = active\r\n        self.status_pub.publish(status_msg)\r\n\r\n        # Publish progress\r\n        progress_msg = Float64()\r\n        if self.start_time:\r\n            progress_msg.data = time.time() - self.start_time\r\n        else:\r\n            progress_msg.data = 0.0\r\n        self.progress_pub.publish(progress_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = IsaacROSNavigationStack()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info(\'Shutting down Isaac ROS navigation stack...\')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-perception-pipeline",children:"Isaac ROS Perception Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"Students implement a complete perception pipeline using Isaac ROS packages integrated with ROS2 navigation stack."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Objectives:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Integrate Isaac ROS perception nodes with ROS2 ecosystem"}),"\n",(0,t.jsx)(n.li,{children:"Implement GPU-accelerated object detection and tracking"}),"\n",(0,t.jsx)(n.li,{children:"Connect perception output to navigation decision making"}),"\n",(0,t.jsx)(n.li,{children:"Optimize performance for real-time operation"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Required Components:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"NVIDIA GPU with CUDA support"}),"\n",(0,t.jsx)(n.li,{children:"Isaac ROS packages"}),"\n",(0,t.jsx)(n.li,{children:"Camera and LIDAR sensors"}),"\n",(0,t.jsx)(n.li,{children:"Robot platform with ROS2"}),"\n",(0,t.jsx)(n.li,{children:"Pre-trained perception models"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Evaluation Criteria:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Perception accuracy and speed"}),"\n",(0,t.jsx)(n.li,{children:"Integration quality with ROS2"}),"\n",(0,t.jsx)(n.li,{children:"Real-time performance"}),"\n",(0,t.jsx)(n.li,{children:"Navigation improvement from perception"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"isaac-sim-hil-testing",children:"Isaac Sim HIL Testing"}),"\n",(0,t.jsx)(n.p,{children:"Students create a hardware-in-the-loop testing system connecting real robot hardware to Isaac Sim."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Objectives:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Set up Isaac Sim to ROS2 bridge"}),"\n",(0,t.jsx)(n.li,{children:"Connect real robot hardware to simulation"}),"\n",(0,t.jsx)(n.li,{children:"Implement safety mechanisms for HIL testing"}),"\n",(0,t.jsx)(n.li,{children:"Validate robot behavior in simulation vs reality"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Required Components:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Real robot platform"}),"\n",(0,t.jsx)(n.li,{children:"Isaac Sim installation"}),"\n",(0,t.jsx)(n.li,{children:"Network infrastructure"}),"\n",(0,t.jsx)(n.li,{children:"Safety monitoring systems"}),"\n",(0,t.jsx)(n.li,{children:"Real and simulated sensors"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Evaluation Criteria:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"System safety and reliability"}),"\n",(0,t.jsx)(n.li,{children:"Simulation fidelity"}),"\n",(0,t.jsx)(n.li,{children:"Hardware integration quality"}),"\n",(0,t.jsx)(n.li,{children:"Testing effectiveness"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"hybrid-navigation-system",children:"Hybrid Navigation System"}),"\n",(0,t.jsx)(n.p,{children:"Students develop a navigation system combining Isaac's GPU-accelerated planning with ROS2's mature tools."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Objectives:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Integrate Isaac's GPU planners with ROS2 navigation"}),"\n",(0,t.jsx)(n.li,{children:"Implement dynamic obstacle avoidance"}),"\n",(0,t.jsx)(n.li,{children:"Create monitoring and visualization tools"}),"\n",(0,t.jsx)(n.li,{children:"Validate performance improvements"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Required Components:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Mobile robot platform"}),"\n",(0,t.jsx)(n.li,{children:"Navigation sensors"}),"\n",(0,t.jsx)(n.li,{children:"Isaac navigation packages"}),"\n",(0,t.jsx)(n.li,{children:"ROS2 navigation stack"}),"\n",(0,t.jsx)(n.li,{children:"Performance monitoring tools"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Evaluation Criteria:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Navigation performance improvement"}),"\n",(0,t.jsx)(n.li,{children:"System integration quality"}),"\n",(0,t.jsx)(n.li,{children:"GPU utilization efficiency"}),"\n",(0,t.jsx)(n.li,{children:"Robustness to dynamic obstacles"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Chapter 14 covered the fundamentals of Gazebo simulation, including robot modeling with URDF, environment design with SDF, and integration with ROS2. Students learned about Gazebo's architecture, physics engine, and plugin system. Through practical examples, they gained hands-on experience with creating and controlling robots in simulation environments."}),"\n",(0,t.jsx)(n.h2,{id:"quiz",children:"Quiz"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What does URDF stand for?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A: Unified Robot Description Format"}),"\n",(0,t.jsx)(n.li,{children:"B: Universal Robot Design Framework"}),"\n",(0,t.jsx)(n.li,{children:"C: Unified Robotics Development Format"}),"\n",(0,t.jsx)(n.li,{children:"D: Universal Robot Description File"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer: A"})," - URDF stands for Unified Robot Description Format, which is used to describe robot geometry, kinematics, and dynamics."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What does SDF stand for?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A: Simulation Development Format"}),"\n",(0,t.jsx)(n.li,{children:"B: Simulation Description Format"}),"\n",(0,t.jsx)(n.li,{children:"C: System Design Framework"}),"\n",(0,t.jsx)(n.li,{children:"D: Sensor Definition Format"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer: B"})," - SDF stands for Simulation Description Format, which is used to describe simulation worlds and objects in Gazebo."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What is the primary purpose of Gazebo plugins?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A: To make Gazebo run faster"}),"\n",(0,t.jsx)(n.li,{children:"B: To extend Gazebo functionality with custom code"}),"\n",(0,t.jsx)(n.li,{children:"C: To reduce memory usage"}),"\n",(0,t.jsx)(n.li,{children:"D: To create 3D models"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer: B"})," - Gazebo plugins are used to extend Gazebo functionality with custom code, such as sensor models, controllers, and other extensions."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Which physics engines can be used with Gazebo?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A: Only ODE"}),"\n",(0,t.jsx)(n.li,{children:"B: ODE, Bullet, and DART"}),"\n",(0,t.jsx)(n.li,{children:"C: Only Bullet"}),"\n",(0,t.jsx)(n.li,{children:"D: Only DART"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer: B"})," - Gazebo supports multiple physics engines including ODE, Bullet, and DART."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What is the typical update rate for IMU sensors in Gazebo?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A: 10 Hz"}),"\n",(0,t.jsx)(n.li,{children:"B: 50 Hz"}),"\n",(0,t.jsx)(n.li,{children:"C: 100 Hz"}),"\n",(0,t.jsx)(n.li,{children:"D: 1000 Hz"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer: C"})," - IMU sensors in Gazebo typically have an update rate of 100 Hz, which matches many real IMU sensors."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,t.jsx)(n.p,{children:"After completing this chapter, students will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Create simulation environments for robot testing"}),"\n",(0,t.jsx)(n.li,{children:"Implement physics-based simulations"}),"\n",(0,t.jsx)(n.li,{children:"Bridge simulation and reality"}),"\n",(0,t.jsx)(n.li,{children:"Validate robot behaviors in simulation"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Basic understanding of Python programming"}),"\n",(0,t.jsx)(n.li,{children:"Fundamentals of linear algebra and calculus"}),"\n",(0,t.jsx)(n.li,{children:"Basic knowledge of robotics concepts"}),"\n",(0,t.jsx)(n.li,{children:"Introduction to machine learning concepts"}),"\n",(0,t.jsx)(n.li,{children:"Completion of Module 0 (Introduction and Foundations)"}),"\n",(0,t.jsx)(n.li,{children:"Completion of Chapter 01 (Physical AI Basics)"}),"\n",(0,t.jsx)(n.li,{children:"Completion of Chapter 03 (ROS2 Nodes, Topics & Services)"}),"\n",(0,t.jsx)(n.li,{children:"Completion of Chapter 11 (Introduction to NVIDIA Isaac)"}),"\n",(0,t.jsx)(n.li,{children:"Completion of Chapter 12 (Isaac SDK & APIs)"}),"\n",(0,t.jsx)(n.li,{children:"Completion of Chapter 13 (Isaac Robot Simulation Examples)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"estimated-duration",children:"Estimated Duration"}),"\n",(0,t.jsx)(n.p,{children:"5 hours"})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>o});var a=r(6540);const t={},s=a.createContext(t);function i(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);