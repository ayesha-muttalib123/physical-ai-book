"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[97],{8453:(n,e,t)=>{t.d(e,{R:()=>i,x:()=>s});var r=t(6540);const a={},o=r.createContext(a);function i(n){const e=r.useContext(o);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:i(n.components),r.createElement(o.Provider,{value:e},n.children)}},9253:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>i,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"20-Chapter-4-Language-Action-Integration","title":"Chapter 4: Language-Action Integration","description":"Overview","source":"@site/docusaurus/docs/20-Chapter-4-Language-Action-Integration.md","sourceDirName":".","slug":"/20-Chapter-4-Language-Action-Integration","permalink":"/physical-ai-book/ur/docs/20-Chapter-4-Language-Action-Integration","draft":false,"unlisted":false,"editUrl":"https://github.com/ayesha-muttalib123/physical-ai-book/tree/main/docusaurus/docs/20-Chapter-4-Language-Action-Integration.md","tags":[],"version":"current","sidebarPosition":20,"frontMatter":{"id":"20-Chapter-4-Language-Action-Integration","title":"Chapter 4: Language-Action Integration","sidebar_position":20},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Vision-Based Navigation Examples","permalink":"/physical-ai-book/ur/docs/19-Chapter-3-Vision-Based-Navigation-Examples"},"next":{"title":"Humanoid Project Examples & Capstone","permalink":"/physical-ai-book/ur/docs/21-humanoid-project-examples-capstone"}}');var a=t(4848),o=t(8453);const i={id:"20-Chapter-4-Language-Action-Integration",title:"Chapter 4: Language-Action Integration",sidebar_position:20},s="Chapter 4: Language-Action Integration",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Why It Matters",id:"why-it-matters",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Natural Language Understanding",id:"natural-language-understanding",level:3},{value:"Semantic Parsing",id:"semantic-parsing",level:3},{value:"Command Grounding",id:"command-grounding",level:3},{value:"Action Planning",id:"action-planning",level:3},{value:"Context Awareness",id:"context-awareness",level:3},{value:"Ambiguity Resolution",id:"ambiguity-resolution",level:3},{value:"Feedback Integration",id:"feedback-integration",level:3},{value:"Multimodal Integration",id:"multimodal-integration",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Natural Language Command Parser",id:"natural-language-command-parser",level:3},{value:"Context-Aware Command Interpreter",id:"context-aware-command-interpreter",level:3},{value:"Language-Guided Task Planning",id:"language-guided-task-planning",level:3},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Human-Robot Command Interface",id:"human-robot-command-interface",level:3},{value:"Context-Aware Navigation System",id:"context-aware-navigation-system",level:3},{value:"Task Planning from Language",id:"task-planning-from-language",level:3},{value:"Summary",id:"summary",level:2},{value:"Quiz",id:"quiz",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Estimated Duration",id:"estimated-duration",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"chapter-4-language-action-integration",children:"Chapter 4: Language-Action Integration"})}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(e.p,{children:"This chapter focuses on the integration of natural language understanding with robotic action execution, enabling robots to interpret human commands and execute corresponding physical actions. Students will learn about natural language processing for robotics, command parsing, action planning, and execution frameworks. The chapter covers both symbolic approaches using semantic parsing and neural approaches using end-to-end learning, with emphasis on creating robust systems that can handle ambiguous or complex human instructions."}),"\n",(0,a.jsx)(e.h2,{id:"why-it-matters",children:"Why It Matters"}),"\n",(0,a.jsx)(e.p,{children:"Language-action integration is essential for creating robots that can interact naturally with humans in everyday environments. This capability allows robots to understand and execute complex tasks described in natural language, making them more accessible to non-expert users. Understanding this integration is crucial for developing robots that can work alongside humans, follow instructions, and perform tasks in response to verbal commands in real-world settings."}),"\n",(0,a.jsx)(e.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,a.jsx)(e.h3,{id:"natural-language-understanding",children:"Natural Language Understanding"}),"\n",(0,a.jsx)(e.p,{children:"Processing and interpreting human language commands. This involves converting natural language into structured representations that can be understood and acted upon by robotic systems."}),"\n",(0,a.jsx)(e.h3,{id:"semantic-parsing",children:"Semantic Parsing"}),"\n",(0,a.jsx)(e.p,{children:"Converting language to structured action representations. This process translates natural language commands into formal structures that represent the intended action, objects, and relationships."}),"\n",(0,a.jsx)(e.h3,{id:"command-grounding",children:"Command Grounding"}),"\n",(0,a.jsx)(e.p,{children:"Connecting language commands to physical actions. This involves mapping abstract language concepts to concrete robot behaviors and environmental entities."}),"\n",(0,a.jsx)(e.h3,{id:"action-planning",children:"Action Planning"}),"\n",(0,a.jsx)(e.p,{children:"Sequencing actions to accomplish complex tasks. This includes breaking down complex commands into executable steps and ensuring proper sequencing and coordination."}),"\n",(0,a.jsx)(e.h3,{id:"context-awareness",children:"Context Awareness"}),"\n",(0,a.jsx)(e.p,{children:"Understanding commands in environmental context. This involves using environmental information to interpret ambiguous or incomplete language commands correctly."}),"\n",(0,a.jsx)(e.h3,{id:"ambiguity-resolution",children:"Ambiguity Resolution"}),"\n",(0,a.jsx)(e.p,{children:"Handling ambiguous or incomplete instructions. This includes using context, prior knowledge, and disambiguation strategies to interpret unclear commands."}),"\n",(0,a.jsx)(e.h3,{id:"feedback-integration",children:"Feedback Integration"}),"\n",(0,a.jsx)(e.p,{children:"Using action outcomes to refine language understanding. This involves updating the language understanding system based on the results of executed actions."}),"\n",(0,a.jsx)(e.h3,{id:"multimodal-integration",children:"Multimodal Integration"}),"\n",(0,a.jsx)(e.p,{children:"Combining language with visual and spatial information. This includes integrating linguistic input with perceptual data to improve command interpretation and execution."}),"\n",(0,a.jsx)(e.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,a.jsx)(e.h3,{id:"natural-language-command-parser",children:"Natural Language Command Parser"}),"\n",(0,a.jsx)(e.p,{children:"Implementation of a natural language command parser for robotic actions:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nNatural Language Command Parser\r\nDemonstrates parsing natural language commands into robotic actions\r\n\"\"\"\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist, Pose\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\nimport spacy\r\nimport numpy as np\r\nimport math\r\nfrom typing import Dict, List, Tuple, Optional\r\nimport json\r\nimport uuid\r\n\r\nclass CommandParser:\r\n    \"\"\"Parse natural language commands into structured actions\"\"\"\r\n    def __init__(self):\r\n        # Load spaCy model (install with: python -m spacy download en_core_web_sm)\r\n        try:\r\n            self.nlp = spacy.load(\"en_core_web_sm\")\r\n        except OSError:\r\n            print(\"spaCy model not found. Install with: python -m spacy download en_core_web_sm\")\r\n            self.nlp = None\r\n\r\n        # Define action vocabulary\r\n        self.action_keywords = {\r\n            'move': ['go', 'move', 'walk', 'navigate', 'approach'],\r\n            'grasp': ['grasp', 'grab', 'pick up', 'take', 'hold'],\r\n            'place': ['place', 'put', 'set', 'release'],\r\n            'look': ['look', 'see', 'find', 'locate', 'search'],\r\n            'turn': ['turn', 'rotate', 'face', 'orient'],\r\n            'stop': ['stop', 'halt', 'pause']\r\n        }\r\n\r\n        # Define object categories\r\n        self.object_categories = [\r\n            'object', 'item', 'thing', 'box', 'cup', 'bottle', 'book',\r\n            'chair', 'table', 'door', 'window', 'person', 'robot'\r\n        ]\r\n\r\n        # Define spatial relations\r\n        self.spatial_relations = [\r\n            'to', 'toward', 'at', 'on', 'in', 'under', 'over',\r\n            'near', 'next to', 'beside', 'left', 'right', 'front', 'back'\r\n        ]\r\n\r\n    def parse_command(self, command: str) -> Dict:\r\n        \"\"\"Parse a natural language command into structured format\"\"\"\r\n        if self.nlp is None:\r\n            return self.fallback_parse(command)\r\n\r\n        doc = self.nlp(command.lower())\r\n\r\n        # Extract action\r\n        action = self.extract_action(doc)\r\n\r\n        # Extract object\r\n        obj = self.extract_object(doc)\r\n\r\n        # Extract location/direction\r\n        location = self.extract_location(doc)\r\n\r\n        # Extract spatial relations\r\n        relation = self.extract_spatial_relation(doc)\r\n\r\n        return {\r\n            'command': command,\r\n            'action': action,\r\n            'object': obj,\r\n            'location': location,\r\n            'relation': relation,\r\n            'entities': [(ent.text, ent.label_) for ent in doc.ents]\r\n        }\r\n\r\n    def fallback_parse(self, command: str) -> Dict:\r\n        \"\"\"Fallback parser if spaCy is not available\"\"\"\r\n        command_lower = command.lower()\r\n\r\n        # Simple keyword matching\r\n        action = None\r\n        for action_type, keywords in self.action_keywords.items():\r\n            for keyword in keywords:\r\n                if keyword in command_lower:\r\n                    action = action_type\r\n                    break\r\n            if action:\r\n                break\r\n\r\n        # Extract simple object mentions\r\n        obj = None\r\n        for category in self.object_categories:\r\n            if category in command_lower:\r\n                obj = category\r\n                break\r\n\r\n        # Extract spatial relations\r\n        relation = None\r\n        for relation_word in self.spatial_relations:\r\n            if relation_word in command_lower:\r\n                relation = relation_word\r\n                break\r\n\r\n        return {\r\n            'command': command,\r\n            'action': action,\r\n            'object': obj,\r\n            'location': None,\r\n            'relation': relation,\r\n            'entities': []\r\n        }\r\n\r\n    def extract_action(self, doc) -> Optional[str]:\r\n        \"\"\"Extract action from parsed document\"\"\"\r\n        for token in doc:\r\n            if token.pos_ in ['VERB', 'AUX'] and not token.is_stop:\r\n                for action_type, keywords in self.action_keywords.items():\r\n                    if any(keyword in [token.text, token.lemma_] for keyword in keywords):\r\n                        return action_type\r\n        return None\r\n\r\n    def extract_object(self, doc) -> Optional[str]:\r\n        \"\"\"Extract object from parsed document\"\"\"\r\n        for token in doc:\r\n            if token.pos_ in ['NOUN', 'PROPN'] and token.text in self.object_categories:\r\n                return token.text\r\n\r\n        # Look for compound nouns\r\n        for chunk in doc.noun_chunks:\r\n            if any(cat in chunk.text.lower() for cat in self.object_categories):\r\n                return chunk.text\r\n\r\n        return None\r\n\r\n    def extract_location(self, doc) -> Optional[str]:\r\n        \"\"\"Extract location from parsed document\"\"\"\r\n        for ent in doc.ents:\r\n            if ent.label_ in ['LOC', 'GPE', 'FAC']:  # Location, GeoPolitical Entity, Facility\r\n                return ent.text\r\n        return None\r\n\r\n    def extract_spatial_relation(self, doc) -> Optional[str]:\r\n        \"\"\"Extract spatial relation from parsed document\"\"\"\r\n        for token in doc:\r\n            if token.text in self.spatial_relations:\r\n                return token.text\r\n        return None\r\n\r\nclass ActionExecutor:\r\n    \"\"\"Execute parsed actions on the robot\"\"\"\r\n    def __init__(self):\r\n        self.action_functions = {\r\n            'move': self.execute_move,\r\n            'grasp': self.execute_grasp,\r\n            'place': self.execute_place,\r\n            'look': self.execute_look,\r\n            'turn': self.execute_turn,\r\n            'stop': self.execute_stop\r\n        }\r\n\r\n    def execute_action(self, parsed_command: Dict):\r\n        \"\"\"Execute action based on parsed command\"\"\"\r\n        action_type = parsed_command.get('action')\r\n        if action_type and action_type in self.action_functions:\r\n            return self.action_functions[action_type](parsed_command)\r\n        else:\r\n            print(f\"Unknown action: {action_type}\")\r\n            return False\r\n\r\n    def execute_move(self, parsed_command: Dict):\r\n        \"\"\"Execute move action\"\"\"\r\n        location = parsed_command.get('location')\r\n        relation = parsed_command.get('relation')\r\n        obj = parsed_command.get('object')\r\n\r\n        print(f\"Moving to {location or obj or 'destination'}\")\r\n        # In real implementation, this would generate movement commands\r\n        return True\r\n\r\n    def execute_grasp(self, parsed_command: Dict):\r\n        \"\"\"Execute grasp action\"\"\"\r\n        obj = parsed_command.get('object')\r\n        print(f\"Grasping {obj or 'object'}\")\r\n        # In real implementation, this would control the gripper\r\n        return True\r\n\r\n    def execute_place(self, parsed_command: Dict):\r\n        \"\"\"Execute place action\"\"\"\r\n        obj = parsed_command.get('object')\r\n        location = parsed_command.get('location')\r\n        print(f\"Placing {obj or 'object'} at {location or 'location'}\")\r\n        # In real implementation, this would control the gripper and arm\r\n        return True\r\n\r\n    def execute_look(self, parsed_command: Dict):\r\n        \"\"\"Execute look action\"\"\"\r\n        obj = parsed_command.get('object')\r\n        print(f\"Looking for {obj or 'object'}\")\r\n        # In real implementation, this would control the camera/neck\r\n        return True\r\n\r\n    def execute_turn(self, parsed_command: Dict):\r\n        \"\"\"Execute turn action\"\"\"\r\n        relation = parsed_command.get('relation')\r\n        print(f\"Turning {relation or 'around'}\")\r\n        # In real implementation, this would rotate the robot\r\n        return True\r\n\r\n    def execute_stop(self, parsed_command: Dict):\r\n        \"\"\"Execute stop action\"\"\"\r\n        print(\"Stopping robot\")\r\n        # In real implementation, this would stop all movement\r\n        return True\r\n\r\nclass LanguageActionNode(Node):\r\n    \"\"\"ROS2 node for language-action integration\"\"\"\r\n    def __init__(self):\r\n        super().__init__('language_action_node')\r\n\r\n        # Publishers and subscribers\r\n        self.command_sub = self.create_subscription(\r\n            String, '/robot_command', self.command_callback, 10)\r\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n        self.action_status_pub = self.create_publisher(String, '/action_status', 10)\r\n\r\n        # Initialize components\r\n        self.command_parser = CommandParser()\r\n        self.action_executor = ActionExecutor()\r\n        self.bridge = CvBridge()\r\n\r\n        # Robot state\r\n        self.current_task = None\r\n        self.task_queue = []\r\n\r\n        self.get_logger().info('Language-Action Integration Node initialized')\r\n\r\n    def command_callback(self, msg):\r\n        \"\"\"Process incoming natural language command\"\"\"\r\n        command = msg.data\r\n        self.get_logger().info(f'Received command: {command}')\r\n\r\n        try:\r\n            # Parse the command\r\n            parsed_command = self.command_parser.parse_command(command)\r\n\r\n            # Log parsed command\r\n            self.get_logger().info(f'Parsed command: {json.dumps(parsed_command, indent=2)}')\r\n\r\n            # Execute the action\r\n            success = self.action_executor.execute_action(parsed_command)\r\n\r\n            # Publish action status\r\n            status_msg = String()\r\n            status_msg.data = f\"Command: {command}, Success: {success}, Action: {parsed_command.get('action', 'none')}\"\r\n            self.action_status_pub.publish(status_msg)\r\n\r\n            if success:\r\n                self.get_logger().info(f'Command executed successfully: {command}')\r\n            else:\r\n                self.get_logger().error(f'Command execution failed: {command}')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing command: {e}')\r\n            status_msg = String()\r\n            status_msg.data = f\"Error processing command: {str(e)}\"\r\n            self.action_status_pub.publish(status_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = LanguageActionNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('Shutting down language-action node...')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,a.jsx)(e.h3,{id:"context-aware-command-interpreter",children:"Context-Aware Command Interpreter"}),"\n",(0,a.jsx)(e.p,{children:"Implementation of a context-aware system that interprets commands based on environmental context and robot state:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nContext-Aware Command Interpreter\r\nDemonstrates interpreting commands based on environmental context and robot state\r\n\"\"\"\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String, Bool\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom geometry_msgs.msg import Pose, Point\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport json\r\nfrom typing import Dict, List, Tuple, Optional\r\nimport time\r\n\r\nclass ContextManager:\r\n    \"\"\"Manage environmental and robot context for command interpretation\"\"\"\r\n    def __init__(self):\r\n        self.robot_pose = Pose()\r\n        self.robot_state = 'idle'  # idle, moving, grasping, etc.\r\n        self.visible_objects = []\r\n        self.known_locations = {\r\n            'kitchen': (-2.0, 0.0, 0.0),\r\n            'living_room': (2.0, 0.0, 0.0),\r\n            'bedroom': (0.0, 2.0, 0.0),\r\n            'office': (0.0, -2.0, 0.0)\r\n        }\r\n        self.object_locations = {}  # object_name: location\r\n        self.robot_capabilities = ['move', 'grasp', 'turn', 'look']\r\n        self.context_history = []\r\n\r\n    def update_robot_pose(self, pose: Pose):\r\n        \"\"\"Update robot's current pose\"\"\"\r\n        self.robot_pose = pose\r\n\r\n    def update_visible_objects(self, detections: List[Dict]):\r\n        \"\"\"Update list of visible objects\"\"\"\r\n        self.visible_objects = detections\r\n\r\n    def update_object_location(self, obj_name: str, location: Tuple[float, float, float]):\r\n        \"\"\"Update known location of an object\"\"\"\r\n        self.object_locations[obj_name] = location\r\n\r\n    def get_context(self) -> Dict:\r\n        \"\"\"Get current context information\"\"\"\r\n        return {\r\n            'robot_pose': {\r\n                'x': self.robot_pose.position.x,\r\n                'y': self.robot_pose.position.y,\r\n                'z': self.robot_pose.position.z\r\n            },\r\n            'robot_state': self.robot_state,\r\n            'visible_objects': [obj['name'] for obj in self.visible_objects],\r\n            'known_locations': self.known_locations,\r\n            'object_locations': self.object_locations,\r\n            'capabilities': self.robot_capabilities\r\n        }\r\n\r\n    def resolve_ambiguous_reference(self, obj_name: str) -> Optional[str]:\r\n        \"\"\"Resolve ambiguous object references based on context\"\"\"\r\n        # If exact match exists\r\n        if obj_name in self.object_locations:\r\n            return obj_name\r\n\r\n        # Look for similar objects in visible range\r\n        for obj in self.visible_objects:\r\n            if obj_name.lower() in obj['name'].lower():\r\n                return obj['name']\r\n\r\n        # Look for objects in known locations\r\n        for known_obj, location in self.object_locations.items():\r\n            if obj_name.lower() in known_obj.lower():\r\n                return known_obj\r\n\r\n        return None\r\n\r\n    def get_nearest_location(self, location_name: str) -> Optional[Tuple[float, float, float]]:\r\n        \"\"\"Get coordinates for a named location\"\"\"\r\n        if location_name in self.known_locations:\r\n            return self.known_locations[location_name]\r\n        return None\r\n\r\n    def calculate_navigation_target(self, obj_name: str, relation: str) -> Optional[Tuple[float, float, float]]:\r\n        \"\"\"Calculate navigation target based on object and spatial relation\"\"\"\r\n        # Resolve object reference\r\n        resolved_obj = self.resolve_ambiguous_reference(obj_name)\r\n        if not resolved_obj or resolved_obj not in self.object_locations:\r\n            return None\r\n\r\n        obj_location = self.object_locations[resolved_obj]\r\n\r\n        # Calculate target based on spatial relation\r\n        if relation in ['at', 'to']:\r\n            # Navigate to object location\r\n            return obj_location\r\n        elif relation in ['near', 'next to', 'beside']:\r\n            # Navigate close to object (1 meter away)\r\n            robot_pos = (self.robot_pose.position.x, self.robot_pose.position.y, self.robot_pose.position.z)\r\n            direction = np.array(obj_location) - np.array(robot_pos)\r\n            direction = direction / np.linalg.norm(direction)  # Normalize\r\n            target = np.array(obj_location) - 1.0 * direction  # 1m away from object\r\n            return tuple(target)\r\n        elif relation in ['left', 'right']:\r\n            # Navigate to left/right of object (simplified)\r\n            obj_x, obj_y, obj_z = obj_location\r\n            offset = 1.0 if relation == 'right' else -1.0\r\n            return (obj_x + offset, obj_y, obj_z)\r\n        else:\r\n            # Default to object location\r\n            return obj_location\r\n\r\nclass ContextualCommandInterpreter:\r\n    \"\"\"Interpret commands using environmental context\"\"\"\r\n    def __init__(self):\r\n        self.context_manager = ContextManager()\r\n        self.action_templates = {\r\n            'move': self.interpret_move_command,\r\n            'grasp': self.interpret_grasp_command,\r\n            'place': self.interpret_place_command,\r\n            'look': self.interpret_look_command\r\n        }\r\n\r\n    def interpret_command(self, command_struct: Dict) -> Dict:\r\n        \"\"\"Interpret command using context\"\"\"\r\n        action = command_struct.get('action')\r\n        obj = command_struct.get('object')\r\n        relation = command_struct.get('relation')\r\n        location = command_struct.get('location')\r\n\r\n        if action in self.action_templates:\r\n            return self.action_templates[action](command_struct)\r\n        else:\r\n            return {'action': 'unknown', 'parameters': {}}\r\n\r\n    def interpret_move_command(self, command_struct: Dict) -> Dict:\r\n        \"\"\"Interpret move command with context\"\"\"\r\n        obj = command_struct.get('object')\r\n        relation = command_struct.get('relation') or 'to'\r\n        location = command_struct.get('location')\r\n\r\n        if location:\r\n            # Move to named location\r\n            target = self.context_manager.get_nearest_location(location)\r\n            if target:\r\n                return {\r\n                    'action': 'navigate',\r\n                    'parameters': {\r\n                        'target_x': target[0],\r\n                        'target_y': target[1],\r\n                        'target_z': target[2]\r\n                    }\r\n                }\r\n\r\n        if obj:\r\n            # Move relative to object\r\n            target = self.context_manager.calculate_navigation_target(obj, relation)\r\n            if target:\r\n                return {\r\n                    'action': 'navigate',\r\n                    'parameters': {\r\n                        'target_x': target[0],\r\n                        'target_y': target[1],\r\n                        'target_z': target[2]\r\n                    }\r\n                }\r\n\r\n        # Default move action\r\n        return {\r\n            'action': 'move',\r\n            'parameters': {\r\n                'distance': 1.0,  # 1 meter forward\r\n                'direction': 'forward'\r\n            }\r\n        }\r\n\r\n    def interpret_grasp_command(self, command_struct: Dict) -> Dict:\r\n        \"\"\"Interpret grasp command with context\"\"\"\r\n        obj = command_struct.get('object')\r\n\r\n        if obj:\r\n            # Resolve object reference\r\n            resolved_obj = self.context_manager.resolve_ambiguous_reference(obj)\r\n            if resolved_obj:\r\n                # Check if object is visible\r\n                visible_obj = next(\r\n                    (o for o in self.context_manager.visible_objects if o['name'] == resolved_obj),\r\n                    None\r\n                )\r\n\r\n                if visible_obj:\r\n                    return {\r\n                        'action': 'grasp',\r\n                        'parameters': {\r\n                            'object_name': resolved_obj,\r\n                            'object_position': visible_obj.get('position', [0, 0, 0]),\r\n                            'confidence': visible_obj.get('confidence', 0.0)\r\n                        }\r\n                    }\r\n\r\n        return {\r\n            'action': 'search_and_grasp',\r\n            'parameters': {\r\n                'target_object': obj or 'unknown'\r\n            }\r\n        }\r\n\r\n    def interpret_place_command(self, command_struct: Dict) -> Dict:\r\n        \"\"\"Interpret place command with context\"\"\"\r\n        obj = command_struct.get('object')\r\n        location = command_struct.get('location') or command_struct.get('relation')\r\n\r\n        if location:\r\n            target = self.context_manager.get_nearest_location(location)\r\n            if target:\r\n                return {\r\n                    'action': 'navigate_and_place',\r\n                    'parameters': {\r\n                        'target_x': target[0],\r\n                        'target_y': target[1],\r\n                        'target_z': target[2],\r\n                        'object_to_place': obj or 'held_object'\r\n                    }\r\n                }\r\n\r\n        return {\r\n            'action': 'place',\r\n            'parameters': {\r\n                'object': obj or 'held_object',\r\n                'relative_position': 'current_position'\r\n            }\r\n        }\r\n\r\n    def interpret_look_command(self, command_struct: Dict) -> Dict:\r\n        \"\"\"Interpret look command with context\"\"\"\r\n        obj = command_struct.get('object')\r\n\r\n        if obj:\r\n            # Look for specific object\r\n            resolved_obj = self.context_manager.resolve_ambiguous_reference(obj)\r\n            if resolved_obj:\r\n                # Check if object location is known\r\n                if resolved_obj in self.context_manager.object_locations:\r\n                    obj_location = self.context_manager.object_locations[resolved_obj]\r\n                    return {\r\n                        'action': 'look_at_location',\r\n                        'parameters': {\r\n                            'target_x': obj_location[0],\r\n                            'target_y': obj_location[1],\r\n                            'target_z': obj_location[2]\r\n                        }\r\n                    }\r\n\r\n        return {\r\n            'action': 'scan_environment',\r\n            'parameters': {\r\n                'target_object': obj or 'any_object'\r\n            }\r\n        }\r\n\r\nclass ContextualLanguageActionNode(Node):\r\n    \"\"\"ROS2 node for contextual language-action integration\"\"\"\r\n    def __init__(self):\r\n        super().__init__('contextual_language_action')\r\n\r\n        # Publishers and subscribers\r\n        self.command_sub = self.create_subscription(\r\n            String, '/contextual_command', self.command_callback, 10)\r\n        self.pose_sub = self.create_subscription(\r\n            Pose, '/robot_pose', self.pose_callback, 10)\r\n        self.detection_sub = self.create_subscription(\r\n            Detection2DArray, '/object_detections', self.detection_callback, 10)\r\n        self.cmd_vel_pub = self.create_publisher(Pose, '/navigation_goal', 10)\r\n        self.action_pub = self.create_publisher(String, '/executed_action', 10)\r\n        self.context_pub = self.create_publisher(String, '/current_context', 10)\r\n\r\n        # Initialize components\r\n        self.interpreter = ContextualCommandInterpreter()\r\n        self.bridge = CvBridge()\r\n\r\n        # Timer for context updates\r\n        self.context_timer = self.create_timer(1.0, self.publish_context)\r\n\r\n        self.get_logger().info('Contextual Language-Action Node initialized')\r\n\r\n    def command_callback(self, msg):\r\n        \"\"\"Process contextual command\"\"\"\r\n        try:\r\n            # Parse command (assuming it's a JSON string with structured data)\r\n            command_data = json.loads(msg.data)\r\n            command_struct = command_data  # In a real system, this would come from NLP parser\r\n\r\n            # Interpret command using context\r\n            interpreted_action = self.interpreter.interpret_command(command_struct)\r\n\r\n            self.get_logger().info(f'Interpreted action: {interpreted_action}')\r\n\r\n            # Execute action (simplified)\r\n            self.execute_interpreted_action(interpreted_action)\r\n\r\n            # Publish executed action\r\n            action_msg = String()\r\n            action_msg.data = json.dumps(interpreted_action)\r\n            self.action_pub.publish(action_msg)\r\n\r\n        except json.JSONDecodeError:\r\n            # If not JSON, treat as simple string command\r\n            command_text = msg.data\r\n            self.get_logger().info(f'Received text command: {command_text}')\r\n            # In a real system, this would go through NLP parser first\r\n\r\n    def pose_callback(self, msg):\r\n        \"\"\"Update robot pose in context\"\"\"\r\n        self.interpreter.context_manager.update_robot_pose(msg)\r\n\r\n    def detection_callback(self, msg):\r\n        \"\"\"Update visible objects in context\"\"\"\r\n        detections = []\r\n        for detection in msg.detections:\r\n            obj_info = {\r\n                'name': detection.results[0].id if detection.results else 'unknown',\r\n                'confidence': detection.results[0].score if detection.results else 0.0,\r\n                'bbox': {\r\n                    'center_x': detection.bbox.center.x,\r\n                    'center_y': detection.bbox.center.y,\r\n                    'size_x': detection.bbox.size_x,\r\n                    'size_y': detection.bbox.size_y\r\n                }\r\n            }\r\n            detections.append(obj_info)\r\n\r\n        self.interpreter.context_manager.update_visible_objects(detections)\r\n\r\n    def execute_interpreted_action(self, action):\r\n        \"\"\"Execute the interpreted action\"\"\"\r\n        action_type = action.get('action')\r\n        params = action.get('parameters', {})\r\n\r\n        if action_type == 'navigate':\r\n            # Publish navigation goal\r\n            goal_pose = Pose()\r\n            goal_pose.position.x = params.get('target_x', 0.0)\r\n            goal_pose.position.y = params.get('target_y', 0.0)\r\n            goal_pose.position.z = params.get('target_z', 0.0)\r\n            self.cmd_vel_pub.publish(goal_pose)\r\n\r\n            self.get_logger().info(f'Navigating to ({goal_pose.position.x}, {goal_pose.position.y})')\r\n\r\n    def publish_context(self):\r\n        \"\"\"Publish current context\"\"\"\r\n        context = self.interpreter.context_manager.get_context()\r\n        context_msg = String()\r\n        context_msg.data = json.dumps(context, indent=2)\r\n        self.context_pub.publish(context_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = ContextualLanguageActionNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('Shutting down contextual language-action node...')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,a.jsx)(e.h3,{id:"language-guided-task-planning",children:"Language-Guided Task Planning"}),"\n",(0,a.jsx)(e.p,{children:"Implementation of task planning guided by natural language commands:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nLanguage-Guided Task Planning\r\nDemonstrates task planning guided by natural language commands\r\n\"\"\"\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom action_msgs.msg import GoalStatus\r\nfrom geometry_msgs.msg import Pose, Point\r\nfrom sensor_msgs.msg import JointState\r\nimport json\r\nfrom typing import List, Dict, Any, Optional, Callable\r\nfrom dataclasses import dataclass\r\nfrom enum import Enum\r\nimport time\r\nimport threading\r\n\r\nclass TaskStatus(Enum):\r\n    PENDING = \"pending\"\r\n    RUNNING = \"running\"\r\n    SUCCESS = \"success\"\r\n    FAILED = \"failed\"\r\n    CANCELLED = \"cancelled\"\r\n\r\n@dataclass\r\nclass TaskStep:\r\n    \"\"\"Represents a single step in a task plan\"\"\"\r\n    action: str\r\n    parameters: Dict[str, Any]\r\n    precondition: Callable[[], bool]\r\n    postcondition: Callable[[], bool]\r\n    description: str\r\n\r\nclass TaskPlanner:\r\n    \"\"\"Plan tasks based on language commands\"\"\"\r\n    def __init__(self):\r\n        self.tasks = {}\r\n        self.current_task_id = 0\r\n        self.robot_state = {\r\n            'position': (0.0, 0.0, 0.0),\r\n            'orientation': 0.0,\r\n            'gripper_state': 'open',  # open, closed\r\n            'holding_object': None,\r\n            'battery_level': 100.0\r\n        }\r\n\r\n    def create_delivery_task(self, start_location: str, end_location: str, object_name: str) -> int:\r\n        \"\"\"Create a delivery task plan\"\"\"\r\n        task_id = self.current_task_id\r\n        self.current_task_id += 1\r\n\r\n        # Define task steps\r\n        steps = [\r\n            TaskStep(\r\n                action='navigate',\r\n                parameters={'target_location': start_location},\r\n                precondition=lambda: self.robot_state['battery_level'] > 20,\r\n                postcondition=lambda: self._check_navigation_success(start_location),\r\n                description=f'Navigate to {start_location}'\r\n            ),\r\n            TaskStep(\r\n                action='grasp',\r\n                parameters={'object_name': object_name},\r\n                precondition=lambda: self._check_object_available(object_name, start_location),\r\n                postcondition=lambda: self.robot_state['holding_object'] == object_name,\r\n                description=f'Grasp {object_name}'\r\n            ),\r\n            TaskStep(\r\n                action='navigate',\r\n                parameters={'target_location': end_location},\r\n                precondition=lambda: self.robot_state['holding_object'] == object_name,\r\n                postcondition=lambda: self._check_navigation_success(end_location),\r\n                description=f'Navigate to {end_location} with {object_name}'\r\n            ),\r\n            TaskStep(\r\n                action='place',\r\n                parameters={'object_name': object_name},\r\n                precondition=lambda: self.robot_state['holding_object'] == object_name,\r\n                postcondition=lambda: self.robot_state['holding_object'] is None,\r\n                description=f'Place {object_name} at {end_location}'\r\n            )\r\n        ]\r\n\r\n        self.tasks[task_id] = {\r\n            'steps': steps,\r\n            'current_step': 0,\r\n            'status': TaskStatus.PENDING,\r\n            'start_time': time.time()\r\n        }\r\n\r\n        return task_id\r\n\r\n    def create_cleaning_task(self, room_name: str) -> int:\r\n        \"\"\"Create a cleaning task plan\"\"\"\r\n        task_id = self.current_task_id\r\n        self.current_task_id += 1\r\n\r\n        # Define cleaning steps\r\n        steps = [\r\n            TaskStep(\r\n                action='navigate',\r\n                parameters={'target_location': f'{room_name}_entrance'},\r\n                precondition=lambda: self.robot_state['battery_level'] > 20,\r\n                postcondition=lambda: self._check_navigation_success(f'{room_name}_entrance'),\r\n                description=f'Navigate to {room_name} entrance'\r\n            ),\r\n            TaskStep(\r\n                action='clean_area',\r\n                parameters={'room_name': room_name},\r\n                precondition=lambda: True,\r\n                postcondition=lambda: self._check_cleaning_success(room_name),\r\n                description=f'Clean {room_name}'\r\n            ),\r\n            TaskStep(\r\n                action='return_to_base',\r\n                parameters={},\r\n                precondition=lambda: True,\r\n                postcondition=lambda: self._check_navigation_success('charging_station'),\r\n                description='Return to charging station'\r\n            )\r\n        ]\r\n\r\n        self.tasks[task_id] = {\r\n            'steps': steps,\r\n            'current_step': 0,\r\n            'status': TaskStatus.PENDING,\r\n            'start_time': time.time()\r\n        }\r\n\r\n        return task_id\r\n\r\n    def create_fetch_task(self, object_name: str, destination: str) -> int:\r\n        \"\"\"Create a fetch task plan\"\"\"\r\n        task_id = self.current_task_id\r\n        self.current_task_id += 1\r\n\r\n        # Define fetch steps\r\n        steps = [\r\n            TaskStep(\r\n                action='find_object',\r\n                parameters={'object_name': object_name},\r\n                precondition=lambda: self.robot_state['battery_level'] > 30,\r\n                postcondition=lambda: self._check_object_found(object_name),\r\n                description=f'Find {object_name}'\r\n            ),\r\n            TaskStep(\r\n                action='grasp',\r\n                parameters={'object_name': object_name},\r\n                precondition=lambda: self._check_object_available(object_name, 'current'),\r\n                postcondition=lambda: self.robot_state['holding_object'] == object_name,\r\n                description=f'Grasp {object_name}'\r\n            ),\r\n            TaskStep(\r\n                action='navigate',\r\n                parameters={'target_location': destination},\r\n                precondition=lambda: self.robot_state['holding_object'] == object_name,\r\n                postcondition=lambda: self._check_navigation_success(destination),\r\n                description=f'Navigate to {destination} with {object_name}'\r\n            ),\r\n            TaskStep(\r\n                action='place',\r\n                parameters={'object_name': object_name},\r\n                precondition=lambda: self.robot_state['holding_object'] == object_name,\r\n                postcondition=lambda: self.robot_state['holding_object'] is None,\r\n                description=f'Place {object_name} at {destination}'\r\n            )\r\n        ]\r\n\r\n        self.tasks[task_id] = {\r\n            'steps': steps,\r\n            'current_step': 0,\r\n            'status': TaskStatus.PENDING,\r\n            'start_time': time.time()\r\n        }\r\n\r\n        return task_id\r\n\r\n    def execute_task_step(self, task_id: int) -> bool:\r\n        \"\"\"Execute the current step of a task\"\"\"\r\n        if task_id not in self.tasks:\r\n            return False\r\n\r\n        task = self.tasks[task_id]\r\n        if task['current_step'] >= len(task['steps']):\r\n            task['status'] = TaskStatus.SUCCESS\r\n            return True\r\n\r\n        step = task['steps'][task['current_step']]\r\n\r\n        # Check precondition\r\n        if not step.precondition():\r\n            task['status'] = TaskStatus.FAILED\r\n            return False\r\n\r\n        # Execute action (simplified - in real implementation, this would call ROS2 actions/services)\r\n        success = self._execute_action(step.action, step.parameters)\r\n\r\n        if success:\r\n            # Check postcondition\r\n            if step.postcondition():\r\n                task['current_step'] += 1\r\n                if task['current_step'] >= len(task['steps']):\r\n                    task['status'] = TaskStatus.SUCCESS\r\n                else:\r\n                    task['status'] = TaskStatus.RUNNING\r\n                return True\r\n            else:\r\n                task['status'] = TaskStatus.FAILED\r\n                return False\r\n        else:\r\n            task['status'] = TaskStatus.FAILED\r\n            return False\r\n\r\n    def _execute_action(self, action: str, parameters: Dict[str, Any]) -> bool:\r\n        \"\"\"Execute a specific action (simplified implementation)\"\"\"\r\n        print(f\"Executing action: {action} with parameters: {parameters}\")\r\n\r\n        # Simulate action execution\r\n        time.sleep(0.5)  # Simulate execution time\r\n\r\n        # Update robot state based on action\r\n        if action == 'grasp':\r\n            obj_name = parameters.get('object_name')\r\n            if obj_name:\r\n                self.robot_state['gripper_state'] = 'closed'\r\n                self.robot_state['holding_object'] = obj_name\r\n        elif action == 'place':\r\n            self.robot_state['gripper_state'] = 'open'\r\n            self.robot_state['holding_object'] = None\r\n        elif action == 'navigate':\r\n            target = parameters.get('target_location')\r\n            if target:\r\n                # Update position (simplified)\r\n                self.robot_state['position'] = self._get_location_coordinates(target)\r\n\r\n        return True  # Simulate success\r\n\r\n    def _get_location_coordinates(self, location_name: str) -> tuple:\r\n        \"\"\"Get coordinates for a named location (simplified)\"\"\"\r\n        locations = {\r\n            'kitchen': (2.0, 0.0, 0.0),\r\n            'living_room': (-2.0, 0.0, 0.0),\r\n            'bedroom': (0.0, 2.0, 0.0),\r\n            'office': (0.0, -2.0, 0.0),\r\n            'charging_station': (0.0, 0.0, 0.0)\r\n        }\r\n        return locations.get(location_name, (0.0, 0.0, 0.0))\r\n\r\n    def _check_navigation_success(self, location: str) -> bool:\r\n        \"\"\"Check if navigation was successful (simplified)\"\"\"\r\n        target_pos = self._get_location_coordinates(location)\r\n        current_pos = self.robot_state['position']\r\n        distance = sum((a - b) ** 2 for a, b in zip(current_pos, target_pos)) ** 0.5\r\n        return distance < 0.5  # Within 0.5m\r\n\r\n    def _check_object_available(self, obj_name: str, location: str) -> bool:\r\n        \"\"\"Check if object is available at location (simplified)\"\"\"\r\n        return True  # Simplified check\r\n\r\n    def _check_object_found(self, obj_name: str) -> bool:\r\n        \"\"\"Check if object was found (simplified)\"\"\"\r\n        return True  # Simplified check\r\n\r\n    def _check_cleaning_success(self, room_name: str) -> bool:\r\n        \"\"\"Check if cleaning was successful (simplified)\"\"\"\r\n        return True  # Simplified check\r\n\r\nclass LanguageTaskPlannerNode(Node):\r\n    \"\"\"ROS2 node for language-guided task planning\"\"\"\r\n    def __init__(self):\r\n        super().__init__('language_task_planner')\r\n\r\n        # Publishers and subscribers\r\n        self.command_sub = self.create_subscription(\r\n            String, '/language_task_command', self.command_callback, 10)\r\n        self.task_status_pub = self.create_publisher(String, '/task_status', 10)\r\n        self.action_pub = self.create_publisher(String, '/planned_action', 10)\r\n\r\n        # Initialize planner\r\n        self.planner = TaskPlanner()\r\n\r\n        # Timer for task execution\r\n        self.task_timer = self.create_timer(0.1, self.execute_current_tasks)\r\n\r\n        self.get_logger().info('Language Task Planner Node initialized')\r\n\r\n    def command_callback(self, msg):\r\n        \"\"\"Process language command to create tasks\"\"\"\r\n        try:\r\n            # Parse command (assuming JSON format)\r\n            command_data = json.loads(msg.data)\r\n            command_type = command_data.get('type')\r\n            parameters = command_data.get('parameters', {})\r\n\r\n            task_id = None\r\n\r\n            if command_type == 'delivery':\r\n                task_id = self.planner.create_delivery_task(\r\n                    parameters.get('start_location', 'kitchen'),\r\n                    parameters.get('end_location', 'living_room'),\r\n                    parameters.get('object_name', 'cup')\r\n                )\r\n                self.get_logger().info(f'Created delivery task: {task_id}')\r\n            elif command_type == 'cleaning':\r\n                task_id = self.planner.create_cleaning_task(\r\n                    parameters.get('room_name', 'living_room')\r\n                )\r\n                self.get_logger().info(f'Created cleaning task: {task_id}')\r\n            elif command_type == 'fetch':\r\n                task_id = self.planner.create_fetch_task(\r\n                    parameters.get('object_name', 'book'),\r\n                    parameters.get('destination', 'table')\r\n                )\r\n                self.get_logger().info(f'Created fetch task: {task_id}')\r\n            else:\r\n                self.get_logger().error(f'Unknown command type: {command_type}')\r\n                return\r\n\r\n            if task_id is not None:\r\n                # Publish task creation\r\n                status_msg = String()\r\n                status_msg.data = json.dumps({\r\n                    'task_id': task_id,\r\n                    'status': 'created',\r\n                    'command': command_data\r\n                })\r\n                self.task_status_pub.publish(status_msg)\r\n\r\n        except json.JSONDecodeError:\r\n            self.get_logger().error(f'Invalid JSON command: {msg.data}')\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing command: {e}')\r\n\r\n    def execute_current_tasks(self):\r\n        \"\"\"Execute current tasks\"\"\"\r\n        for task_id, task_info in self.planner.tasks.items():\r\n            if task_info['status'] in [TaskStatus.PENDING, TaskStatus.RUNNING]:\r\n                success = self.planner.execute_task_step(task_id)\r\n\r\n                # Publish task status\r\n                status_msg = String()\r\n                status_msg.data = json.dumps({\r\n                    'task_id': task_id,\r\n                    'status': task_info['status'].value,\r\n                    'current_step': task_info['current_step'],\r\n                    'total_steps': len(task_info['steps'])\r\n                })\r\n                self.task_status_pub.publish(status_msg)\r\n\r\n                if task_info['status'] == TaskStatus.SUCCESS:\r\n                    self.get_logger().info(f'Task {task_id} completed successfully')\r\n                elif task_info['status'] == TaskStatus.FAILED:\r\n                    self.get_logger().error(f'Task {task_id} failed')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = LanguageTaskPlannerNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('Shutting down language task planner node...')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,a.jsx)(e.h3,{id:"human-robot-command-interface",children:"Human-Robot Command Interface"}),"\n",(0,a.jsx)(e.p,{children:"Students implement a complete human-robot command interface that can interpret natural language and execute robotic actions."}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Objectives:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Implement natural language processing pipeline"}),"\n",(0,a.jsx)(e.li,{children:"Create context-aware command interpretation"}),"\n",(0,a.jsx)(e.li,{children:"Develop action execution framework"}),"\n",(0,a.jsx)(e.li,{children:"Test with human users in realistic scenarios"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Required Components:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Natural language processing tools"}),"\n",(0,a.jsx)(e.li,{children:"Robot control interface"}),"\n",(0,a.jsx)(e.li,{children:"Context management system"}),"\n",(0,a.jsx)(e.li,{children:"User interface for command input"}),"\n",(0,a.jsx)(e.li,{children:"Robot platform with mobility and manipulation"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Evaluation Criteria:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Command interpretation accuracy"}),"\n",(0,a.jsx)(e.li,{children:"Action execution success rate"}),"\n",(0,a.jsx)(e.li,{children:"User satisfaction with interface"}),"\n",(0,a.jsx)(e.li,{children:"System robustness to varied commands"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"context-aware-navigation-system",children:"Context-Aware Navigation System"}),"\n",(0,a.jsx)(e.p,{children:"Students develop a navigation system that interprets location references in natural language commands."}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Objectives:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Implement spatial language understanding"}),"\n",(0,a.jsx)(e.li,{children:"Create map-based location resolution"}),"\n",(0,a.jsx)(e.li,{children:"Develop navigation planning from language"}),"\n",(0,a.jsx)(e.li,{children:"Validate performance in real environment"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Required Components:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Robot with navigation capabilities"}),"\n",(0,a.jsx)(e.li,{children:"Environmental mapping system"}),"\n",(0,a.jsx)(e.li,{children:"Natural language processing tools"}),"\n",(0,a.jsx)(e.li,{children:"Location recognition algorithms"}),"\n",(0,a.jsx)(e.li,{children:"Path planning system"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Evaluation Criteria:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Location reference resolution accuracy"}),"\n",(0,a.jsx)(e.li,{children:"Navigation success rate"}),"\n",(0,a.jsx)(e.li,{children:"Path efficiency"}),"\n",(0,a.jsx)(e.li,{children:"Adaptation to environment changes"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"task-planning-from-language",children:"Task Planning from Language"}),"\n",(0,a.jsx)(e.p,{children:"Students create a system that can decompose complex language commands into executable task sequences."}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Objectives:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Implement task decomposition algorithms"}),"\n",(0,a.jsx)(e.li,{children:"Create action planning framework"}),"\n",(0,a.jsx)(e.li,{children:"Develop precondition/postcondition checking"}),"\n",(0,a.jsx)(e.li,{children:"Validate task completion success"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Required Components:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Task planning algorithms"}),"\n",(0,a.jsx)(e.li,{children:"Action execution framework"}),"\n",(0,a.jsx)(e.li,{children:"State monitoring system"}),"\n",(0,a.jsx)(e.li,{children:"Language parsing tools"}),"\n",(0,a.jsx)(e.li,{children:"Robot with multiple capabilities"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Evaluation Criteria:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Task decomposition accuracy"}),"\n",(0,a.jsx)(e.li,{children:"Plan execution success rate"}),"\n",(0,a.jsx)(e.li,{children:"Handling of task failures"}),"\n",(0,a.jsx)(e.li,{children:"Efficiency of task completion"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"Chapter 20 covered language-action integration, focusing on connecting natural language commands to robotic actions. Students learned about natural language processing for robotics, context-aware command interpretation, and task planning guided by language. The chapter emphasized creating robust systems that can handle ambiguous instructions and execute complex tasks described in natural language, enabling more intuitive human-robot interaction."}),"\n",(0,a.jsx)(e.h2,{id:"quiz",children:"Quiz"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:"What is the main purpose of language-action integration in robotics?"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"A: To eliminate the need for programming"}),"\n",(0,a.jsx)(e.li,{children:"B: To enable robots to understand and execute natural language commands"}),"\n",(0,a.jsx)(e.li,{children:"C: To make robots speak human languages"}),"\n",(0,a.jsx)(e.li,{children:"D: To reduce robot hardware requirements"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Answer: B"})," - Language-action integration enables robots to understand natural language commands and execute corresponding physical actions."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:"What does semantic parsing do in language-action systems?"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"A: Converts language to structured action representations"}),"\n",(0,a.jsx)(e.li,{children:"B: Makes the robot speak more clearly"}),"\n",(0,a.jsx)(e.li,{children:"C: Improves robot vision"}),"\n",(0,a.jsx)(e.li,{children:"D: Reduces power consumption"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Answer: A"})," - Semantic parsing converts natural language commands into structured action representations that robots can execute."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:"Why is context awareness important in language-action systems?"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"A: It makes robots faster"}),"\n",(0,a.jsx)(e.li,{children:"B: It helps resolve ambiguous references based on environment"}),"\n",(0,a.jsx)(e.li,{children:"C: It reduces hardware costs"}),"\n",(0,a.jsx)(e.li,{children:"D: It eliminates the need for sensors"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Answer: B"})," - Context awareness helps resolve ambiguous language references based on the current environmental context."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:"What is ambiguity resolution in language-action systems?"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"A: Making commands more confusing"}),"\n",(0,a.jsx)(e.li,{children:"B: Handling ambiguous or incomplete instructions using context"}),"\n",(0,a.jsx)(e.li,{children:"C: Adding more sensors to robots"}),"\n",(0,a.jsx)(e.li,{children:"D: Making robots speak multiple languages"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Answer: B"})," - Ambiguity resolution handles ambiguous or incomplete instructions by using environmental context and knowledge."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:"What does multimodal integration mean in language-action systems?"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"A: Using multiple programming languages"}),"\n",(0,a.jsx)(e.li,{children:"B: Combining language with visual and spatial information"}),"\n",(0,a.jsx)(e.li,{children:"C: Using multiple robots at once"}),"\n",(0,a.jsx)(e.li,{children:"D: Making robots move in multiple ways"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Answer: B"})," - Multimodal integration combines language understanding with visual and spatial information for better command interpretation."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,a.jsx)(e.p,{children:"After completing this chapter, students will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Implement multimodal learning systems"}),"\n",(0,a.jsx)(e.li,{children:"Integrate vision, language, and action components"}),"\n",(0,a.jsx)(e.li,{children:"Develop interactive learning algorithms"}),"\n",(0,a.jsx)(e.li,{children:"Create human-robot interaction systems"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Basic understanding of Python programming"}),"\n",(0,a.jsx)(e.li,{children:"Fundamentals of linear algebra and calculus"}),"\n",(0,a.jsx)(e.li,{children:"Basic knowledge of robotics concepts"}),"\n",(0,a.jsx)(e.li,{children:"Introduction to machine learning concepts"}),"\n",(0,a.jsx)(e.li,{children:"Completion of Module 0 (Introduction and Foundations)"}),"\n",(0,a.jsx)(e.li,{children:"Completion of Chapter 01 (Physical AI Basics)"}),"\n",(0,a.jsx)(e.li,{children:"Completion of Chapter 03 (ROS2 Nodes, Topics & Services)"}),"\n",(0,a.jsx)(e.li,{children:"Completion of Chapter 16 (Vision-Language-Action Concepts)"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"estimated-duration",children:"Estimated Duration"}),"\n",(0,a.jsx)(e.p,{children:"5 hours"})]})}function m(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}}}]);