"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[604],{176:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"06-Chapter-2-Practical-ROS2-Examples","title":"Chapter 2: Practical ROS2 Examples","description":"Overview","source":"@site/docusaurus/docs/06-Chapter-2-Practical-ROS2-Examples.md","sourceDirName":".","slug":"/06-Chapter-2-Practical-ROS2-Examples","permalink":"/physical-ai-book/ur/docs/06-Chapter-2-Practical-ROS2-Examples","draft":false,"unlisted":false,"editUrl":"https://github.com/ayesha-muttalib123/physical-ai-book/tree/main/docusaurus/docs/06-Chapter-2-Practical-ROS2-Examples.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"id":"06-Chapter-2-Practical-ROS2-Examples","title":"Chapter 2: Practical ROS2 Examples","sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: ROS2 Communication Patterns","permalink":"/physical-ai-book/ur/docs/05-Chapter-1-ROS2-Communication-Patterns"},"next":{"title":"Chapter 3: Introduction To Digital Twins","permalink":"/physical-ai-book/ur/docs/07-Chapter-3-Introduction-To-Digital-Twins"}}');var i=r(4848),t=r(8453);const o={id:"06-Chapter-2-Practical-ROS2-Examples",title:"Chapter 2: Practical ROS2 Examples",sidebar_position:6},a="Chapter 2: Practical ROS2 Examples",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Why It Matters",id:"why-it-matters",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Complete ROS2 System Architecture",id:"complete-ros2-system-architecture",level:3},{value:"Robot Bringup Procedures",id:"robot-bringup-procedures",level:3},{value:"Sensor Integration",id:"sensor-integration",level:3},{value:"Robot Control Systems",id:"robot-control-systems",level:3},{value:"Navigation Stacks",id:"navigation-stacks",level:3},{value:"System Debugging",id:"system-debugging",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Complete Mobile Robot Controller",id:"complete-mobile-robot-controller",level:3},{value:"Multi-Sensor Fusion Node",id:"multi-sensor-fusion-node",level:3},{value:"ROS2 Navigation Stack Integration",id:"ros2-navigation-stack-integration",level:3},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Autonomous Patrol Robot",id:"autonomous-patrol-robot",level:3},{value:"Object Detection and Grasping System",id:"object-detection-and-grasping-system",level:3},{value:"Multi-Robot Coordination System",id:"multi-robot-coordination-system",level:3},{value:"Summary",id:"summary",level:2},{value:"Quiz",id:"quiz",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Estimated Duration",id:"estimated-duration",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-2-practical-ros2-examples",children:"Chapter 2: Practical ROS2 Examples"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"This chapter provides practical, real-world examples of ROS2 implementations in robotics applications. Students will work through complete examples that demonstrate how to combine all ROS2 communication patterns to build functional robotic systems. The chapter covers robot control, sensor integration, navigation, and system integration with hands-on exercises that reinforce the theoretical concepts learned in previous chapters."}),"\n",(0,i.jsx)(n.h2,{id:"why-it-matters",children:"Why It Matters"}),"\n",(0,i.jsx)(n.p,{children:"Practical examples are essential for understanding how ROS2 concepts apply to real robotic systems. This chapter bridges the gap between theory and implementation, showing students how to combine nodes, topics, services, actions, and parameters to solve actual robotics problems. These examples provide templates and patterns that students can adapt for their own robotic applications."}),"\n",(0,i.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,i.jsx)(n.h3,{id:"complete-ros2-system-architecture",children:"Complete ROS2 System Architecture"}),"\n",(0,i.jsx)(n.p,{children:"How all components work together in a real system. Understanding the integration of nodes, topics, services, actions, and parameters in a complete robotic application."}),"\n",(0,i.jsx)(n.h3,{id:"robot-bringup-procedures",children:"Robot Bringup Procedures"}),"\n",(0,i.jsx)(n.p,{children:"Standard processes for starting robot systems. These procedures ensure all required nodes start in the correct order with proper configuration and parameter settings."}),"\n",(0,i.jsx)(n.h3,{id:"sensor-integration",children:"Sensor Integration"}),"\n",(0,i.jsx)(n.p,{children:"Combining multiple sensors using appropriate ROS2 patterns. This involves selecting the right communication patterns for different sensor types and synchronizing data from multiple sources."}),"\n",(0,i.jsx)(n.h3,{id:"robot-control-systems",children:"Robot Control Systems"}),"\n",(0,i.jsx)(n.p,{children:"Implementing feedback control with ROS2. This includes creating control loops that process sensor data and generate appropriate commands to achieve desired robot behavior."}),"\n",(0,i.jsx)(n.h3,{id:"navigation-stacks",children:"Navigation Stacks"}),"\n",(0,i.jsx)(n.p,{children:"Understanding and implementing ROS2 navigation. The navigation stack provides modular components for localization, mapping, path planning, and motion control."}),"\n",(0,i.jsx)(n.h3,{id:"system-debugging",children:"System Debugging"}),"\n",(0,i.jsx)(n.p,{children:"Techniques for troubleshooting ROS2 systems. This includes using ROS2 tools for introspection, logging, and diagnosing communication issues."}),"\n",(0,i.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(n.p,{children:"Best practices for efficient ROS2 implementations. This involves optimizing communication patterns, reducing latency, and managing resources effectively."}),"\n",(0,i.jsx)(n.h3,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,i.jsx)(n.p,{children:"Implementing safety in ROS2 robot systems. This includes creating safety checks, emergency stops, and fail-safe behaviors to ensure safe robot operation."}),"\n",(0,i.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,i.jsx)(n.h3,{id:"complete-mobile-robot-controller",children:"Complete Mobile Robot Controller"}),"\n",(0,i.jsx)(n.p,{children:"Integrated ROS2 node that controls a differential drive robot with sensor feedback:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy\r\nfrom geometry_msgs.msg import Twist, PoseStamped\r\nfrom sensor_msgs.msg import LaserScan\r\nfrom nav_msgs.msg import Odometry\r\nimport math\r\nimport numpy as np\r\n\r\nclass MobileRobotController(Node):\r\n    def __init__(self):\r\n        super().__init__('mobile_robot_controller')\r\n\r\n        # QoS profile for sensor data\r\n        sensor_qos = QoSProfile(\r\n            depth=10,\r\n            reliability=ReliabilityPolicy.BEST_EFFORT,\r\n            history=HistoryPolicy.KEEP_LAST\r\n        )\r\n\r\n        # Publishers\r\n        self.cmd_vel_publisher = self.create_publisher(Twist, 'cmd_vel', 10)\r\n        self.goal_publisher = self.create_publisher(PoseStamped, 'goal_pose', 10)\r\n\r\n        # Subscriptions\r\n        self.odom_subscription = self.create_subscription(\r\n            Odometry, 'odom', self.odom_callback, 10)\r\n        self.scan_subscription = self.create_subscription(\r\n            LaserScan, 'scan', self.scan_callback, sensor_qos)\r\n\r\n        # Parameters\r\n        self.declare_parameter('linear_speed', 0.5)\r\n        self.declare_parameter('angular_speed', 0.5)\r\n        self.declare_parameter('safety_distance', 0.5)\r\n        self.declare_parameter('control_frequency', 10)\r\n\r\n        # Robot state\r\n        self.current_pose = None\r\n        self.current_twist = None\r\n        self.scan_data = None\r\n        self.safety_enabled = True\r\n\r\n        # Control timer\r\n        control_freq = self.get_parameter('control_frequency').value\r\n        self.control_timer = self.create_timer(1.0/control_freq, self.control_loop)\r\n\r\n        self.get_logger().info('Mobile robot controller initialized')\r\n\r\n    def odom_callback(self, msg):\r\n        # Extract position and orientation from odometry\r\n        self.current_pose = msg.pose.pose\r\n        self.current_twist = msg.twist.twist\r\n\r\n    def scan_callback(self, msg):\r\n        # Store latest scan data\r\n        self.scan_data = msg\r\n\r\n    def control_loop(self):\r\n        if self.scan_data is None or self.current_pose is None:\r\n            return\r\n\r\n        # Get parameters\r\n        linear_speed = self.get_parameter('linear_speed').value\r\n        angular_speed = self.get_parameter('angular_speed').value\r\n        safety_dist = self.get_parameter('safety_distance').value\r\n\r\n        # Check for obstacles\r\n        if self.safety_enabled and self.has_obstacle_ahead(safety_dist):\r\n            # Stop the robot if obstacle is detected\r\n            cmd_vel = Twist()\r\n            cmd_vel.linear.x = 0.0\r\n            cmd_vel.angular.z = 0.0\r\n            self.cmd_vel_publisher.publish(cmd_vel)\r\n            self.get_logger().warn('Obstacle detected! Stopping robot.')\r\n            return\r\n\r\n        # Example: Simple wall following behavior\r\n        cmd_vel = Twist()\r\n        cmd_vel.linear.x = linear_speed * 0.7  # Move forward at 70% speed\r\n        cmd_vel.angular.z = self.calculate_wall_following_turn()\r\n\r\n        self.cmd_vel_publisher.publish(cmd_vel)\r\n\r\n    def has_obstacle_ahead(self, threshold):\r\n        if self.scan_data is None:\r\n            return False\r\n\r\n        # Check the front 30 degrees for obstacles\r\n        front_indices = range(\r\n            len(self.scan_data.ranges) // 2 - 15,\r\n            len(self.scan_data.ranges) // 2 + 15\r\n        )\r\n\r\n        for i in front_indices:\r\n            if 0 < self.scan_data.ranges[i] < threshold:\r\n                return True\r\n        return False\r\n\r\n    def calculate_wall_following_turn(self):\r\n        if self.scan_data is None:\r\n            return 0.0\r\n\r\n        # Simple wall following: turn toward open space\r\n        left_avg = np.mean(self.scan_data.ranges[:len(self.scan_data.ranges)//3])\r\n        right_avg = np.mean(self.scan_data.ranges[2*len(self.scan_data.ranges)//3:])\r\n\r\n        # Turn toward the side with more open space\r\n        if left_avg > right_avg:\r\n            return 0.3  # Turn right\r\n        else:\r\n            return -0.3  # Turn left\r\n\r\n    def set_goal(self, x, y):\r\n        goal_msg = PoseStamped()\r\n        goal_msg.header.stamp = self.get_clock().now().to_msg()\r\n        goal_msg.header.frame_id = 'map'\r\n        goal_msg.pose.position.x = x\r\n        goal_msg.pose.position.y = y\r\n        goal_msg.pose.orientation.w = 1.0\r\n\r\n        self.goal_publisher.publish(goal_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    controller = MobileRobotController()\r\n\r\n    # Example: Set a goal after 5 seconds\r\n    def set_example_goal():\r\n        controller.set_goal(2.0, 1.0)\r\n\r\n    goal_timer = controller.create_timer(5.0, set_example_goal)\r\n\r\n    try:\r\n        rclpy.spin(controller)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        controller.destroy_node()\r\n        rclpy.shutdown()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"multi-sensor-fusion-node",children:"Multi-Sensor Fusion Node"}),"\n",(0,i.jsx)(n.p,{children:"Node that combines data from multiple sensors for enhanced perception:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan, Image, Imu\r\nfrom geometry_msgs.msg import PoseWithCovarianceStamped\r\nfrom std_msgs.msg import String\r\nimport numpy as np\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\n\r\nclass SensorFusionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('sensor_fusion_node')\r\n\r\n        # Initialize CV bridge\r\n        self.cv_bridge = CvBridge()\r\n\r\n        # Latest sensor data\r\n        self.laser_data = None\r\n        self.image_data = None\r\n        self.imu_data = None\r\n\r\n        # QoS profiles\r\n        sensor_qos = rclpy.qos.QoSProfile(depth=10, reliability=rclpy.qos.ReliabilityPolicy.BEST_EFFORT)\r\n\r\n        # Subscriptions\r\n        self.laser_subscription = self.create_subscription(\r\n            LaserScan, 'scan', self.laser_callback, 10)\r\n        self.image_subscription = self.create_subscription(\r\n            Image, 'camera/image_raw', self.image_callback, sensor_qos)\r\n        self.imu_subscription = self.create_subscription(\r\n            Imu, 'imu/data', self.imu_callback, 10)\r\n\r\n        # Publishers\r\n        self.fused_data_publisher = self.create_publisher(\r\n            PoseWithCovarianceStamped, 'fused_pose', 10)\r\n        self.status_publisher = self.create_publisher(String, 'fusion_status', 10)\r\n\r\n        # Timer for fusion\r\n        self.fusion_timer = self.create_timer(0.1, self.perform_fusion)\r\n\r\n        self.get_logger().info('Sensor fusion node initialized')\r\n\r\n    def laser_callback(self, msg):\r\n        self.laser_data = msg\r\n        self.get_logger().debug('Received laser data')\r\n\r\n    def image_callback(self, msg):\r\n        try:\r\n            # Convert ROS Image to OpenCV\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\r\n            self.image_data = cv_image\r\n            self.get_logger().debug('Received image data')\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error converting image: {e}')\r\n\r\n    def imu_callback(self, msg):\r\n        self.imu_data = msg\r\n        self.get_logger().debug('Received IMU data')\r\n\r\n    def perform_fusion(self):\r\n        if not all([self.laser_data, self.image_data, self.imu_data]):\r\n            status_msg = String()\r\n            status_msg.data = 'Waiting for complete sensor data'\r\n            self.status_publisher.publish(status_msg)\r\n            return\r\n\r\n        # Perform sensor fusion (simplified example)\r\n        try:\r\n            # Extract orientation from IMU\r\n            orientation = self.extract_orientation_from_imu(self.imu_data)\r\n\r\n            # Process laser data for position estimation\r\n            position = self.estimate_position_from_laser(self.laser_data)\r\n\r\n            # Combine data into fused estimate\r\n            fused_pose = self.create_fused_estimate(position, orientation)\r\n\r\n            # Publish fused data\r\n            self.fused_data_publisher.publish(fused_pose)\r\n\r\n            # Publish status\r\n            status_msg = String()\r\n            status_msg.data = f'Fusion completed: pos=({position[0]:.2f},{position[1]:.2f}), orient={orientation[2]:.2f}'\r\n            self.status_publisher.publish(status_msg)\r\n\r\n            self.get_logger().info(status_msg.data)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Fusion error: {e}')\r\n\r\n    def extract_orientation_from_imu(self, imu_msg):\r\n        # Extract orientation from IMU quaternion\r\n        import math\r\n        x, y, z, w = (imu_msg.orientation.x, imu_msg.orientation.y,\r\n                      imu_msg.orientation.z, imu_msg.orientation.w)\r\n\r\n        # Convert quaternion to Euler angles (roll, pitch, yaw)\r\n        sinr_cosp = 2 * (w * x + y * z)\r\n        cosr_cosp = 1 - 2 * (x * x + y * y)\r\n        roll = math.atan2(sinr_cosp, cosr_cosp)\r\n\r\n        sinp = 2 * (w * y - z * x)\r\n        pitch = math.asin(sinp)\r\n\r\n        siny_cosp = 2 * (w * z + x * y)\r\n        cosy_cosp = 1 - 2 * (y * y + z * z)\r\n        yaw = math.atan2(siny_cosp, cosy_cosp)\r\n\r\n        return (roll, pitch, yaw)\r\n\r\n    def estimate_position_from_laser(self, laser_msg):\r\n        # Simple position estimation from laser scan (could be more sophisticated)\r\n        # For this example, we'll just use a basic approach\r\n        ranges = [r for r in laser_msg.ranges if not (r == float('inf') or r == float('nan'))]\r\n\r\n        if ranges:\r\n            # Use the median range as a simple position estimate\r\n            median_range = np.median(ranges)\r\n            # This is a simplified approach - in reality, you'd use more sophisticated methods\r\n            return (median_range * 0.7, 0.0)  # x, y position estimate\r\n        else:\r\n            return (0.0, 0.0)\r\n\r\n    def create_fused_estimate(self, position, orientation):\r\n        pose_msg = PoseWithCovarianceStamped()\r\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\r\n        pose_msg.header.frame_id = 'map'\r\n\r\n        # Set position\r\n        pose_msg.pose.pose.position.x = position[0]\r\n        pose_msg.pose.pose.position.y = position[1]\r\n        pose_msg.pose.pose.position.z = 0.0\r\n\r\n        # Set orientation (simplified - convert Euler to quaternion)\r\n        yaw = orientation[2]\r\n        pose_msg.pose.pose.orientation.z = math.sin(yaw / 2.0)\r\n        pose_msg.pose.pose.orientation.w = math.cos(yaw / 2.0)\r\n\r\n        # Set covariance (simplified)\r\n        pose_msg.pose.covariance = [0.1, 0.0, 0.0, 0.0, 0.0, 0.0,\r\n                                   0.0, 0.1, 0.0, 0.0, 0.0, 0.0,\r\n                                   0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\r\n                                   0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\r\n                                   0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\r\n                                   0.0, 0.0, 0.0, 0.0, 0.0, 0.1]\r\n\r\n        return pose_msg\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    fusion_node = SensorFusionNode()\r\n\r\n    try:\r\n        rclpy.spin(fusion_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        fusion_node.destroy_node()\r\n        rclpy.shutdown()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"ros2-navigation-stack-integration",children:"ROS2 Navigation Stack Integration"}),"\n",(0,i.jsx)(n.p,{children:"Example of integrating with ROS2 navigation stack components:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.action import ActionClient\r\nfrom rclpy.qos import QoSProfile\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom nav_msgs.msg import Path\r\nfrom std_msgs.msg import String\r\nfrom rclpy.duration import Duration\r\n\r\n# Import navigation action (would use actual nav2_msgs in real implementation)\r\n# from nav2_msgs.action import NavigateToPose\r\n\r\nclass NavigationIntegrationNode(Node):\r\n    def __init__(self):\r\n        super().__init__('navigation_integration')\r\n\r\n        # Publishers\r\n        self.goal_publisher = self.create_publisher(PoseStamped, 'goal_pose', 10)\r\n        self.path_subscriber = self.create_subscription(\r\n            Path, 'plan', self.path_callback, 10)\r\n        self.status_publisher = self.create_publisher(String, 'nav_status', 10)\r\n\r\n        # Action client for navigation (using placeholder since nav2_msgs may not be available)\r\n        # self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\r\n\r\n        # Timer to send navigation goals\r\n        self.nav_timer = self.create_timer(10.0, self.send_navigation_goal)\r\n\r\n        self.get_logger().info('Navigation integration node initialized')\r\n\r\n    def path_callback(self, msg):\r\n        # Handle received navigation path\r\n        self.get_logger().info(f'Received path with {len(msg.poses)} waypoints')\r\n\r\n    def send_navigation_goal(self):\r\n        # Create and send a navigation goal\r\n        goal_msg = PoseStamped()\r\n        goal_msg.header.stamp = self.get_clock().now().to_msg()\r\n        goal_msg.header.frame_id = 'map'\r\n\r\n        # Set goal position (example: move 2m forward)\r\n        goal_msg.pose.position.x = 2.0\r\n        goal_msg.pose.position.y = 1.0\r\n        goal_msg.pose.position.z = 0.0\r\n\r\n        # Set goal orientation (facing forward)\r\n        goal_msg.pose.orientation.w = 1.0\r\n\r\n        self.goal_publisher.publish(goal_msg)\r\n        self.get_logger().info(f'Sent navigation goal to ({goal_msg.pose.position.x}, {goal_msg.pose.position.y})')\r\n\r\n        # Publish status\r\n        status_msg = String()\r\n        status_msg.data = f'Navigation goal sent to ({goal_msg.pose.position.x}, {goal_msg.pose.position.y})'\r\n        self.status_publisher.publish(status_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    nav_node = NavigationIntegrationNode()\r\n\r\n    try:\r\n        rclpy.spin(nav_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        nav_node.destroy_node()\r\n        rclpy.shutdown()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,i.jsx)(n.h3,{id:"autonomous-patrol-robot",children:"Autonomous Patrol Robot"}),"\n",(0,i.jsx)(n.p,{children:"Students implement a complete autonomous patrol robot that navigates between waypoints, avoids obstacles, and reports status."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Objectives:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate sensor processing with navigation"}),"\n",(0,i.jsx)(n.li,{children:"Implement obstacle avoidance behavior"}),"\n",(0,i.jsx)(n.li,{children:"Create patrol path following"}),"\n",(0,i.jsx)(n.li,{children:"Implement status reporting"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Required Components:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Laser scanner for obstacle detection"}),"\n",(0,i.jsx)(n.li,{children:"Odometry for localization"}),"\n",(0,i.jsx)(n.li,{children:"Velocity command interface"}),"\n",(0,i.jsx)(n.li,{children:"Navigation system"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Evaluation Criteria:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Successful waypoint navigation"}),"\n",(0,i.jsx)(n.li,{children:"Effective obstacle avoidance"}),"\n",(0,i.jsx)(n.li,{children:"Robust patrol behavior"}),"\n",(0,i.jsx)(n.li,{children:"Reliable status reporting"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"object-detection-and-grasping-system",children:"Object Detection and Grasping System"}),"\n",(0,i.jsx)(n.p,{children:"Students develop a system that detects objects using camera and LIDAR, then plans and executes grasping motions."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Objectives:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate visual and range sensing"}),"\n",(0,i.jsx)(n.li,{children:"Implement object detection and localization"}),"\n",(0,i.jsx)(n.li,{children:"Plan grasping motions"}),"\n",(0,i.jsx)(n.li,{children:"Execute grasping with feedback"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Required Components:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Camera for object detection"}),"\n",(0,i.jsx)(n.li,{children:"LIDAR for range sensing"}),"\n",(0,i.jsx)(n.li,{children:"Robotic arm with gripper"}),"\n",(0,i.jsx)(n.li,{children:"Motion planning system"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Evaluation Criteria:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Accurate object detection"}),"\n",(0,i.jsx)(n.li,{children:"Successful grasping attempts"}),"\n",(0,i.jsx)(n.li,{children:"Robust sensor integration"}),"\n",(0,i.jsx)(n.li,{children:"Safe motion execution"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"multi-robot-coordination-system",children:"Multi-Robot Coordination System"}),"\n",(0,i.jsx)(n.p,{children:"Students create a system where multiple robots coordinate to perform a task, sharing information and avoiding conflicts."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Objectives:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement inter-robot communication"}),"\n",(0,i.jsx)(n.li,{children:"Coordinate robot activities"}),"\n",(0,i.jsx)(n.li,{children:"Avoid resource conflicts"}),"\n",(0,i.jsx)(n.li,{children:"Share environmental information"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Required Components:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Multiple robot platforms"}),"\n",(0,i.jsx)(n.li,{children:"Communication network"}),"\n",(0,i.jsx)(n.li,{children:"Task allocation system"}),"\n",(0,i.jsx)(n.li,{children:"Conflict resolution"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Evaluation Criteria:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Effective coordination"}),"\n",(0,i.jsx)(n.li,{children:"Conflict-free operation"}),"\n",(0,i.jsx)(n.li,{children:"Efficient resource usage"}),"\n",(0,i.jsx)(n.li,{children:"Robust communication"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"Chapter 5 provides practical, real-world examples of ROS2 implementations in robotics applications. Students learned how to combine all ROS2 communication patterns to build functional robotic systems including mobile robot controllers, sensor fusion systems, and navigation integration. The examples demonstrated complete system architectures and provided templates for students to adapt for their own applications."}),"\n",(0,i.jsx)(n.h2,{id:"quiz",children:"Quiz"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"What is the primary purpose of a robot bringup procedure?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A: To physically build the robot"}),"\n",(0,i.jsx)(n.li,{children:"B: Standard processes for starting robot systems with all required nodes"}),"\n",(0,i.jsx)(n.li,{children:"C: To calibrate sensors"}),"\n",(0,i.jsx)(n.li,{children:"D: To write code for the robot"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Answer: B"})," - Robot bringup procedures are standard processes for starting robot systems with all required nodes in the correct order and with proper configuration."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Why is sensor fusion important in robotics?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A: It makes sensors cheaper"}),"\n",(0,i.jsx)(n.li,{children:"B: It combines data from multiple sensors for enhanced perception and reliability"}),"\n",(0,i.jsx)(n.li,{children:"C: It reduces the number of sensors needed"}),"\n",(0,i.jsx)(n.li,{children:"D: It makes sensors faster"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Answer: B"})," - Sensor fusion combines data from multiple sensors to create more accurate, reliable, and comprehensive understanding of the environment than any single sensor could provide."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"What QoS policy should be used for critical safety-related messages?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A: Best effort with small history"}),"\n",(0,i.jsx)(n.li,{children:"B: Reliable with keep-all history"}),"\n",(0,i.jsx)(n.li,{children:"C: Volatile durability"}),"\n",(0,i.jsx)(n.li,{children:"D: Deadline-based policy only"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Answer: B"})," - For critical safety-related messages, reliable delivery with keep-all history ensures that all messages are delivered and none are lost."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"What is the main benefit of using ROS2 navigation stack?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A: It makes robots move faster"}),"\n",(0,i.jsx)(n.li,{children:"B: It provides tested, modular components for robot navigation"}),"\n",(0,i.jsx)(n.li,{children:"C: It reduces hardware requirements"}),"\n",(0,i.jsx)(n.li,{children:"D: It eliminates the need for sensors"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Answer: B"})," - The ROS2 navigation stack provides tested, modular components for robot navigation that can be configured and used rather than implementing navigation from scratch."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"When should you use composition in ROS2?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A: Always, to make systems faster"}),"\n",(0,i.jsx)(n.li,{children:"B: To run multiple nodes in the same process for efficiency and reduced latency"}),"\n",(0,i.jsx)(n.li,{children:"C: Only for debugging"}),"\n",(0,i.jsx)(n.li,{children:"D: When you need more memory"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Answer: B"})," - Composition is used to run multiple nodes in the same process to improve efficiency and reduce latency by avoiding network overhead."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,i.jsx)(n.p,{children:"After completing this chapter, students will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Design ROS2 architectures for robot systems"}),"\n",(0,i.jsx)(n.li,{children:"Implement nodes, topics, services, and actions"}),"\n",(0,i.jsx)(n.li,{children:"Manage parameters and configurations"}),"\n",(0,i.jsx)(n.li,{children:"Develop ROS2 packages for multi-robot systems"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Basic understanding of Python programming"}),"\n",(0,i.jsx)(n.li,{children:"Fundamentals of linear algebra and calculus"}),"\n",(0,i.jsx)(n.li,{children:"Basic knowledge of robotics concepts"}),"\n",(0,i.jsx)(n.li,{children:"Introduction to machine learning concepts"}),"\n",(0,i.jsx)(n.li,{children:"Completion of Module 0 (Introduction and Foundations)"}),"\n",(0,i.jsx)(n.li,{children:"Completion of Chapter 01 (Physical AI Basics)"}),"\n",(0,i.jsx)(n.li,{children:"Completion of Chapter 03 (ROS2 Nodes, Topics, and Services)"}),"\n",(0,i.jsx)(n.li,{children:"Completion of Chapter 04 (ROS2 Communication Patterns)"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"estimated-duration",children:"Estimated Duration"}),"\n",(0,i.jsx)(n.p,{children:"6 hours"})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var s=r(6540);const i={},t=s.createContext(i);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);