"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[309],{8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>s});var i=r(6540);const t={},o=i.createContext(t);function a(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(o.Provider,{value:n},e.children)}},9774:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"19-Chapter-3-Vision-Based-Navigation-Examples","title":"Chapter 3: Vision-Based Navigation Examples","description":"Overview","source":"@site/docusaurus/docs/19-Chapter-3-Vision-Based-Navigation-Examples.md","sourceDirName":".","slug":"/19-Chapter-3-Vision-Based-Navigation-Examples","permalink":"/physical-ai-book/ur/docs/19-Chapter-3-Vision-Based-Navigation-Examples","draft":false,"unlisted":false,"editUrl":"https://github.com/ayesha-muttalib123/physical-ai-book/tree/main/docusaurus/docs/19-Chapter-3-Vision-Based-Navigation-Examples.md","tags":[],"version":"current","sidebarPosition":19,"frontMatter":{"id":"19-Chapter-3-Vision-Based-Navigation-Examples","title":"Chapter 3: Vision-Based Navigation Examples","sidebar_position":19},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Humanoid Locomotion & Control","permalink":"/physical-ai-book/ur/docs/18-Chapter-2-Humanoid-Locomotion-Control"},"next":{"title":"Chapter 4: Language-Action Integration","permalink":"/physical-ai-book/ur/docs/20-Chapter-4-Language-Action-Integration"}}');var t=r(4848),o=r(8453);const a={id:"19-Chapter-3-Vision-Based-Navigation-Examples",title:"Chapter 3: Vision-Based Navigation Examples",sidebar_position:19},s="Chapter 3: Vision-Based Navigation Examples",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Why It Matters",id:"why-it-matters",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Visual SLAM",id:"visual-slam",level:3},{value:"Feature Detection and Matching",id:"feature-detection-and-matching",level:3},{value:"Optical Flow",id:"optical-flow",level:3},{value:"Deep Learning Navigation",id:"deep-learning-navigation",level:3},{value:"Monocular Depth Estimation",id:"monocular-depth-estimation",level:3},{value:"Semantic Navigation",id:"semantic-navigation",level:3},{value:"Visual Path Planning",id:"visual-path-planning",level:3},{value:"Navigation in Dynamic Environments",id:"navigation-in-dynamic-environments",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Visual SLAM Implementation",id:"visual-slam-implementation",level:3},{value:"Deep Learning Navigation System",id:"deep-learning-navigation-system",level:3},{value:"Semantic Navigation with Object Detection",id:"semantic-navigation-with-object-detection",level:3},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Autonomous Indoor Navigation System",id:"autonomous-indoor-navigation-system",level:3},{value:"Outdoor Exploration Robot",id:"outdoor-exploration-robot",level:3},{value:"Object Following Robot",id:"object-following-robot",level:3},{value:"Summary",id:"summary",level:2},{value:"Quiz",id:"quiz",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Estimated Duration",id:"estimated-duration",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-3-vision-based-navigation-examples",children:"Chapter 3: Vision-Based Navigation Examples"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"This chapter provides comprehensive examples of vision-based navigation systems that enable robots to perceive their environment and navigate autonomously. Students will learn to implement visual SLAM, landmark-based navigation, and deep learning approaches for navigation. The chapter covers both classical computer vision techniques and modern deep learning methods for visual navigation, with emphasis on real-world deployment and robustness in dynamic environments."}),"\n",(0,t.jsx)(n.h2,{id:"why-it-matters",children:"Why It Matters"}),"\n",(0,t.jsx)(n.p,{children:"Vision-based navigation is crucial for robots operating in unknown or partially mapped environments where traditional navigation methods may not be sufficient. Understanding how to extract navigation information from visual data enables robots to operate in diverse environments without relying solely on pre-built maps or external infrastructure. Vision-based navigation provides rich environmental information that can be used for both localization and path planning, making robots more adaptable and versatile."}),"\n",(0,t.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,t.jsx)(n.h3,{id:"visual-slam",children:"Visual SLAM"}),"\n",(0,t.jsx)(n.p,{children:"Simultaneous Localization and Mapping using visual inputs. This involves using camera images to simultaneously estimate the robot's position and create a map of the environment, enabling navigation in unknown spaces."}),"\n",(0,t.jsx)(n.h3,{id:"feature-detection-and-matching",children:"Feature Detection and Matching"}),"\n",(0,t.jsx)(n.p,{children:"Identifying and tracking visual landmarks. This includes detecting distinctive points in images and matching them across different viewpoints to enable camera pose estimation and mapping."}),"\n",(0,t.jsx)(n.h3,{id:"optical-flow",children:"Optical Flow"}),"\n",(0,t.jsx)(n.p,{children:"Tracking motion between consecutive frames. Optical flow provides information about the motion of objects and the camera, useful for navigation and obstacle detection."}),"\n",(0,t.jsx)(n.h3,{id:"deep-learning-navigation",children:"Deep Learning Navigation"}),"\n",(0,t.jsx)(n.p,{children:"Using neural networks for end-to-end navigation. This approach uses deep learning models to directly map visual inputs to navigation commands, bypassing traditional perception and planning stages."}),"\n",(0,t.jsx)(n.h3,{id:"monocular-depth-estimation",children:"Monocular Depth Estimation"}),"\n",(0,t.jsx)(n.p,{children:"Estimating depth from single camera images. This enables robots to understand 3D structure from 2D images, crucial for navigation in complex environments."}),"\n",(0,t.jsx)(n.h3,{id:"semantic-navigation",children:"Semantic Navigation"}),"\n",(0,t.jsx)(n.p,{children:"Using object recognition for navigation decisions. This involves understanding the semantic meaning of objects in the environment to make more informed navigation decisions."}),"\n",(0,t.jsx)(n.h3,{id:"visual-path-planning",children:"Visual Path Planning"}),"\n",(0,t.jsx)(n.p,{children:"Planning routes based on visual scene understanding. This includes identifying traversable areas, obstacles, and optimal paths based on visual input."}),"\n",(0,t.jsx)(n.h3,{id:"navigation-in-dynamic-environments",children:"Navigation in Dynamic Environments"}),"\n",(0,t.jsx)(n.p,{children:"Handling moving obstacles and changing scenes. This involves detecting and tracking moving objects while navigating, and adapting plans accordingly."}),"\n",(0,t.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,t.jsx)(n.h3,{id:"visual-slam-implementation",children:"Visual SLAM Implementation"}),"\n",(0,t.jsx)(n.p,{children:"Implementation of a basic visual SLAM system for robot navigation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nVisual SLAM Implementation\r\nDemonstrates basic visual SLAM for robot navigation\r\n"""\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom geometry_msgs.msg import PoseStamped, Twist\r\nfrom nav_msgs.msg import Odometry\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\nimport math\r\nfrom collections import deque\r\nimport tf2_ros\r\nfrom tf2_geometry_msgs import PoseStamped\r\nfrom typing import List, Tuple, Dict\r\nimport time\r\n\r\nclass FeatureTracker:\r\n    """Track visual features across frames for SLAM"""\r\n    def __init__(self, max_features=100):\r\n        self.max_features = max_features\r\n        self.detector = cv2.FastFeatureDetector_create(threshold=25)\r\n        self.descriptor = cv2.ORB_create()\r\n        self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\r\n\r\n        # Feature storage\r\n        self.previous_keypoints = None\r\n        self.current_keypoints = None\r\n        self.previous_descriptors = None\r\n        self.current_descriptors = None\r\n\r\n        # Track feature correspondences\r\n        self.feature_history = deque(maxlen=100)\r\n        self.tracked_features = []\r\n\r\n    def detect_features(self, image):\r\n        """Detect features in the current image"""\r\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\r\n        keypoints = self.detector.detect(gray, None)\r\n\r\n        # Limit number of features\r\n        if len(keypoints) > self.max_features:\r\n            # Sort by response and keep strongest\r\n            keypoints = sorted(keypoints, key=lambda x: x.response, reverse=True)[:self.max_features]\r\n\r\n        return keypoints\r\n\r\n    def compute_descriptors(self, image, keypoints):\r\n        """Compute descriptors for the detected keypoints"""\r\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\r\n        keypoints, descriptors = self.descriptor.compute(gray, keypoints)\r\n        return keypoints, descriptors\r\n\r\n    def match_features(self, kp1, desc1, kp2, desc2):\r\n        """Match features between two sets of keypoints and descriptors"""\r\n        if desc1 is None or desc2 is None or len(desc1) == 0 or len(desc2) == 0:\r\n            return [], []\r\n\r\n        # Match features\r\n        matches = self.matcher.knnMatch(desc1, desc2, k=2)\r\n\r\n        # Apply Lowe\'s ratio test\r\n        good_matches = []\r\n        for match_pair in matches:\r\n            if len(match_pair) == 2:\r\n                m, n = match_pair\r\n                if m.distance < 0.7 * n.distance:\r\n                    good_matches.append(m)\r\n\r\n        # Extract matched points\r\n        pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\r\n        pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\r\n\r\n        return pts1, pts2\r\n\r\n    def update_features(self, image):\r\n        """Update feature tracking with new image"""\r\n        current_kp = self.detect_features(image)\r\n        current_kp, current_desc = self.compute_descriptors(image, current_kp)\r\n\r\n        if self.previous_keypoints is not None and self.previous_descriptors is not None:\r\n            # Match current features with previous\r\n            pts_prev, pts_curr = self.match_features(\r\n                self.previous_keypoints, self.previous_descriptors,\r\n                current_kp, current_desc\r\n            )\r\n\r\n            # Store matches for pose estimation\r\n            self.matches_prev = pts_prev\r\n            self.matches_curr = pts_curr\r\n\r\n        # Update current features\r\n        self.previous_keypoints = current_kp\r\n        self.previous_descriptors = current_desc\r\n        self.current_keypoints = current_kp\r\n        self.current_descriptors = current_desc\r\n\r\nclass VisualSLAMNode(Node):\r\n    """ROS2 node for visual SLAM"""\r\n    def __init__(self):\r\n        super().__init__(\'visual_slam\')\r\n\r\n        # Publishers and subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'/camera/rgb/image_raw\', self.image_callback, 10)\r\n        self.camera_info_sub = self.create_subscription(\r\n            CameraInfo, \'/camera/rgb/camera_info\', self.camera_info_callback, 10)\r\n        self.odom_pub = self.create_publisher(Odometry, \'/visual_slam/odometry\', 10)\r\n        self.pose_pub = self.create_publisher(PoseStamped, \'/visual_slam/pose\', 10)\r\n\r\n        # Initialize OpenCV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Initialize feature tracker\r\n        self.feature_tracker = FeatureTracker()\r\n\r\n        # Camera parameters\r\n        self.camera_matrix = None\r\n        self.dist_coeffs = None\r\n\r\n        # Robot state\r\n        self.current_position = np.array([0.0, 0.0, 0.0])\r\n        self.current_orientation = np.array([0.0, 0.0, 0.0, 1.0])  # quaternion\r\n        self.previous_position = self.current_position.copy()\r\n        self.estimated_velocity = np.array([0.0, 0.0, 0.0])\r\n\r\n        # Map representation\r\n        self.map_points = []  # 3D points in the map\r\n        self.keyframes = []   # Key poses of the robot\r\n\r\n        # Frame timing\r\n        self.prev_time = time.time()\r\n        self.frame_count = 0\r\n\r\n        self.get_logger().info(\'Visual SLAM node initialized\')\r\n\r\n    def camera_info_callback(self, msg):\r\n        """Update camera parameters from camera info"""\r\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\r\n        self.dist_coeffs = np.array(msg.d)\r\n\r\n    def image_callback(self, msg):\r\n        """Process incoming camera image for SLAM"""\r\n        try:\r\n            # Convert ROS image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\r\n\r\n            # Update feature tracking\r\n            self.feature_tracker.update_features(cv_image)\r\n\r\n            # Estimate camera motion from feature matches\r\n            if hasattr(self.feature_tracker, \'matches_prev\') and \\\r\n               len(self.feature_tracker.matches_prev) >= 10:  # Need enough matches\r\n\r\n                # Estimate motion using essential matrix\r\n                motion_estimate = self.estimate_camera_motion(\r\n                    self.feature_tracker.matches_prev,\r\n                    self.feature_tracker.matches_curr\r\n                )\r\n\r\n                if motion_estimate is not None:\r\n                    # Update robot pose based on motion estimate\r\n                    self.update_robot_pose(motion_estimate)\r\n\r\n                    # Add keyframe if movement is significant\r\n                    displacement = np.linalg.norm(self.current_position - self.previous_position)\r\n                    if displacement > 0.1:  # Add keyframe every 10cm\r\n                        self.add_keyframe()\r\n                        self.previous_position = self.current_position.copy()\r\n\r\n            # Publish current pose and odometry\r\n            self.publish_pose()\r\n            self.publish_odometry(msg.header)\r\n\r\n            self.frame_count += 1\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error processing image: {e}\')\r\n\r\n    def estimate_camera_motion(self, prev_points, curr_points):\r\n        """Estimate camera motion from matched feature points"""\r\n        if len(prev_points) < 5 or len(curr_points) < 5:\r\n            return None\r\n\r\n        # Estimate essential matrix\r\n        E, mask = cv2.findEssentialMat(\r\n            curr_points, prev_points,\r\n            self.camera_matrix,\r\n            method=cv2.RANSAC,\r\n            prob=0.999,\r\n            threshold=1.0\r\n        )\r\n\r\n        if E is None or len(E) < 3:\r\n            return None\r\n\r\n        # Decompose essential matrix to get rotation and translation\r\n        _, R, t, _ = cv2.recoverPose(E, curr_points, prev_points, self.camera_matrix)\r\n\r\n        # Convert rotation matrix to angle-axis representation\r\n        rotation_vec, _ = cv2.Rodrigues(R)\r\n        rotation_angle = np.linalg.norm(rotation_vec)\r\n\r\n        if rotation_angle > 0:\r\n            rotation_axis = rotation_vec / rotation_angle\r\n        else:\r\n            rotation_axis = np.array([0, 0, 1])  # Default axis\r\n\r\n        # Calculate translation magnitude\r\n        translation_magnitude = np.linalg.norm(t)\r\n\r\n        return {\r\n            \'rotation_axis\': rotation_axis,\r\n            \'rotation_angle\': rotation_angle,\r\n            \'translation\': t.flatten(),\r\n            \'translation_magnitude\': translation_magnitude\r\n        }\r\n\r\n    def update_robot_pose(self, motion_estimate):\r\n        """Update robot pose based on motion estimate"""\r\n        # Calculate time since last frame\r\n        current_time = time.time()\r\n        dt = current_time - self.prev_time\r\n        self.prev_time = current_time\r\n\r\n        if dt > 0:\r\n            # Update position based on translation\r\n            # Scale translation by some factor to convert to real-world units\r\n            scale_factor = 0.1  # Adjust based on camera calibration\r\n            translation_scaled = motion_estimate[\'translation\'] * scale_factor\r\n\r\n            # Update position (in robot\'s local frame, need to transform to global)\r\n            # This is a simplified approach - in real SLAM, you\'d use more sophisticated pose graph optimization\r\n            self.current_position += translation_scaled\r\n\r\n            # Update orientation\r\n            # This is also simplified - real implementation would use proper quaternion math\r\n            delta_quat = self.axis_angle_to_quaternion(\r\n                motion_estimate[\'rotation_axis\'],\r\n                motion_estimate[\'rotation_angle\']\r\n            )\r\n\r\n            # Integrate rotation\r\n            self.current_orientation = self.multiply_quaternions(\r\n                self.current_orientation, delta_quat\r\n            )\r\n\r\n            # Calculate velocity\r\n            self.estimated_velocity = translation_scaled / dt\r\n\r\n    def axis_angle_to_quaternion(self, axis, angle):\r\n        """Convert axis-angle representation to quaternion"""\r\n        half_angle = angle / 2.0\r\n        sin_half = math.sin(half_angle)\r\n\r\n        qx = axis[0] * sin_half\r\n        qy = axis[1] * sin_half\r\n        qz = axis[2] * sin_half\r\n        qw = math.cos(half_angle)\r\n\r\n        return np.array([qx, qy, qz, qw])\r\n\r\n    def multiply_quaternions(self, q1, q2):\r\n        """Multiply two quaternions"""\r\n        w1, x1, y1, z1 = q1\r\n        w2, x2, y2, z2 = q2\r\n\r\n        w = w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2\r\n        x = w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2\r\n        y = w1 * y2 - x1 * z2 + y1 * w2 + z1 * x2\r\n        z = w1 * z2 + x1 * y2 - y1 * x2 + z1 * w2\r\n\r\n        # Normalize quaternion\r\n        norm = math.sqrt(w*w + x*x + y*y + z*z)\r\n        if norm > 0:\r\n            return np.array([x/norm, y/norm, z/norm, w/norm])\r\n        else:\r\n            return np.array([0, 0, 0, 1])\r\n\r\n    def add_keyframe(self):\r\n        """Add current pose as a keyframe in the map"""\r\n        keyframe = {\r\n            \'position\': self.current_position.copy(),\r\n            \'orientation\': self.current_orientation.copy(),\r\n            \'timestamp\': self.get_clock().now().to_msg(),\r\n            \'features\': len(self.feature_tracker.current_keypoints) if self.feature_tracker.current_keypoints else 0\r\n        }\r\n        self.keyframes.append(keyframe)\r\n\r\n        # Limit number of keyframes to prevent memory growth\r\n        if len(self.keyframes) > 100:\r\n            self.keyframes.pop(0)\r\n\r\n    def publish_pose(self):\r\n        """Publish current robot pose"""\r\n        pose_msg = PoseStamped()\r\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\r\n        pose_msg.header.frame_id = \'map\'\r\n\r\n        pose_msg.pose.position.x = float(self.current_position[0])\r\n        pose_msg.pose.position.y = float(self.current_position[1])\r\n        pose_msg.pose.position.z = float(self.current_position[2])\r\n\r\n        pose_msg.pose.orientation.x = float(self.current_orientation[0])\r\n        pose_msg.pose.orientation.y = float(self.current_orientation[1])\r\n        pose_msg.pose.orientation.z = float(self.current_orientation[2])\r\n        pose_msg.pose.orientation.w = float(self.current_orientation[3])\r\n\r\n        self.pose_pub.publish(pose_msg)\r\n\r\n    def publish_odometry(self, header):\r\n        """Publish odometry information"""\r\n        odom_msg = Odometry()\r\n        odom_msg.header = header\r\n        odom_msg.child_frame_id = \'base_link\'\r\n\r\n        # Position\r\n        odom_msg.pose.pose.position.x = float(self.current_position[0])\r\n        odom_msg.pose.pose.position.y = float(self.current_position[1])\r\n        odom_msg.pose.pose.position.z = float(self.current_position[2])\r\n\r\n        odom_msg.pose.pose.orientation.x = float(self.current_orientation[0])\r\n        odom_msg.pose.pose.orientation.y = float(self.current_orientation[1])\r\n        odom_msg.pose.pose.orientation.z = float(self.current_orientation[2])\r\n        odom_msg.pose.pose.orientation.w = float(self.current_orientation[3])\r\n\r\n        # Velocity\r\n        odom_msg.twist.twist.linear.x = float(self.estimated_velocity[0])\r\n        odom_msg.twist.twist.linear.y = float(self.estimated_velocity[1])\r\n        odom_msg.twist.twist.linear.z = float(self.estimated_velocity[2])\r\n\r\n        self.odom_pub.publish(odom_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VisualSLAMNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info(\'Shutting down visual SLAM node...\')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"deep-learning-navigation-system",children:"Deep Learning Navigation System"}),"\n",(0,t.jsx)(n.p,{children:"Implementation of a deep learning-based navigation system using visual inputs:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nDeep Learning Navigation System\r\nDemonstrates end-to-end learning for visual navigation\r\n"""\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom geometry_msgs.msg import Twist\r\nfrom std_msgs.msg import Float32\r\nfrom cv_bridge import CvBridge\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torchvision.transforms as transforms\r\nimport numpy as np\r\nimport cv2\r\nfrom collections import deque\r\nimport time\r\n\r\nclass NavigationCNN(nn.Module):\r\n    """Convolutional Neural Network for visual navigation"""\r\n    def __init__(self, num_outputs=2):  # linear vel, angular vel\r\n        super().__init__()\r\n\r\n        # Convolutional layers for feature extraction\r\n        self.conv_layers = nn.Sequential(\r\n            nn.Conv2d(3, 32, kernel_size=8, stride=4),  # 3->32 channels, 8x8 kernel, 4x4 stride\r\n            nn.ReLU(),\r\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),  # 32->64 channels, 4x4 kernel, 2x2 stride\r\n            nn.ReLU(),\r\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),  # 64->64 channels, 3x3 kernel, 1x1 stride\r\n            nn.ReLU(),\r\n            nn.Flatten()\r\n        )\r\n\r\n        # Calculate the size after convolutions\r\n        # Assuming input is 224x224: (224-8)/4+1=55, (55-4)/2+1=26, (26-3)+1=24\r\n        # So after convolutions: 64 * 24 * 24 = 36864\r\n        conv_output_size = 64 * 54 * 54  # Adjust based on actual input size after convolutions\r\n\r\n        # Fully connected layers for navigation decision\r\n        self.fc_layers = nn.Sequential(\r\n            nn.Linear(conv_output_size, 512),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.5),\r\n            nn.Linear(512, 256),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.5),\r\n            nn.Linear(256, num_outputs)\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.conv_layers(x)\r\n        x = self.fc_layers(x)\r\n        return x\r\n\r\nclass DepthEstimationNet(nn.Module):\r\n    """Network for monocular depth estimation"""\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        # Encoder (simplified ResNet-like structure)\r\n        self.encoder = nn.Sequential(\r\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\r\n            nn.ReLU()\r\n        )\r\n\r\n        # Decoder to upsample back to original size\r\n        self.decoder = nn.Sequential(\r\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\r\n            nn.ReLU(),\r\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\r\n            nn.ReLU(),\r\n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(32, 1, kernel_size=3, stride=1, padding=1),  # Output single channel for depth\r\n            nn.Sigmoid()  # Normalize to [0,1] range\r\n        )\r\n\r\n    def forward(self, x):\r\n        encoded = self.encoder(x)\r\n        depth = self.decoder(encoded)\r\n        return depth\r\n\r\nclass DeepNavigationNode(Node):\r\n    """ROS2 node for deep learning-based navigation"""\r\n    def __init__(self):\r\n        super().__init__(\'deep_navigation\')\r\n\r\n        # Publishers and subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'/camera/rgb/image_raw\', self.image_callback, 10)\r\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        self.depth_pub = self.create_publisher(Image, \'/estimated_depth\', 10)\r\n        self.confidence_pub = self.create_publisher(Float32, \'/navigation_confidence\', 10)\r\n\r\n        # Initialize OpenCV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Initialize neural networks\r\n        self.navigation_net = NavigationCNN(num_outputs=2)\r\n        self.depth_net = DepthEstimationNet()\r\n\r\n        # Load pre-trained models if available\r\n        # For this example, we\'ll use randomly initialized models\r\n        # In practice, you would load trained models\r\n        self.get_logger().info(\'Loading navigation models...\')\r\n\r\n        # Set to evaluation mode\r\n        self.navigation_net.eval()\r\n        self.depth_net.eval()\r\n\r\n        # Image preprocessing\r\n        self.transform = transforms.Compose([\r\n            transforms.ToPILImage(),\r\n            transforms.Resize((224, 224)),\r\n            transforms.ToTensor(),\r\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\r\n        ])\r\n\r\n        # Navigation parameters\r\n        self.speed_scale = 0.5  # Scale factor for output velocities\r\n        self.depth_enabled = True  # Whether to estimate depth\r\n\r\n        # Performance tracking\r\n        self.inference_times = deque(maxlen=100)\r\n        self.last_inference_time = time.time()\r\n\r\n        self.get_logger().info(\'Deep Navigation node initialized\')\r\n\r\n    def image_callback(self, msg):\r\n        """Process image and generate navigation command using deep learning"""\r\n        try:\r\n            start_time = time.time()\r\n\r\n            # Convert ROS image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\r\n\r\n            # Preprocess image for neural network\r\n            # Convert BGR to RGB\r\n            rgb_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\r\n\r\n            # Apply transformations\r\n            input_tensor = self.transform(rgb_image).unsqueeze(0)  # Add batch dimension\r\n\r\n            # Run navigation network inference\r\n            with torch.no_grad():\r\n                navigation_output = self.navigation_net(input_tensor)\r\n\r\n                # Extract linear and angular velocities\r\n                linear_vel = float(navigation_output[0, 0])\r\n                angular_vel = float(navigation_output[0, 1])\r\n\r\n                # Apply scaling and saturation\r\n                linear_vel = max(min(linear_vel * self.speed_scale, 0.5), -0.5)  # Max 0.5 m/s\r\n                angular_vel = max(min(angular_vel * self.speed_scale, 1.0), -1.0)  # Max 1.0 rad/s\r\n\r\n            # Optionally run depth estimation\r\n            if self.depth_enabled:\r\n                with torch.no_grad():\r\n                    depth_output = self.depth_net(input_tensor)\r\n\r\n                    # Convert depth output back to image format\r\n                    depth_map = depth_output.squeeze().cpu().numpy()\r\n                    depth_map = (depth_map * 255).astype(np.uint8)  # Scale to 0-255\r\n\r\n                    # Publish depth estimate\r\n                    depth_msg = self.bridge.cv2_to_imgmsg(depth_map, encoding=\'mono8\')\r\n                    depth_msg.header = msg.header\r\n                    self.depth_pub.publish(depth_msg)\r\n\r\n            # Create and publish velocity command\r\n            cmd_vel = Twist()\r\n            cmd_vel.linear.x = linear_vel\r\n            cmd_vel.angular.z = angular_vel\r\n            self.cmd_vel_pub.publish(cmd_vel)\r\n\r\n            # Calculate and log inference time\r\n            inference_time = (time.time() - start_time) * 1000  # ms\r\n            self.inference_times.append(inference_time)\r\n\r\n            # Publish navigation confidence (based on variance of recent inferences)\r\n            if len(self.inference_times) > 10:\r\n                avg_time = sum(self.inference_times) / len(self.inference_times)\r\n                confidence = 1.0 / (1.0 + avg_time/100.0)  # Lower inference time = higher confidence\r\n                confidence_msg = Float32()\r\n                confidence_msg.data = confidence\r\n                self.confidence_pub.publish(confidence_msg)\r\n\r\n            # Log performance periodically\r\n            if len(self.inference_times) % 20 == 0:\r\n                avg_inference = sum(self.inference_times) / len(self.inference_times)\r\n                self.get_logger().info(\r\n                    f\'Navigation inference: {avg_inference:.2f}ms, \'\r\n                    f\'Linear: {linear_vel:.2f}, Angular: {angular_vel:.2f}\'\r\n                )\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error in deep navigation: {e}\')\r\n\r\n    def enable_depth_estimation(self, enable=True):\r\n        """Enable or disable depth estimation"""\r\n        self.depth_enabled = enable\r\n        self.get_logger().info(f\'Depth estimation {"enabled" if enable else "disabled"}\')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = DeepNavigationNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info(\'Shutting down deep navigation node...\')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"semantic-navigation-with-object-detection",children:"Semantic Navigation with Object Detection"}),"\n",(0,t.jsx)(n.p,{children:"Implementation of semantic navigation using object detection to make navigation decisions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nSemantic Navigation with Object Detection\r\nDemonstrates navigation based on semantic understanding of the environment\r\n\"\"\"\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom geometry_msgs.msg import Twist, Point\r\nfrom vision_msgs.msg import Detection2DArray, Detection2D\r\nfrom std_msgs.msg import String\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\nimport torch\r\nimport torchvision.transforms as transforms\r\nfrom ultralytics import YOLO\r\nimport math\r\nfrom typing import List, Dict, Tuple\r\n\r\nclass SemanticNavigatorNode(Node):\r\n    \"\"\"ROS2 node for semantic navigation using object detection\"\"\"\r\n    def __init__(self):\r\n        super().__init__('semantic_navigator')\r\n\r\n        # Publishers and subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image, '/camera/rgb/image_raw', self.image_callback, 10)\r\n        self.detection_pub = self.create_publisher(Detection2DArray, '/semantic_detections', 10)\r\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n        self.status_pub = self.create_publisher(String, '/navigation_status', 10)\r\n\r\n        # Initialize OpenCV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Load YOLO object detection model\r\n        self.get_logger().info('Loading YOLO model...')\r\n        try:\r\n            self.model = YOLO('yolov8n.pt')  # Use smaller model for efficiency\r\n        except:\r\n            # If YOLO model is not available, use a mock detector\r\n            self.model = None\r\n            self.get_logger().warn('YOLO model not available, using mock detector')\r\n\r\n        # Navigation parameters\r\n        self.safe_distance = 0.8  # meters\r\n        self.target_object_class = 'person'  # Class to follow\r\n        self.avoid_object_classes = ['car', 'truck', 'bus']  # Classes to avoid\r\n        self.follow_distance = 2.0  # meters to maintain from target\r\n        self.linear_speed = 0.3  # m/s\r\n        self.angular_speed = 0.5  # rad/s\r\n\r\n        # Object tracking\r\n        self.tracked_objects = {}\r\n        self.navigation_targets = []\r\n        self.avoidance_targets = []\r\n\r\n        # Performance tracking\r\n        self.frame_count = 0\r\n        self.detection_count = 0\r\n\r\n        self.get_logger().info('Semantic Navigator initialized')\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Process image and perform semantic navigation\"\"\"\r\n        try:\r\n            # Convert ROS image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\r\n\r\n            # Perform object detection\r\n            if self.model is not None:\r\n                results = self.model(cv_image)\r\n                detections = self.process_detections(results, cv_image.shape)\r\n            else:\r\n                # Mock detection for simulation\r\n                detections = self.mock_detections(cv_image)\r\n\r\n            # Update tracked objects\r\n            self.update_tracked_objects(detections)\r\n\r\n            # Determine navigation action based on semantic understanding\r\n            cmd_vel = self.determine_navigation_action(detections, cv_image.shape)\r\n\r\n            # Publish detections for visualization\r\n            self.publish_detections(detections, msg.header)\r\n\r\n            # Publish navigation command\r\n            if cmd_vel is not None:\r\n                self.cmd_vel_pub.publish(cmd_vel)\r\n\r\n            # Update statistics\r\n            self.frame_count += 1\r\n            if detections:\r\n                self.detection_count += 1\r\n\r\n            # Log status periodically\r\n            if self.frame_count % 30 == 0:  # Every 30 frames\r\n                detection_rate = (self.detection_count / self.frame_count) * 100\r\n                status_msg = String()\r\n                status_msg.data = f'Detection rate: {detection_rate:.1f}%, Objects tracked: {len(self.tracked_objects)}'\r\n                self.status_pub.publish(status_msg)\r\n                self.get_logger().info(status_msg.data)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in semantic navigation: {e}')\r\n\r\n    def process_detections(self, results, image_shape):\r\n        \"\"\"Process YOLO results into standardized format\"\"\"\r\n        detections = []\r\n        height, width = image_shape[:2]\r\n\r\n        for result in results:\r\n            if result.boxes is not None:\r\n                for box in result.boxes:\r\n                    # Extract bounding box information\r\n                    xyxy = box.xyxy[0].cpu().numpy()  # [x1, y1, x2, y2]\r\n                    conf = float(box.conf[0])\r\n                    cls = int(box.cls[0])\r\n\r\n                    # Get class name\r\n                    class_name = self.model.names[cls]\r\n\r\n                    # Create detection object\r\n                    detection = {\r\n                        'class_name': class_name,\r\n                        'confidence': conf,\r\n                        'bbox': {\r\n                            'x': int(xyxy[0]),\r\n                            'y': int(xyxy[1]),\r\n                            'width': int(xyxy[2] - xyxy[0]),\r\n                            'height': int(xyxy[3] - xyxy[1])\r\n                        },\r\n                        'center': {\r\n                            'x': int((xyxy[0] + xyxy[2]) / 2),\r\n                            'y': int((xyxy[1] + xyxy[3]) / 2)\r\n                        }\r\n                    }\r\n\r\n                    # Only include high-confidence detections\r\n                    if conf > 0.5:\r\n                        detections.append(detection)\r\n\r\n        return detections\r\n\r\n    def mock_detections(self, image):\r\n        \"\"\"Mock detection function for simulation\"\"\"\r\n        height, width = image.shape[:2]\r\n        detections = []\r\n\r\n        # Simulate some detections\r\n        if np.random.random() > 0.3:  # 70% chance of detection\r\n            detection = {\r\n                'class_name': 'person',\r\n                'confidence': np.random.uniform(0.6, 0.95),\r\n                'bbox': {\r\n                    'x': np.random.randint(0, width//2),\r\n                    'y': np.random.randint(height//4, 3*height//4),\r\n                    'width': np.random.randint(50, 150),\r\n                    'height': np.random.randint(100, 300)\r\n                },\r\n                'center': {\r\n                    'x': np.random.randint(0, width//2),\r\n                    'y': np.random.randint(height//4, 3*height//4)\r\n                }\r\n            }\r\n            detections.append(detection)\r\n\r\n        return detections\r\n\r\n    def update_tracked_objects(self, detections):\r\n        \"\"\"Update tracked objects based on current detections\"\"\"\r\n        current_time = self.get_clock().now().seconds_nanoseconds()[0]\r\n\r\n        # Clear old tracks\r\n        for obj_id in list(self.tracked_objects.keys()):\r\n            if current_time - self.tracked_objects[obj_id]['last_seen'] > 2.0:  # 2 seconds\r\n                del self.tracked_objects[obj_id]\r\n\r\n        # Update existing tracks and add new ones\r\n        for detection in detections:\r\n            # Find if this detection matches an existing track\r\n            matched_track = None\r\n            for obj_id, track in self.tracked_objects.items():\r\n                # Simple distance-based matching\r\n                prev_center = track['detection']['center']\r\n                curr_center = detection['center']\r\n                distance = math.sqrt(\r\n                    (prev_center['x'] - curr_center['x'])**2 +\r\n                    (prev_center['y'] - curr_center['y'])**2\r\n                )\r\n\r\n                if distance < 100:  # Threshold for matching\r\n                    matched_track = obj_id\r\n                    break\r\n\r\n            if matched_track:\r\n                # Update existing track\r\n                self.tracked_objects[matched_track]['detection'] = detection\r\n                self.tracked_objects[matched_track]['last_seen'] = current_time\r\n                self.tracked_objects[matched_track]['history'].append(detection)\r\n                if len(self.tracked_objects[matched_track]['history']) > 10:\r\n                    self.tracked_objects[matched_track]['history'].pop(0)\r\n            else:\r\n                # Create new track\r\n                new_id = f\"obj_{len(self.tracked_objects)}\"\r\n                self.tracked_objects[new_id] = {\r\n                    'detection': detection,\r\n                    'first_seen': current_time,\r\n                    'last_seen': current_time,\r\n                    'history': [detection],\r\n                    'velocity': None\r\n                }\r\n\r\n    def determine_navigation_action(self, detections, image_shape):\r\n        \"\"\"Determine navigation action based on semantic understanding\"\"\"\r\n        height, width = image_shape[:2]\r\n        cmd_vel = Twist()\r\n\r\n        # Find target objects to follow\r\n        target_detections = [det for det in detections if det['class_name'] == self.target_object_class]\r\n\r\n        # Find objects to avoid\r\n        avoid_detections = [det for det in detections if det['class_name'] in self.avoid_object_classes]\r\n\r\n        if avoid_detections:\r\n            # Priority: avoid obstacles first\r\n            closest_avoid = min(avoid_detections, key=lambda x: x['center']['x'])\r\n            cmd_vel = self.avoid_obstacle(closest_avoid, width)\r\n        elif target_detections:\r\n            # Follow target object\r\n            closest_target = min(target_detections, key=lambda x: x['center']['x'])\r\n            cmd_vel = self.follow_object(closest_target, width, height)\r\n        else:\r\n            # No targets, explore or stop\r\n            cmd_vel.linear.x = 0.0\r\n            cmd_vel.angular.z = 0.0\r\n\r\n        return cmd_vel\r\n\r\n    def avoid_obstacle(self, detection, image_width):\r\n        \"\"\"Generate command to avoid an obstacle\"\"\"\r\n        cmd_vel = Twist()\r\n\r\n        # Determine direction to turn based on object position\r\n        obj_center_x = detection['center']['x']\r\n        image_center = image_width / 2\r\n\r\n        if obj_center_x < image_center:\r\n            # Obstacle is on the left, turn right\r\n            cmd_vel.linear.x = self.linear_speed * 0.3  # Slow down\r\n            cmd_vel.angular.z = -self.angular_speed\r\n        else:\r\n            # Obstacle is on the right, turn left\r\n            cmd_vel.linear.x = self.linear_speed * 0.3  # Slow down\r\n            cmd_vel.angular.z = self.angular_speed\r\n\r\n        self.get_logger().info(f'Avoiding {detection[\"class_name\"]} at {obj_center_x}')\r\n\r\n        return cmd_vel\r\n\r\n    def follow_object(self, detection, image_width, image_height):\r\n        \"\"\"Generate command to follow an object\"\"\"\r\n        cmd_vel = Twist()\r\n\r\n        # Get object position in image\r\n        obj_center_x = detection['center']['x']\r\n        obj_center_y = detection['center']['y']\r\n        obj_bbox = detection['bbox']\r\n\r\n        # Calculate position error\r\n        image_center_x = image_width / 2\r\n        image_center_y = image_height / 2\r\n\r\n        x_error = obj_center_x - image_center_x\r\n        y_error = obj_center_y - image_center_y\r\n\r\n        # Calculate approximate distance based on object size (simplified)\r\n        obj_size = obj_bbox['width'] * obj_bbox['height']\r\n        # Larger objects are closer, smaller are farther (simplified model)\r\n        distance_estimate = 10000 / (obj_size + 1)  # Inverse relationship\r\n\r\n        # Navigation logic\r\n        if distance_estimate > self.follow_distance * 1.2:  # Too far\r\n            cmd_vel.linear.x = self.linear_speed\r\n        elif distance_estimate < self.follow_distance * 0.8:  # Too close\r\n            cmd_vel.linear.x = -self.linear_speed * 0.5  # Back up slowly\r\n        else:  # Just right\r\n            cmd_vel.linear.x = 0.0\r\n\r\n        # Turn to center object horizontally\r\n        cmd_vel.angular.z = -x_error * 0.001  # Proportional control\r\n\r\n        # Limit angular velocity\r\n        cmd_vel.angular.z = max(min(cmd_vel.angular.z, self.angular_speed), -self.angular_speed)\r\n\r\n        self.get_logger().info(\r\n            f'Following {detection[\"class_name\"]} - '\r\n            f'Distance: {distance_estimate:.2f}m, '\r\n            f'X error: {x_error:.1f}px'\r\n        )\r\n\r\n        return cmd_vel\r\n\r\n    def publish_detections(self, detections, header):\r\n        \"\"\"Publish detections for visualization\"\"\"\r\n        detection_array = Detection2DArray()\r\n        detection_array.header = header\r\n\r\n        for detection in detections:\r\n            detection_2d = Detection2D()\r\n            detection_2d.header = header\r\n\r\n            # Set bounding box\r\n            bbox = BoundingBox2D()\r\n            bbox.center.x = detection['bbox']['x'] + detection['bbox']['width'] / 2\r\n            bbox.center.y = detection['bbox']['y'] + detection['bbox']['height'] / 2\r\n            bbox.center.theta = 0.0\r\n            bbox.size_x = detection['bbox']['width']\r\n            bbox.size_y = detection['bbox']['height']\r\n            detection_2d.bbox = bbox\r\n\r\n            # Add results\r\n            result = ObjectHypothesisWithPose()\r\n            result.id = detection['class_name']\r\n            result.score = detection['confidence']\r\n            detection_2d.results.append(result)\r\n\r\n            detection_array.detections.append(detection_2d)\r\n\r\n        self.detection_pub.publish(detection_array)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = SemanticNavigationNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('Shutting down semantic navigation node...')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,t.jsx)(n.h3,{id:"autonomous-indoor-navigation-system",children:"Autonomous Indoor Navigation System"}),"\n",(0,t.jsx)(n.p,{children:"Students implement a complete indoor navigation system using visual SLAM and semantic understanding."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Objectives:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement visual SLAM for localization"}),"\n",(0,t.jsx)(n.li,{children:"Create semantic mapping of environment"}),"\n",(0,t.jsx)(n.li,{children:"Develop navigation planner using visual information"}),"\n",(0,t.jsx)(n.li,{children:"Test system in indoor environments"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Required Components:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Robot platform with camera"}),"\n",(0,t.jsx)(n.li,{children:"Visual SLAM implementation"}),"\n",(0,t.jsx)(n.li,{children:"Semantic segmentation model"}),"\n",(0,t.jsx)(n.li,{children:"Navigation planning algorithms"}),"\n",(0,t.jsx)(n.li,{children:"Indoor environment"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Evaluation Criteria:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Localization accuracy"}),"\n",(0,t.jsx)(n.li,{children:"Semantic mapping quality"}),"\n",(0,t.jsx)(n.li,{children:"Navigation success rate"}),"\n",(0,t.jsx)(n.li,{children:"System robustness"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"outdoor-exploration-robot",children:"Outdoor Exploration Robot"}),"\n",(0,t.jsx)(n.p,{children:"Students develop a robot that can explore outdoor environments using vision-based navigation and obstacle avoidance."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Objectives:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement outdoor navigation with visual cues"}),"\n",(0,t.jsx)(n.li,{children:"Create robust obstacle detection and avoidance"}),"\n",(0,t.jsx)(n.li,{children:"Develop terrain classification from visual input"}),"\n",(0,t.jsx)(n.li,{children:"Test in diverse outdoor conditions"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Required Components:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"All-terrain robot platform"}),"\n",(0,t.jsx)(n.li,{children:"Stereo camera or depth sensor"}),"\n",(0,t.jsx)(n.li,{children:"Outdoor navigation algorithms"}),"\n",(0,t.jsx)(n.li,{children:"Terrain classification models"}),"\n",(0,t.jsx)(n.li,{children:"Testing environment"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Evaluation Criteria:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Navigation performance outdoors"}),"\n",(0,t.jsx)(n.li,{children:"Obstacle detection accuracy"}),"\n",(0,t.jsx)(n.li,{children:"Terrain classification effectiveness"}),"\n",(0,t.jsx)(n.li,{children:"System reliability"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"object-following-robot",children:"Object Following Robot"}),"\n",(0,t.jsx)(n.p,{children:"Students create a robot that can follow specific objects using deep learning-based detection and tracking."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Objectives:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement object detection and tracking"}),"\n",(0,t.jsx)(n.li,{children:"Create smooth following behavior"}),"\n",(0,t.jsx)(n.li,{children:"Handle occlusions and tracking failures"}),"\n",(0,t.jsx)(n.li,{children:"Maintain safe following distance"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Required Components:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Robot with camera"}),"\n",(0,t.jsx)(n.li,{children:"Object detection model"}),"\n",(0,t.jsx)(n.li,{children:"Tracking algorithm"}),"\n",(0,t.jsx)(n.li,{children:"Following behavior controller"}),"\n",(0,t.jsx)(n.li,{children:"Test objects for following"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Evaluation Criteria:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Tracking accuracy and robustness"}),"\n",(0,t.jsx)(n.li,{children:"Smooth following behavior"}),"\n",(0,t.jsx)(n.li,{children:"Safe distance maintenance"}),"\n",(0,t.jsx)(n.li,{children:"Occlusion handling"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Chapter 15 covered integration of vision-based navigation systems, including visual SLAM, deep learning navigation, and semantic navigation approaches. Students learned to process visual information for robot localization, mapping, and navigation decisions. The chapter emphasized the importance of visual perception in enabling robots to operate in unknown environments and demonstrated various approaches from classical computer vision to modern deep learning methods."}),"\n",(0,t.jsx)(n.h2,{id:"quiz",children:"Quiz"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What is the main purpose of visual SLAM in robotics?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A: To eliminate the need for sensors"}),"\n",(0,t.jsx)(n.li,{children:"B: To simultaneously localize the robot and map the environment using visual input"}),"\n",(0,t.jsx)(n.li,{children:"C: To make robots move faster"}),"\n",(0,t.jsx)(n.li,{children:"D: To simplify robot programming"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer: B"})," - Visual SLAM (Simultaneous Localization and Mapping) uses visual input to simultaneously estimate the robot's position and create a map of the environment."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What does optical flow measure in visual navigation?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A: Color information only"}),"\n",(0,t.jsx)(n.li,{children:"B: Motion between consecutive frames"}),"\n",(0,t.jsx)(n.li,{children:"C: Depth information"}),"\n",(0,t.jsx)(n.li,{children:"D: Lighting conditions"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer: B"})," - Optical flow measures the motion of pixels between consecutive frames, providing information about camera and object movement."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What is semantic navigation?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A: Navigation using only geometric information"}),"\n",(0,t.jsx)(n.li,{children:"B: Navigation using object recognition and scene understanding"}),"\n",(0,t.jsx)(n.li,{children:"C: Navigation with semantic maps only"}),"\n",(0,t.jsx)(n.li,{children:"D: Navigation using text commands"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer: B"})," - Semantic navigation uses object recognition and scene understanding to make navigation decisions based on the meaning of objects in the environment."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Why is feature detection important in visual SLAM?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A: To reduce computation time only"}),"\n",(0,t.jsx)(n.li,{children:"B: To identify and track distinctive points for pose estimation and mapping"}),"\n",(0,t.jsx)(n.li,{children:"C: To increase image brightness"}),"\n",(0,t.jsx)(n.li,{children:"D: To eliminate the need for cameras"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer: B"})," - Feature detection identifies distinctive points in images that can be tracked across frames to enable camera pose estimation and mapping."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What is monocular depth estimation?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A: Measuring depth with multiple cameras"}),"\n",(0,t.jsx)(n.li,{children:"B: Estimating depth from a single camera image"}),"\n",(0,t.jsx)(n.li,{children:"C: Depth estimation with LiDAR only"}),"\n",(0,t.jsx)(n.li,{children:"D: Measuring depth with stereo cameras"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer: B"})," - Monocular depth estimation is the process of estimating depth information from a single camera image using learned models or geometric cues."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,t.jsx)(n.p,{children:"After completing this chapter, students will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement visual navigation systems"}),"\n",(0,t.jsx)(n.li,{children:"Integrate perception and navigation capabilities"}),"\n",(0,t.jsx)(n.li,{children:"Develop simulation-to-reality pipelines"}),"\n",(0,t.jsx)(n.li,{children:"Create robust robot behaviors using vision"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Basic understanding of Python programming"}),"\n",(0,t.jsx)(n.li,{children:"Fundamentals of linear algebra and calculus"}),"\n",(0,t.jsx)(n.li,{children:"Basic knowledge of robotics concepts"}),"\n",(0,t.jsx)(n.li,{children:"Introduction to machine learning concepts"}),"\n",(0,t.jsx)(n.li,{children:"Completion of Module 0 (Introduction and Foundations)"}),"\n",(0,t.jsx)(n.li,{children:"Completion of Chapter 01 (Physical AI Basics)"}),"\n",(0,t.jsx)(n.li,{children:"Completion of Chapter 03 (ROS2 Nodes, Topics & Services)"}),"\n",(0,t.jsx)(n.li,{children:"Completion of Chapter 11 (Introduction to NVIDIA Isaac)"}),"\n",(0,t.jsx)(n.li,{children:"Completion of Chapter 16 (Vision-Language-Action Concepts)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"estimated-duration",children:"Estimated Duration"}),"\n",(0,t.jsx)(n.p,{children:"6 hours"})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);