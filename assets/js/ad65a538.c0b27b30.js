"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[248],{2764:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"02-Chapter-2-Physical-AI-Basics","title":"Chapter 2: Physical AI Basics","description":"Overview","source":"@site/docusaurus/docs/02-Chapter-2-Physical-AI-Basics.md","sourceDirName":".","slug":"/02-Chapter-2-Physical-AI-Basics","permalink":"/physical-ai-book/docs/02-Chapter-2-Physical-AI-Basics","draft":false,"unlisted":false,"editUrl":"https://github.com/ayesha-muttalib123/physical-ai-book/tree/main/docusaurus/docs/02-Chapter-2-Physical-AI-Basics.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"02-Chapter-2-Physical-AI-Basics","title":"Chapter 2: Physical AI Basics","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: Introduction","permalink":"/physical-ai-book/docs/01-Chapter-1-Introduction"},"next":{"title":"Chapter 3: Sensing and Embodied Intelligence","permalink":"/physical-ai-book/docs/03-Chapter-3-Sensing-And-Embodied-Intelligence"}}');var r=i(4848),t=i(8453);const o={id:"02-Chapter-2-Physical-AI-Basics",title:"Chapter 2: Physical AI Basics",sidebar_position:2},a="Chapter 2: Physical AI Basics",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Why It Matters",id:"why-it-matters",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Embodied Cognition",id:"embodied-cognition",level:3},{value:"Sensorimotor Learning",id:"sensorimotor-learning",level:3},{value:"Perception-Action Loop",id:"perception-action-loop",level:3},{value:"Environmental Affordances",id:"environmental-affordances",level:3},{value:"Morphological Computation",id:"morphological-computation",level:3},{value:"Physical Reasoning",id:"physical-reasoning",level:3},{value:"Uncertainty in Physical Systems",id:"uncertainty-in-physical-systems",level:3},{value:"Simulation-to-Reality Gap",id:"simulation-to-reality-gap",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"ROS2 Node for Basic Sensor Reading",id:"ros2-node-for-basic-sensor-reading",level:3},{value:"Gazebo Simulation Environment Setup",id:"gazebo-simulation-environment-setup",level:3},{value:"NVIDIA Isaac Sim Basic Robot Control",id:"nvidia-isaac-sim-basic-robot-control",level:3},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Mobile Robot Navigation Challenge",id:"mobile-robot-navigation-challenge",level:3},{value:"Object Manipulation Task",id:"object-manipulation-task",level:3},{value:"Human-Robot Interaction Scenario",id:"human-robot-interaction-scenario",level:3},{value:"Summary",id:"summary",level:2},{value:"Quiz",id:"quiz",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Estimated Duration",id:"estimated-duration",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-2-physical-ai-basics",children:"Chapter 2: Physical AI Basics"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"This chapter introduces the fundamental concepts of Physical AI, exploring the intersection between artificial intelligence and physical systems. Students will learn about embodied intelligence, the importance of sensorimotor learning, and how AI agents interact with the physical world through robotic platforms. We'll cover the theoretical foundations that underpin physical AI, including perception-action loops, environmental interaction, and the challenges of operating in real-world environments with uncertainty and noise."}),"\n",(0,r.jsx)(n.h2,{id:"why-it-matters",children:"Why It Matters"}),"\n",(0,r.jsx)(n.p,{children:"Understanding Physical AI basics is crucial for developing intelligent systems that can operate in the real world. Unlike traditional AI that operates on abstract data, Physical AI must deal with embodiment, physics, sensor limitations, and dynamic environments. This foundation is essential for creating robots that can navigate, manipulate objects, and interact safely with humans and environments. Physical AI principles form the backbone of applications in robotics, autonomous vehicles, manufacturing, and human-robot interaction."}),"\n",(0,r.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,r.jsx)(n.h3,{id:"embodied-cognition",children:"Embodied Cognition"}),"\n",(0,r.jsx)(n.p,{children:"How the physical form influences cognitive processes and decision-making. In Physical AI, the body is not just a vessel for computation but an active participant in the cognitive process. The physical properties of the robot, such as its shape, weight distribution, and material properties, directly influence how it perceives and interacts with the world."}),"\n",(0,r.jsx)(n.h3,{id:"sensorimotor-learning",children:"Sensorimotor Learning"}),"\n",(0,r.jsx)(n.p,{children:"The integration of sensory input with motor actions for learning. This concept emphasizes that learning occurs through the continuous interaction between sensing and acting. Rather than learning abstract representations, Physical AI systems learn through direct experience with their environment."}),"\n",(0,r.jsx)(n.h3,{id:"perception-action-loop",children:"Perception-Action Loop"}),"\n",(0,r.jsx)(n.p,{children:"The continuous cycle of sensing, processing, acting, and sensing again. This forms the fundamental feedback loop that enables intelligent behavior in physical systems. The robot perceives its environment, processes the information, acts upon the environment, and then perceives the effects of its action, creating an ongoing cycle of interaction."}),"\n",(0,r.jsx)(n.h3,{id:"environmental-affordances",children:"Environmental Affordances"}),"\n",(0,r.jsx)(n.p,{children:"Opportunities for action provided by the environment. This concept from ecological psychology suggests that the environment contains information about what actions are possible. For example, a handle affords grasping, a ramp affords climbing, and a chair affords sitting."}),"\n",(0,r.jsx)(n.h3,{id:"morphological-computation",children:"Morphological Computation"}),"\n",(0,r.jsx)(n.p,{children:"How physical properties contribute to computational processes. This refers to how the physical body can perform computations that would otherwise require processing power. For example, the elasticity of tendons can naturally store and release energy during locomotion, reducing the computational burden on the controller."}),"\n",(0,r.jsx)(n.h3,{id:"physical-reasoning",children:"Physical Reasoning"}),"\n",(0,r.jsx)(n.p,{children:"Understanding spatial relationships, physics, and object properties. Physical AI systems must reason about the physical world using knowledge of physics, geometry, and material properties to predict the outcomes of their actions."}),"\n",(0,r.jsx)(n.h3,{id:"uncertainty-in-physical-systems",children:"Uncertainty in Physical Systems"}),"\n",(0,r.jsx)(n.p,{children:"Dealing with sensor noise, actuator limitations, and environmental unpredictability. Real-world physical systems are inherently uncertain due to imperfect sensors, noisy actuators, and dynamic environments. Robust Physical AI systems must handle this uncertainty effectively."}),"\n",(0,r.jsx)(n.h3,{id:"simulation-to-reality-gap",children:"Simulation-to-Reality Gap"}),"\n",(0,r.jsx)(n.p,{children:"Bridging the differences between simulated and real-world environments. This gap refers to the challenges of transferring behaviors learned in simulation to real-world deployment, where factors like sensor noise, actuator dynamics, and environmental conditions may differ."}),"\n",(0,r.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,r.jsx)(n.h3,{id:"ros2-node-for-basic-sensor-reading",children:"ROS2 Node for Basic Sensor Reading"}),"\n",(0,r.jsx)(n.p,{children:"Simple ROS2 node that reads from a laser scanner and publishes processed distance information:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan\r\nfrom std_msgs.msg import Float32\r\n\r\nclass SensorProcessor(Node):\r\n    def __init__(self):\r\n        super().__init__('sensor_processor')\r\n        self.subscription = self.create_subscription(\r\n            LaserScan,\r\n            '/scan',\r\n            self.scan_callback,\r\n            10)\r\n        self.publisher = self.create_publisher(Float32, '/closest_obstacle', 10)\r\n\r\n    def scan_callback(self, msg):\r\n        # Find minimum distance in scan range\r\n        min_distance = min(msg.ranges)\r\n        obstacle_msg = Float32()\r\n        obstacle_msg.data = min_distance\r\n        self.publisher.publish(obstacle_msg)\r\n        self.get_logger().info(f'Closest obstacle: {min_distance:.2f}m')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    sensor_processor = SensorProcessor()\r\n    rclpy.spin(sensor_processor)\r\n    sensor_processor.destroy_node()\r\n    rclpy.shutdown()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"gazebo-simulation-environment-setup",children:"Gazebo Simulation Environment Setup"}),"\n",(0,r.jsx)(n.p,{children:"Basic Gazebo world file defining a simple environment for robot simulation:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\r\n<sdf version="1.7">\r\n  <world name="physical_ai_basics_world">\r\n    <include>\r\n      <uri>model://ground_plane</uri>\r\n    </include>\r\n    <include>\r\n      <uri>model://sun</uri>\r\n    </include>\r\n\r\n    \x3c!-- Simple obstacles --\x3e\r\n    <model name="box_1">\r\n      <pose>2 0 0.5 0 0 0</pose>\r\n      <link name="link">\r\n        <collision name="collision">\r\n          <geometry>\r\n            <box><size>1 1 1</size></box>\r\n          </geometry>\r\n        </collision>\r\n        <visual name="visual">\r\n          <geometry>\r\n            <box><size>1 1 1</size></box>\r\n          </geometry>\r\n        </visual>\r\n      </link>\r\n    </model>\r\n\r\n    \x3c!-- Light obstacles for testing --\x3e\r\n    <model name="cylinder_1">\r\n      <pose>-2 1 0.5 0 0 0</pose>\r\n      <link name="link">\r\n        <collision name="collision">\r\n          <geometry>\r\n            <cylinder>\r\n              <radius>0.3</radius>\r\n              <length>1.0</length>\r\n            </cylinder>\r\n          </geometry>\r\n        </collision>\r\n        <visual name="visual">\r\n          <geometry>\r\n            <cylinder>\r\n              <radius>0.3</radius>\r\n              <length>1.0</length>\r\n            </cylinder>\r\n          </geometry>\r\n        </visual>\r\n      </link>\r\n    </model>\r\n  </world>\r\n</sdf>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"nvidia-isaac-sim-basic-robot-control",children:"NVIDIA Isaac Sim Basic Robot Control"}),"\n",(0,r.jsx)(n.p,{children:"Simple control script for a robot in Isaac Sim demonstrating basic movement:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import omni\r\nimport carb\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\r\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\r\nfrom omni.isaac.wheeled_robots.robots import WheeledRobot\r\nfrom omni.isaac.core.utils.viewports import set_camera_view\r\nimport numpy as np\r\n\r\n# Initialize world\r\nworld = World(stage_units_in_meters=1.0)\r\n\r\n# Add robot to stage\r\nassets_root_path = get_assets_root_path()\r\njetbot_asset_path = assets_root_path + "/Isaac/Robots/Jetbot/jetbot_nucleus.usd"\r\n\r\nrobot = world.scene.add(\r\n    WheeledRobot(\r\n        prim_path="/World/Jetbot",\r\n        name="my_jetbot",\r\n        wheel_dof_names=["left_wheel_joint", "right_wheel_joint"],\r\n        create_robot=True,\r\n        usd_path=jetbot_asset_path,\r\n        position=np.array([0, 0, 0.2]),\r\n        orientation=np.array([1.0, 0.0, 0.0, 0.0])\r\n    )\r\n)\r\n\r\n# Set initial camera view\r\nset_camera_view(eye=[-2, -2, 1.5], target=[0, 0, 0])\r\n\r\n# Reset world and play\r\nworld.reset()\r\n\r\n# Control loop\r\nfor i in range(1000):\r\n    if i % 100 == 0:\r\n        # Apply forward velocity\r\n        robot.apply_wheel_actions(\r\n            wheel_velocities=np.array([5.0, 5.0]),\r\n            wheel_names=["left_wheel", "right_wheel"]\r\n        )\r\n    world.step(render=True)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,r.jsx)(n.h3,{id:"mobile-robot-navigation-challenge",children:"Mobile Robot Navigation Challenge"}),"\n",(0,r.jsx)(n.p,{children:"Students implement a robot that can navigate around obstacles in a known environment using sensor data and basic path planning algorithms."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Objectives:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Integrate sensor data (LIDAR/camera) for environment mapping"}),"\n",(0,r.jsx)(n.li,{children:"Implement reactive behaviors for obstacle avoidance"}),"\n",(0,r.jsx)(n.li,{children:"Navigate to a target location while avoiding obstacles"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Required Components:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Differential drive robot model"}),"\n",(0,r.jsx)(n.li,{children:"Laser scanner sensor"}),"\n",(0,r.jsx)(n.li,{children:"Known map of environment"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Evaluation Criteria:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Successfully reach target location"}),"\n",(0,r.jsx)(n.li,{children:"Avoid all obstacles"}),"\n",(0,r.jsx)(n.li,{children:"Efficient path planning"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"object-manipulation-task",children:"Object Manipulation Task"}),"\n",(0,r.jsx)(n.p,{children:"Students program a robotic arm to pick up and move objects of different shapes and weights, demonstrating understanding of physical properties and sensor feedback."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Objectives:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Detect object location and orientation"}),"\n",(0,r.jsx)(n.li,{children:"Plan grasp based on object properties"}),"\n",(0,r.jsx)(n.li,{children:"Execute manipulation with force control"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Required Components:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Robotic manipulator arm"}),"\n",(0,r.jsx)(n.li,{children:"Gripper/end-effector"}),"\n",(0,r.jsx)(n.li,{children:"Camera for object detection"}),"\n",(0,r.jsx)(n.li,{children:"Force/torque sensors"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Evaluation Criteria:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Successful grasping of objects"}),"\n",(0,r.jsx)(n.li,{children:"Safe manipulation without dropping"}),"\n",(0,r.jsx)(n.li,{children:"Adaptation to object variations"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"human-robot-interaction-scenario",children:"Human-Robot Interaction Scenario"}),"\n",(0,r.jsx)(n.p,{children:"Students develop a robot that can safely interact with humans in shared spaces, demonstrating understanding of social navigation and safety protocols."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Objectives:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Detect human presence and intent"}),"\n",(0,r.jsx)(n.li,{children:"Maintain safe distances"}),"\n",(0,r.jsx)(n.li,{children:"Communicate intentions clearly"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Required Components:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Person detection system"}),"\n",(0,r.jsx)(n.li,{children:"Social navigation algorithms"}),"\n",(0,r.jsx)(n.li,{children:"Communication interface"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Evaluation Criteria:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Maintain safety boundaries"}),"\n",(0,r.jsx)(n.li,{children:"Smooth interaction without disruption"}),"\n",(0,r.jsx)(n.li,{children:"Appropriate response to human behavior"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"Chapter 1 establishes the foundational concepts of Physical AI, emphasizing the importance of embodiment, sensorimotor integration, and the challenges of operating in real-world environments. Students learned about perception-action loops, environmental affordances, and morphological computation. Through ROS2, Gazebo, and Isaac examples, they gained hands-on experience with physical AI implementations. The practical examples demonstrated how these concepts translate into real robotic applications, preparing students for more advanced topics in subsequent chapters."}),"\n",(0,r.jsx)(n.h2,{id:"quiz",children:"Quiz"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"What is the primary difference between traditional AI and Physical AI?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A: Physical AI uses more compute power"}),"\n",(0,r.jsx)(n.li,{children:"B: Physical AI operates in real physical environments with embodiment"}),"\n",(0,r.jsx)(n.li,{children:"C: Physical AI uses different programming languages"}),"\n",(0,r.jsx)(n.li,{children:"D: Traditional AI is slower than Physical AI"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Answer: B"})," - Physical AI differs from traditional AI in that it must operate in real physical environments with embodiment, dealing with sensor noise, actuator limitations, physics, and dynamic environmental conditions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"What is a perception-action loop?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A: A type of neural network architecture"}),"\n",(0,r.jsx)(n.li,{children:"B: The continuous cycle of sensing, processing, acting, and sensing again"}),"\n",(0,r.jsx)(n.li,{children:"C: A programming paradigm for AI systems"}),"\n",(0,r.jsx)(n.li,{children:"D: A way to visualize sensor data"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Answer: B"})," - A perception-action loop is the continuous cycle where an agent senses its environment, processes the information, acts upon the environment, and then senses again to perceive the effects of its action."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"What are environmental affordances?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A: Computational resources available to the AI"}),"\n",(0,r.jsx)(n.li,{children:"B: Opportunities for action provided by the environment"}),"\n",(0,r.jsx)(n.li,{children:"C: Sensors embedded in the environment"}),"\n",(0,r.jsx)(n.li,{children:"D: Types of actuators available to the robot"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Answer: B"})," - Environmental affordances refer to the opportunities for action that are provided by the environment itself, such as a handle affording grasping or a ramp affording climbing."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"What is morphological computation?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A: Computing using DNA molecules"}),"\n",(0,r.jsx)(n.li,{children:"B: How physical properties contribute to computational processes"}),"\n",(0,r.jsx)(n.li,{children:"C: A type of parallel computing"}),"\n",(0,r.jsx)(n.li,{children:"D: Computation on robotic hardware"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Answer: B"})," - Morphological computation refers to how the physical properties of a system (such as the elasticity of tendons or the shape of a foot) contribute to computational processes, reducing the burden on the controller."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Why is the simulation-to-reality gap important in Physical AI?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A: Because simulations are always more difficult than reality"}),"\n",(0,r.jsx)(n.li,{children:"B: Because differences between simulated and real environments can cause unexpected behaviors"}),"\n",(0,r.jsx)(n.li,{children:"C: Because reality is more computationally expensive"}),"\n",(0,r.jsx)(n.li,{children:"D: Because simulations cannot model physics accurately"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Answer: B"})," - The simulation-to-reality gap is important because differences between simulated and real environments can cause controllers trained in simulation to behave unexpectedly when deployed on real robots, requiring domain adaptation techniques."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,r.jsx)(n.p,{children:"After completing this chapter, students will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Design ROS2 architectures for robot systems"}),"\n",(0,r.jsx)(n.li,{children:"Implement nodes, topics, services, and actions"}),"\n",(0,r.jsx)(n.li,{children:"Manage parameters and configurations"}),"\n",(0,r.jsx)(n.li,{children:"Develop ROS2 packages for multi-robot systems"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Basic understanding of Python programming"}),"\n",(0,r.jsx)(n.li,{children:"Fundamentals of linear algebra and calculus"}),"\n",(0,r.jsx)(n.li,{children:"Basic knowledge of robotics concepts"}),"\n",(0,r.jsx)(n.li,{children:"Introduction to machine learning concepts"}),"\n",(0,r.jsx)(n.li,{children:"Completion of Module 0 (Introduction and Foundations)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"estimated-duration",children:"Estimated Duration"}),"\n",(0,r.jsx)(n.p,{children:"4 hours"})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var s=i(6540);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);